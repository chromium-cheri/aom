diff --git a/av1/common/generic_code.c b/av1/common/generic_code.c
new file mode 100644
index 0000000..e256b9d
--- /dev/null
+++ b/av1/common/generic_code.c
@@ -0,0 +1,158 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#ifdef HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include "generic_code.h"
+
+void od_cdf_init(uint16_t *cdf, int ncdfs, int nsyms, int val, int first) {
+  int i;
+  int j;
+  for (i = 0; i < ncdfs; i++) {
+    for (j = 0; j < nsyms; j++) {
+      cdf[i*nsyms + j] = val*j + first;
+    }
+  }
+}
+
+/** Adapts a Q15 cdf after encoding/decoding a symbol. */
+void od_cdf_adapt_q15(int val, uint16_t *cdf, int n, int *count, int rate) {
+  int i;
+  *count = OD_MINI(*count + 1, 1 << rate);
+  OD_ASSERT(cdf[n - 1] == 32768);
+  if (*count >= 1 << rate) {
+    /* Steady-state adaptation based on a simple IIR with dyadic rate. */
+    for (i = 0; i < n; i++) {
+      int tmp;
+      /* When (i < val), we want the adjustment ((cdf[i] - tmp) >> rate) to be
+         positive so long as (cdf[i] > i + 1), and 0 when (cdf[i] == i + 1),
+         to ensure we don't drive any probabilities to 0. Replacing cdf[i] with
+         (i + 2) and solving ((i + 2 - tmp) >> rate == 1) for tmp produces
+         tmp == i + 2 - (1 << rate). Using this value of tmp with
+         cdf[i] == i + 1 instead gives an adjustment of 0 as desired.
+
+         When (i >= val), we want ((cdf[i] - tmp) >> rate) to be negative so
+         long as cdf[i] < 32768 - (n - 1 - i), and 0 when
+         cdf[i] == 32768 - (n - 1 - i), again to ensure we don't drive any
+         probabilities to 0. Since right-shifting any negative value is still
+         negative, we can solve (32768 - (n - 1 - i) - tmp == 0) for tmp,
+         producing tmp = 32769 - n + i. Using this value of tmp with smaller
+         values of cdf[i] instead gives negative adjustments, as desired.
+
+         Combining the two cases gives the expression below. These could be
+         stored in a lookup table indexed by n and rate to avoid the
+         arithmetic. */
+      tmp = 2 - (1<<rate) + i + (32767 + (1<<rate) - n)*(i >= val);
+      cdf[i] -= (cdf[i] - tmp) >> rate;
+    }
+  }
+  else {
+    int alpha;
+    /* Initial adaptation for the first symbols. The adaptation rate is
+       computed to be equivalent to what od_{en,de}code_cdf_adapt() does
+       when the initial cdf is set to increment/4. */
+    alpha = 4*32768/(n + 4**count);
+    for (i = 0; i < n; i++) {
+      int tmp;
+      tmp = (32768 - n)*(i >= val) + i + 1;
+      cdf[i] -= ((cdf[i] - tmp)*alpha) >> 15;
+    }
+  }
+  OD_ASSERT(cdf[n - 1] == 32768);
+}
+
+/** Initializes the cdfs and freq counts for a model.
+ *
+ * @param [out] model model being initialized
+ */
+void generic_model_init(generic_encoder *model) {
+  int i;
+  int j;
+  model->increment = 64;
+  for (i = 0; i < GENERIC_TABLES; i++) {
+    for (j = 0; j < 16; j++) {
+      /* Do flat initialization equivalent to a single symbol in each bin. */
+      model->cdf[i][j] = (j + 1) * model->increment;
+    }
+  }
+}
+
+/** Takes the base-2 log of E(x) in Q1.
+ *
+ * @param [in] ExQ16 expectation of x in Q16
+ *
+ * @retval 2*log2(ExQ16/2^16)
+ */
+int log_ex(int ex_q16) {
+  int lg;
+  int lg_q1;
+  int odd;
+  lg = OD_ILOG(ex_q16);
+  if (lg < 15) {
+    odd = ex_q16*ex_q16 > 2 << 2*lg;
+  }
+  else {
+    int tmp;
+    tmp = ex_q16 >> (lg - 8);
+    odd = tmp*tmp > (1 << 15);
+  }
+  lg_q1 = OD_MAXI(0, 2*lg - 33 + odd);
+  return lg_q1;
+}
+
+/** Updates the probability model based on the encoded/decoded value
+ *
+ * @param [in,out] model generic prob model
+ * @param [in,out] ExQ16 expectation of x
+ * @param [in]     x     variable encoded/decoded (used for ExQ16)
+ * @param [in]     xs    variable x after shift (used for the model)
+ * @param [in]     id    id of the icdf to adapt
+ * @param [in]     integration integration period of ExQ16 (leaky average over
+ * 1<<integration samples)
+ */
+void generic_model_update(generic_encoder *model, int *ex_q16, int x, int xs,
+ int id, int integration) {
+  int i;
+  int xenc;
+  uint16_t *cdf;
+  cdf = model->cdf[id];
+  /* Renormalize if we cannot add increment */
+  if (cdf[15] + model->increment > 32767) {
+    for (i = 0; i < 16; i++) {
+      /* Second term ensures that the pdf is non-null */
+      cdf[i] = (cdf[i] >> 1) + i + 1;
+    }
+  }
+  /* Update freq count */
+  xenc = OD_MINI(15, xs);
+  /* This can be easily vectorized */
+  for (i = xenc; i < 16; i++) cdf[i] += model->increment;
+  /* We could have saturated ExQ16 directly, but this is safe and simpler */
+  x = OD_MINI(x, 32767);
+  OD_IIR_DIADIC(*ex_q16, x << 16, integration);
+}
diff --git a/av1/common/generic_code.h b/av1/common/generic_code.h
new file mode 100644
index 0000000..ec45d00
--- /dev/null
+++ b/av1/common/generic_code.h
@@ -0,0 +1,99 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#if !defined(_generic_code_H)
+# define _generic_code_H
+
+# include "aom_dsp/entdec.h"
+# include "aom_dsp/entenc.h"
+
+# define GENERIC_TABLES 12
+
+#if OD_ACCOUNTING
+# define generic_decode(dec, model, max, ex_q16, integration, str) generic_decode_(dec, model, max, ex_q16, integration, str)
+# define od_decode_cdf_adapt_q15(ec, cdf, n, count, rate, str) od_decode_cdf_adapt_q15_(ec, cdf, n, count, rate, str)
+# define od_decode_cdf_adapt(ec, cdf, n, increment, str) od_decode_cdf_adapt_(ec, cdf, n, increment, str)
+#else
+# define generic_decode(dec, model, max, ex_q16, integration, str) generic_decode_(dec, model, max, ex_q16, integration)
+# define od_decode_cdf_adapt_q15(ec, cdf, n, count, rate, str) od_decode_cdf_adapt_q15_(ec, cdf, n, count, rate)
+# define od_decode_cdf_adapt(ec, cdf, n, increment, str) od_decode_cdf_adapt_(ec, cdf, n, increment)
+#endif
+
+typedef struct {
+  /** cdf for multiple expectations of x */
+  uint16_t cdf[GENERIC_TABLES][16];
+  /** Frequency increment for learning the cdfs */
+  int increment;
+} generic_encoder;
+
+#define OD_IIR_DIADIC(y, x, shift) ((y) += ((x) - (y)) >> (shift))
+
+void generic_model_init(generic_encoder *model);
+
+#define OD_CDFS_INIT(cdf, val) od_cdf_init(&cdf[0][0],\
+ sizeof(cdf)/sizeof(cdf[0]), sizeof(cdf[0])/sizeof(cdf[0][0]), val, val)
+
+#define OD_CDFS_INIT_FIRST(cdf, val, first) od_cdf_init(&cdf[0][0],\
+ sizeof(cdf)/sizeof(cdf[0]), sizeof(cdf[0])/sizeof(cdf[0][0]), val, first)
+
+#define OD_SINGLE_CDF_INIT(cdf, val) od_cdf_init(cdf,\
+ 1, sizeof(cdf)/sizeof(cdf[0]), val, val)
+
+#define OD_SINGLE_CDF_INIT_FIRST(cdf, val, first) od_cdf_init(cdf,\
+ 1, sizeof(cdf)/sizeof(cdf[0]), val, first)
+
+void od_cdf_init(uint16_t *cdf, int ncdfs, int nsyms, int val, int first);
+
+void od_cdf_adapt_q15(int val, uint16_t *cdf, int n, int *count, int rate);
+
+void od_encode_cdf_adapt_q15(od_ec_enc *ec, int val, uint16_t *cdf, int n,
+ int *count, int rate);
+
+void od_encode_cdf_adapt(od_ec_enc *ec, int val, uint16_t *cdf, int n,
+ int increment);
+
+int od_decode_cdf_adapt_(od_ec_dec *ec, uint16_t *cdf, int n,
+ int increment OD_ACC_STR);
+
+void generic_encode(od_ec_enc *enc, generic_encoder *model, int x, int max,
+ int *ex_q16, int integration);
+double generic_encode_cost(generic_encoder *model, int x, int max,
+ int *ex_q16);
+
+double od_encode_cdf_cost(int val, uint16_t *cdf, int n);
+
+int od_decode_cdf_adapt_q15_(od_ec_dec *ec, uint16_t *cdf, int n,
+ int *count, int rate OD_ACC_STR);
+
+int generic_decode_(od_ec_dec *dec, generic_encoder *model, int max,
+ int *ex_q16, int integration OD_ACC_STR);
+
+int log_ex(int ex_q16);
+
+void generic_model_update(generic_encoder *model, int *ex_q16, int x, int xs,
+ int id, int integration);
+
+#endif
diff --git a/av1/common/laplace_tables.c b/av1/common/laplace_tables.c
new file mode 100644
index 0000000..f1c3f9a
--- /dev/null
+++ b/av1/common/laplace_tables.c
@@ -0,0 +1,272 @@
+/* This file is auto-generated using "gen_laplace_tables 128 7" */
+
+/* clang-format off */
+
+#ifdef HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include "pvq.h"
+
+const uint16_t EXP_CDF_TABLE[128][16] = {
+  {32753,32754,32755,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {32499,32753,32755,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {32243,32747,32755,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {31987,32737,32755,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {31732,32724,32755,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {31476,32706,32754,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {31220,32684,32753,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {30964,32658,32751,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {30708,32628,32748,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {30452,32594,32745,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {30198,32558,32742,32756,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {29941,32515,32736,32755,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {29686,32470,32731,32755,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {29429,32419,32723,32754,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {29174,32366,32715,32753,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {28918,32308,32705,32752,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {28662,32246,32694,32750,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {28406,32180,32681,32748,32757,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {28150,32110,32667,32745,32756,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {27894,32036,32651,32742,32756,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {27639,31959,32634,32739,32755,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {27383,31877,32614,32735,32755,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {27126,31790,32592,32730,32754,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {26871,31701,32569,32725,32753,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {26615,31607,32543,32719,32752,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {26361,31511,32517,32713,32751,32758,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {26104,31408,32485,32704,32748,32757,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {25848,31302,32452,32695,32746,32757,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {25591,31191,32416,32684,32743,32756,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {25336,31078,32379,32674,32741,32756,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {25080,30960,32338,32661,32737,32755,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {24824,30838,32295,32648,32733,32754,32759,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {24568,30712,32248,32632,32728,32752,32758,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {24313,30583,32199,32616,32723,32751,32758,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {24057,30449,32147,32598,32718,32750,32758,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {23801,30311,32091,32578,32711,32747,32757,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {23546,30170,32033,32557,32704,32745,32757,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {23288,30022,31969,32532,32695,32742,32756,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {23033,29873,31904,32507,32686,32739,32755,32760,32761,32762,32763,32764,32765,32766,32767,32768},
+  {22778,29720,31835,32479,32675,32735,32753,32759,32761,32762,32763,32764,32765,32766,32767,32768},
+  {22521,29561,31761,32449,32664,32731,32752,32759,32761,32762,32763,32764,32765,32766,32767,32768},
+  {22267,29401,31686,32418,32652,32727,32751,32759,32761,32762,32763,32764,32765,32766,32767,32768},
+  {22011,29235,31605,32383,32638,32722,32749,32758,32761,32762,32763,32764,32765,32766,32767,32768},
+  {21754,29064,31520,32345,32622,32715,32746,32757,32761,32762,32763,32764,32765,32766,32767,32768},
+  {21501,28893,31434,32307,32607,32710,32745,32757,32761,32762,32763,32764,32765,32766,32767,32768},
+  {21243,28713,31339,32262,32587,32701,32741,32755,32760,32762,32763,32764,32765,32766,32767,32768},
+  {20988,28532,31243,32217,32567,32693,32738,32754,32760,32762,32763,32764,32765,32766,32767,32768},
+  {20730,28344,31140,32167,32544,32682,32733,32752,32759,32762,32763,32764,32765,32766,32767,32768},
+  {20476,28156,31036,32116,32521,32673,32730,32751,32759,32762,32763,32764,32765,32766,32767,32768},
+  {20220,27962,30926,32061,32495,32661,32725,32749,32758,32762,32763,32764,32765,32766,32767,32768},
+  {19963,27763,30810,32000,32465,32647,32718,32746,32757,32761,32763,32764,32765,32766,32767,32768},
+  {19708,27562,30691,31938,32435,32633,32712,32743,32756,32761,32763,32764,32765,32766,32767,32768},
+  {19454,27358,30569,31873,32403,32618,32705,32741,32755,32761,32763,32764,32765,32766,32767,32768},
+  {19196,27146,30438,31801,32365,32599,32696,32736,32753,32760,32763,32764,32765,32766,32767,32768},
+  {18942,26934,30306,31728,32328,32581,32688,32733,32752,32760,32763,32764,32765,32766,32767,32768},
+  {18684,26714,30164,31647,32284,32558,32676,32727,32749,32758,32762,32764,32765,32766,32767,32768},
+  {18429,26493,30021,31565,32240,32535,32664,32721,32746,32757,32762,32764,32765,32766,32767,32768},
+  {18174,26268,29872,31477,32192,32510,32652,32715,32743,32756,32762,32764,32765,32766,32767,32768},
+  {17920,26040,29719,31386,32141,32483,32638,32708,32740,32754,32761,32764,32765,32766,32767,32768},
+  {17661,25803,29556,31286,32083,32451,32620,32698,32734,32751,32759,32763,32765,32766,32767,32768},
+  {17406,25566,29391,31184,32024,32418,32603,32690,32731,32750,32759,32763,32765,32766,32767,32768},
+  {17151,25325,29220,31076,31961,32383,32584,32680,32726,32748,32758,32763,32765,32766,32767,32768},
+  {16896,25080,29044,30964,31894,32344,32562,32668,32719,32744,32756,32762,32765,32766,32767,32768},
+  {16639,24829,28860,30844,31821,32302,32539,32655,32712,32740,32754,32761,32764,32766,32767,32768},
+  {16384,24576,28672,30720,31744,32256,32512,32640,32704,32736,32752,32760,32764,32766,32767,32768},
+  {16130,24320,28479,30591,31663,32208,32485,32625,32696,32732,32750,32759,32764,32766,32767,32768},
+  {15872,24056,28276,30452,31574,32152,32450,32604,32683,32724,32745,32756,32762,32765,32766,32768},
+  {15615,23789,28068,30308,31480,32094,32415,32583,32671,32717,32741,32754,32761,32764,32766,32768},
+  {15361,23521,27856,30159,31382,32032,32377,32560,32657,32709,32737,32752,32760,32764,32766,32768},
+  {15103,23245,27634,30000,31275,31963,32334,32534,32642,32700,32731,32748,32757,32762,32765,32768},
+  {14848,22968,27409,29837,31165,31891,32288,32505,32624,32689,32725,32744,32755,32761,32764,32768},
+  {14592,22686,27176,29666,31047,31813,32238,32474,32605,32678,32718,32740,32752,32759,32763,32768},
+  {14336,22400,26936,29488,30923,31730,32184,32439,32583,32664,32709,32735,32749,32757,32762,32768},
+  {14079,22109,26689,29301,30791,31641,32125,32401,32559,32649,32700,32729,32746,32756,32761,32768},
+  {13825,21817,26437,29108,30652,31545,32061,32359,32532,32632,32690,32723,32742,32753,32759,32768},
+  {13568,21518,26176,28905,30504,31441,31990,32312,32501,32611,32676,32714,32736,32749,32757,32768},
+  {13314,21218,25911,28697,30351,31333,31916,32262,32468,32590,32662,32705,32731,32746,32755,32768},
+  {13054,20908,25633,28475,30185,31214,31833,32205,32429,32564,32645,32694,32723,32741,32752,32768},
+  {12803,20603,25356,28252,30017,31093,31748,32147,32390,32538,32628,32683,32717,32737,32749,32768},
+  {12544,20286,25064,28013,29833,30956,31649,32077,32341,32504,32605,32667,32705,32729,32744,32768},
+  {12288,19968,24768,27768,29643,30815,31547,32005,32291,32470,32582,32652,32696,32723,32740,32768},
+  {12033,19647,24465,27514,29443,30664,31437,31926,32235,32431,32555,32633,32683,32714,32734,32768},
+  {11777,19321,24154,27250,29233,30504,31318,31839,32173,32387,32524,32612,32668,32704,32727,32768},
+  {11521,18991,23835,26976,29013,30334,31190,31745,32105,32338,32489,32587,32651,32692,32719,32768},
+  {11265,18657,23508,26691,28780,30151,31051,31641,32028,32282,32449,32559,32631,32678,32709,32768},
+  {11006,18316,23170,26394,28535,29957,30901,31528,31944,32220,32404,32526,32607,32661,32697,32768},
+  {10752,17976,22830,26091,28282,29754,30743,31408,31854,32154,32356,32491,32582,32643,32684,32768},
+  {10496,17630,22479,25775,28015,29538,30573,31276,31754,32079,32300,32450,32552,32621,32668,32768},
+  {10240,17280,22120,25448,27736,29309,30390,31133,31644,31995,32237,32403,32517,32595,32649,32768},
+  { 9984,16926,21753,25109,27443,29066,30194,30978,31523,31902,32166,32349,32476,32565,32627,32768},
+  { 9728,16568,21377,24759,27137,28809,29984,30811,31392,31801,32088,32290,32432,32532,32602,32768},
+  { 9474,16208,20995,24399,26819,28539,29762,30631,31249,31688,32000,32222,32380,32492,32572,32768},
+  { 9216,15840,20601,24023,26483,28251,29522,30435,31091,31563,31902,32146,32321,32447,32537,32768},
+  { 8959,15469,20199,23636,26133,27947,29265,30223,30919,31425,31792,32059,32253,32394,32496,32768},
+  { 8705,15097,19791,23238,25770,27629,28994,29997,30733,31274,31671,31963,32177,32334,32449,32768},
+  { 8449,14719,19373,22827,25390,27292,28704,29752,30530,31107,31535,31853,32089,32264,32394,32768},
+  { 8192,14336,18944,22400,24992,26936,28394,29488,30308,30923,31384,31730,31989,32184,32330,32768},
+  { 7936,13950,18507,21961,24578,26561,28064,29203,30066,30720,31216,31592,31877,32093,32256,32768},
+  { 7678,13558,18060,21507,24146,26166,27713,28897,29804,30498,31030,31437,31749,31988,32171,32768},
+  { 7423,13165,17606,21041,23698,25753,27342,28571,29522,30257,30826,31266,31606,31869,32073,32768},
+  { 7168,12768,17143,20561,23231,25317,26947,28220,29215,29992,30599,31073,31444,31734,31960,32768},
+  { 6911,12365,16669,20065,22744,24858,26526,27842,28881,29701,30348,30858,31261,31579,31830,32768},
+  { 6657,11961,16188,19556,22240,24379,26083,27441,28523,29385,30072,30620,31056,31404,31681,32768},
+  { 6400,11550,15694,19029,21712,23871,25609,27007,28132,29037,29766,30352,30824,31204,31509,32768},
+  { 6142,11134,15190,18486,21164,23340,25108,26544,27711,28659,29429,30055,30564,30977,31313,32768},
+  { 5890,10720,14682,17932,20598,22785,24579,26051,27258,28248,29060,29726,30273,30721,31089,32768},
+  { 5631,10295,14157,17356,20005,22199,24016,25520,26766,27798,28652,29359,29945,30430,30832,32768},
+  { 5377, 9871,13628,16768,19393,21587,23421,24954,26236,27308,28204,28953,29579,30102,30539,32768},
+  { 5121, 9441,13086,16161,18756,20945,22792,24351,25666,26776,27712,28502,29169,29731,30206,32768},
+  { 4865, 9007,12534,15538,18096,20274,22129,23708,25053,26198,27173,28004,28711,29313,29826,32768},
+  { 4608, 8568,11971,14896,17409,19569,21425,23020,24391,25569,26581,27451,28199,28842,29394,32768},
+  { 4351, 8125,11398,14236,16697,18831,20682,22287,23679,24886,25933,26841,27628,28311,28903,32768},
+  { 4096, 7680,10816,13560,15961,18062,19900,21508,22915,24146,25224,26167,26992,27714,28346,32768},
+  { 3840, 7230,10223,12865,15197,17256,19074,20679,22096,23347,24451,25426,26287,27047,27718,32768},
+  { 3584, 6776, 9619,12151,14406,16414,18203,19796,21215,22479,23604,24606,25499,26294,27002,32768},
+  { 3328, 6318, 9004,11417,13585,15533,17283,18856,20269,21538,22678,23703,24624,25451,26194,32768},
+  { 3072, 5856, 8379,10665,12737,14615,16317,17859,19257,20524,21672,22712,23655,24509,25283,32768},
+  { 2816, 5390, 7743, 9894,11860,13657,15299,16800,18172,19426,20573,21621,22579,23455,24255,32768},
+  { 2560, 4920, 7096, 9102,10951,12656,14227,15676,17011,18242,19377,20423,21388,22277,23097,32768},
+  { 2304, 4446, 6437, 8288,10009,11609,13097,14480,15766,16961,18072,19105,20066,20959,21789,32768},
+  { 2048, 3968, 5768, 7456, 9038,10521,11911,13215,14437,15583,16657,17664,18608,19493,20323,32768},
+  { 1792, 3486, 5087, 6601, 8032, 9385,10664,11873,13016,14096,15117,16082,16995,17858,18673,32768},
+  { 1536, 3000, 4395, 5725, 6993, 8201, 9353,10451,11497,12494,13444,14350,15213,16036,16820,32768},
+  { 1280, 2510, 3692, 4828, 5919, 6968, 7976, 8944, 9875,10769,11628,12454,13248,14011,14744,32768},
+  { 1024, 2016, 2977, 3908, 4810, 5684, 6530, 7350, 8144, 8913, 9658,10380,11080,11758,12415,32768},
+  {  768, 1518, 2250, 2965, 3663, 4345, 5011, 5662, 6297, 6917, 7523, 8115, 8693, 9257, 9808,32768},
+  {  512, 1016, 1512, 2000, 2481, 2954, 3420, 3879, 4330, 4774, 5211, 5642, 6066, 6483, 6894,32768},
+  {  256,  510,  762, 1012, 1260, 1506, 1750, 1992, 2232, 2471, 2708, 2943, 3176, 3407, 3636,32768},
+};
+
+
+const uint16_t LAPLACE_OFFSET[128] = {
+  0,
+  29871,
+  28672,
+  27751,
+  26975,
+  26291,
+  25673,
+  25105,
+  24576,
+  24079,
+  23609,
+  23162,
+  22734,
+  22325,
+  21931,
+  21550,
+  21182,
+  20826,
+  20480,
+  20143,
+  19815,
+  19495,
+  19183,
+  18877,
+  18579,
+  18286,
+  17999,
+  17718,
+  17442,
+  17170,
+  16904,
+  16642,
+  16384,
+  16129,
+  15879,
+  15633,
+  15390,
+  15150,
+  14913,
+  14680,
+  14450,
+  14222,
+  13997,
+  13775,
+  13556,
+  13338,
+  13124,
+  12911,
+  12701,
+  12493,
+  12288,
+  12084,
+  11882,
+  11682,
+  11484,
+  11288,
+  11094,
+  10901,
+  10710,
+  10521,
+  10333,
+  10147,
+  9962,
+  9779,
+  9597,
+  9417,
+  9238,
+  9060,
+  8884,
+  8709,
+  8535,
+  8363,
+  8192,
+  8021,
+  7853,
+  7685,
+  7518,
+  7352,
+  7188,
+  7025,
+  6862,
+  6701,
+  6540,
+  6381,
+  6222,
+  6065,
+  5908,
+  5753,
+  5598,
+  5444,
+  5291,
+  5138,
+  4987,
+  4837,
+  4687,
+  4538,
+  4390,
+  4242,
+  4096,
+  3950,
+  3804,
+  3660,
+  3516,
+  3373,
+  3231,
+  3089,
+  2948,
+  2808,
+  2668,
+  2529,
+  2391,
+  2253,
+  2116,
+  1979,
+  1843,
+  1708,
+  1573,
+  1439,
+  1306,
+  1172,
+  1040,
+  908,
+  777,
+  646,
+  516,
+  386,
+  257,
+  128,
+};
diff --git a/av1/common/partition.c b/av1/common/partition.c
new file mode 100644
index 0000000..e3b958a
--- /dev/null
+++ b/av1/common/partition.c
@@ -0,0 +1,283 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#ifdef HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include "enums.h"
+#include "odintrin.h"
+#include "partition.h"
+#include "zigzag.h"
+
+OD_EXTERN const index_pair *OD_ZIGZAG4[4] = {
+  OD_ZIGZAG4_DCT_DCT,
+  OD_ZIGZAG4_ADST_DCT,
+  OD_ZIGZAG4_DCT_ADST,
+  OD_ZIGZAG4_ADST_ADST
+};
+
+OD_EXTERN const index_pair *OD_ZIGZAG8[4] = {
+  OD_ZIGZAG8_DCT_DCT,
+  OD_ZIGZAG8_ADST_DCT,
+  OD_ZIGZAG8_DCT_ADST,
+  OD_ZIGZAG8_ADST_ADST
+};
+
+OD_EXTERN const index_pair *OD_ZIGZAG16[4] = {
+  OD_ZIGZAG16_DCT_DCT,
+  OD_ZIGZAG16_ADST_DCT,
+  OD_ZIGZAG16_DCT_ADST,
+  OD_ZIGZAG16_ADST_ADST
+};
+
+OD_EXTERN const index_pair *OD_ZIGZAG32[4] = {
+  OD_ZIGZAG32_DCT_DCT,
+  OD_ZIGZAG32_DCT_DCT,
+  OD_ZIGZAG32_DCT_DCT,
+  OD_ZIGZAG32_DCT_DCT
+};
+
+OD_EXTERN const index_pair *OD_ZIGZAG64[4] = {
+  OD_ZIGZAG64_DCT_DCT,
+  OD_ZIGZAG64_DCT_DCT,
+  OD_ZIGZAG64_DCT_DCT,
+  OD_ZIGZAG64_DCT_DCT
+};
+
+/* The tables below specify how coefficient blocks are translated to
+   and from PVQ partition coding scan order for 4x4, 8x8 and 16x16 */
+static const int OD_LAYOUT64_OFFSETS[4] = { 0, 512, 1024, 3072 };
+const band_layout OD_LAYOUT64 = {
+  OD_ZIGZAG64,
+  64,
+  3,
+  OD_LAYOUT64_OFFSETS
+};
+
+static const int OD_LAYOUT32_OFFSETS[4] = { 0, 128, 256, 768 };
+const band_layout OD_LAYOUT32 = {
+  OD_ZIGZAG32,
+  32,
+  3,
+  OD_LAYOUT32_OFFSETS
+};
+
+static const int OD_LAYOUT16_OFFSETS[4] = { 0, 32, 64, 192 };
+const band_layout OD_LAYOUT16 = {
+  OD_ZIGZAG16,
+  16,
+  3,
+  OD_LAYOUT16_OFFSETS
+};
+
+const int OD_LAYOUT8_OFFSETS[4] = { 0, 8, 16, 48 };
+const band_layout OD_LAYOUT8 = {
+  OD_ZIGZAG8,
+  8,
+  3,
+  OD_LAYOUT8_OFFSETS
+};
+
+static const int OD_LAYOUT4_OFFSETS[2] = { 0, 15 };
+const band_layout OD_LAYOUT4 = {
+  OD_ZIGZAG4,
+  4,
+  1,
+  OD_LAYOUT4_OFFSETS
+};
+
+/* First element is the number of bands, followed by the list all the band
+  boundaries. */
+static const int OD_BAND_OFFSETS4[] = {1, 1, 16};
+static const int OD_BAND_OFFSETS8[] = {4, 1, 16, 24, 32, 64};
+static const int OD_BAND_OFFSETS16[] = {7, 1, 16, 24, 32, 64, 96, 128, 256};
+static const int OD_BAND_OFFSETS32[] = {10, 1, 16, 24, 32, 64, 96, 128, 256,
+ 384, 512, 1024};
+static const int OD_BAND_OFFSETS64[] = {13, 1, 16, 24, 32, 64, 96, 128, 256,
+ 384, 512, 1024, 1536, 2048, 4096};
+
+const int *const OD_BAND_OFFSETS[OD_NBSIZES + 1] = {
+  OD_BAND_OFFSETS4,
+  OD_BAND_OFFSETS8,
+  OD_BAND_OFFSETS16,
+  OD_BAND_OFFSETS32,
+  OD_BAND_OFFSETS64
+};
+
+/** Perform a single stage of conversion from a coefficient block in
+ * raster order into coding scan order
+ *
+ * @param [in]     layout  scan order specification
+ * @param [out]    dst     destination vector
+ * @param [in]     src     source coefficient block
+ * @param [int]    int     source vector row stride
+ */
+static void od_band_from_raster(const band_layout *layout, int16_t *dst,
+ const int16_t *src, int stride, TX_TYPE tx_type) {
+  int i;
+  int len;
+  len = layout->band_offsets[layout->nb_bands];
+  for (i = 0; i < len; i++) {
+    dst[i] = src[layout->dst_table[tx_type][i][1]*stride + layout->dst_table[tx_type][i][0]];
+  }
+}
+
+/** Perform a single stage of conversion from a vector in coding scan
+    order back into a coefficient block in raster order
+ *
+ * @param [in]     layout  scan order specification
+ * @param [out]    dst     destination coefficient block
+ * @param [in]     src     source vector
+ * @param [int]    stride  destination vector row stride
+ */
+static void od_raster_from_band(const band_layout *layout, int16_t *dst,
+ int stride, TX_TYPE tx_type, const int16_t *src) {
+  int i;
+  int len;
+  len = layout->band_offsets[layout->nb_bands];
+  for (i = 0; i < len; i++) {
+    dst[layout->dst_table[tx_type][i][1]*stride + layout->dst_table[tx_type][i][0]] = src[i];
+  }
+}
+
+static const band_layout *const OD_LAYOUTS[] = {&OD_LAYOUT4, &OD_LAYOUT8,
+ &OD_LAYOUT16, &OD_LAYOUT32, &OD_LAYOUT64};
+
+/** Converts a coefficient block in raster order into a vector in
+ * coding scan order with the PVQ partitions laid out one after
+ * another.  This works in stages; the 4x4 conversion is applied to
+ * the coefficients nearest DC, then the 8x8 applied to the 8x8 block
+ * nearest DC that was not already coded by 4x4, then 16x16 following
+ * the same pattern.
+ *
+ * @param [out]    dst        destination vector
+ * @param [in]     n          block size (along one side)
+ * @param [in]     ty_type    transfrom type
+ * @param [in]     src        source coefficient block
+ * @param [in]     stride     source vector row stride
+ */
+void od_raster_to_coding_order(int16_t *dst, int n, TX_TYPE ty_type,
+ const int16_t *src, int stride) {
+  int bs;
+  /* dst + 1 because DC is not included for 4x4 blocks. */
+  od_band_from_raster(OD_LAYOUTS[0], dst + 1, src, stride, ty_type);
+  for (bs = 1; bs < OD_NBSIZES; bs++) {
+    int size;
+    int offset;
+    /* Length of block size > 4. */
+    size = 1 << (OD_LOG_BSIZE0 + bs);
+    /* Offset is the size of the previous block squared. */
+    offset = 1 << 2*(OD_LOG_BSIZE0 - 1 + bs);
+    if (n >= size) {
+      /* 3 16x16 bands come after 3 8x8 bands, which come after 2 4x4 bands. */
+      od_band_from_raster(OD_LAYOUTS[bs], dst + offset, src, stride, ty_type);
+    }
+  }
+  dst[0] = src[0];
+}
+
+/** Converts a vector in coding scan order witht he PVQ partitions
+ * laid out one after another into a coefficient block in raster
+ * order. This works in stages in the reverse order of raster->scan
+ * order; the 16x16 conversion is applied to the coefficients that
+ * don't appear in an 8x8 block, then the 8x8 applied to the 8x8 block
+ * sans the 4x4 block it contains, then 4x4 is converted sans DC.
+ *
+ * @param [out]    dst        destination coefficient block
+ * @param [in]     stride     destination vector row stride
+ * @param [in]     src        source vector
+ * @param [in]     n          block size (along one side)
+ */
+void od_coding_order_to_raster(int16_t *dst, int stride, TX_TYPE ty_type,
+ const int16_t *src, int n) {
+  int bs;
+  /* src + 1 because DC is not included for 4x4 blocks. */
+  od_raster_from_band(OD_LAYOUTS[0], dst, stride, ty_type, src + 1);
+  for (bs = 1; bs < OD_NBSIZES; bs++) {
+    int size;
+    int offset;
+    /* Length of block size > 4 */
+    size = 1 << (OD_LOG_BSIZE0 + bs);
+    /* Offset is the size of the previous block squared. */
+    offset = 1 << 2*(OD_LOG_BSIZE0 - 1 + bs);
+    if (n >= size) {
+      /* 3 16x16 bands come after 3 8x8 bands, which come after 2 4x4 bands. */
+      od_raster_from_band(OD_LAYOUTS[bs], dst, stride, ty_type, src + offset);
+    }
+  }
+  dst[0] = src[0];
+}
+
+/** Perform a single stage of conversion from a coefficient block in
+ * raster order into coding scan order
+ *
+ * @param [in]     layout  scan order specification
+ * @param [out]    dst     destination vector
+ * @param [in]     src     source coefficient block
+ * @param [int]    int     source vector row stride
+ */
+static void od_band_from_raster_16(const band_layout *layout, int16_t *dst,
+ const int16_t *src, int stride) {
+  int i;
+  int len;
+  len = layout->band_offsets[layout->nb_bands];
+  for (i = 0; i < len; i++) {
+    dst[i] = src[layout->dst_table[DCT_DCT][i][1]*stride + layout->dst_table[DCT_DCT][i][0]];
+  }
+}
+
+/** Converts a coefficient block in raster order into a vector in
+ * coding scan order with the PVQ partitions laid out one after
+ * another.  This works in stages; the 4x4 conversion is applied to
+ * the coefficients nearest DC, then the 8x8 applied to the 8x8 block
+ * nearest DC that was not already coded by 4x4, then 16x16 following
+ * the same pattern.
+ *
+ * @param [out]    dst        destination vector
+ * @param [in]     n          block size (along one side)
+ * @param [in]     src        source coefficient block
+ * @param [in]     stride     source vector row stride
+ */
+void od_raster_to_coding_order_16(int16_t *dst, int n, const int16_t *src,
+ int stride) {
+  int bs;
+  /* dst + 1 because DC is not included for 4x4 blocks. */
+  od_band_from_raster_16(OD_LAYOUTS[0], dst + 1, src, stride);
+  for (bs = 1; bs < OD_NBSIZES; bs++) {
+    int size;
+    int offset;
+    /* Length of block size > 4. */
+    size = 1 << (OD_LOG_BSIZE0 + bs);
+    /* Offset is the size of the previous block squared. */
+    offset = 1 << 2*(OD_LOG_BSIZE0 - 1 + bs);
+    if (n >= size) {
+      /* 3 16x16 bands come after 3 8x8 bands, which come after 2 4x4 bands. */
+      od_band_from_raster_16(OD_LAYOUTS[bs], dst + offset, src, stride);
+    }
+  }
+  dst[0] = src[0];
+}
diff --git a/av1/common/partition.h b/av1/common/partition.h
new file mode 100644
index 0000000..1473d80
--- /dev/null
+++ b/av1/common/partition.h
@@ -0,0 +1,52 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#if !defined(_partition_H)
+# define _partition_H
+
+#include "odintrin.h"
+
+typedef unsigned char index_pair[2];
+
+typedef struct {
+  const index_pair **const dst_table;
+  int size;
+  int nb_bands;
+  const int *const band_offsets;
+} band_layout;
+
+extern const int *const OD_BAND_OFFSETS[OD_NBSIZES + 1];
+
+void od_raster_to_coding_order(int16_t *dst, int n,  TX_TYPE ty_type,
+ const int16_t *src, int stride);
+
+void od_coding_order_to_raster(int16_t *dst, int stride,  TX_TYPE ty_type,
+ const int16_t *src, int n);
+
+void od_raster_to_coding_order_16(int16_t *dst, int n, const int16_t *src,
+ int stride);
+
+#endif
diff --git a/av1/common/pvq.c b/av1/common/pvq.c
new file mode 100644
index 0000000..53edfda
--- /dev/null
+++ b/av1/common/pvq.c
@@ -0,0 +1,1083 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#ifdef HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include "odintrin.h"
+#include "partition.h"
+#include "pvq.h"
+#include <math.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+/*These tables were generated using compute_basis.c. If OD_FILT_SIZE is
+   changed, they have to be regenerated.*/
+static const double MAG4[] = {
+  0.870774, 0.872037, 0.949493, 0.947936
+};
+static const double MAG8[] = {
+  0.936496, 0.892830, 0.938452, 0.970087,
+  0.974272, 0.967954, 0.974035, 0.990480
+};
+static const double MAG16[] = {
+  0.968807, 0.940969, 0.947977, 0.957741,
+  0.969762, 0.978644, 0.984885, 0.988009,
+  0.987424, 0.985569, 0.984215, 0.984462,
+  0.987205, 0.991415, 0.994985, 0.998237
+};
+static const double MAG32[] = {
+  0.985068, 0.970006, 0.969893, 0.973192,
+  0.973444, 0.975881, 0.979601, 0.981070,
+  0.984989, 0.987520, 0.988830, 0.990983,
+  0.992376, 0.992884, 0.993447, 0.993381,
+  0.993712, 0.994060, 0.993294, 0.992392,
+  0.991338, 0.992410, 0.992051, 0.993874,
+  0.993488, 0.994162, 0.995318, 0.995925,
+  0.997475, 0.999027, 0.998303, 1.001413,
+};
+static const double MAG64[] = {
+  0.992453, 0.984930, 0.985137, 0.985029,
+  0.985514, 0.985784, 0.986269, 0.986854,
+  0.989932, 0.987780, 0.988269, 0.989175,
+  0.989951, 0.990466, 0.991145, 0.991839,
+  0.990773, 0.993191, 0.993618, 0.994221,
+  0.994662, 0.995259, 0.995826, 0.995996,
+  0.999070, 0.996624, 0.996835, 0.996948,
+  0.997022, 0.996973, 0.996993, 0.996996,
+  0.996871, 0.996828, 0.996598, 0.996688,
+  0.996845, 0.996407, 0.996327, 0.996435,
+  0.999173, 0.996216, 0.995981, 0.996173,
+  0.996595, 0.996334, 0.996512, 0.996627,
+  0.994976, 0.997113, 0.997248, 0.997548,
+  0.997943, 0.998121, 0.998291, 0.998687,
+  1.001696, 0.999133, 0.999315, 0.999621,
+  0.999745, 0.999905, 0.999936, 1.000075
+};
+
+static const double MAG4_CHROMA_420[] = {
+  0.870774, 0.872037, 0.949493, 0.947936
+};
+static const double MAG8_CHROMA_420[] = {
+  0.936496, 0.892830, 0.938452, 0.970087,
+  0.974272, 0.967954, 0.974035, 0.990480
+};
+static const double MAG16_CHROMA_420[] = {
+  0.968807, 0.940969, 0.947977, 0.957741,
+  0.969762, 0.978644, 0.984885, 0.988009,
+  0.987424, 0.985569, 0.984215, 0.984462,
+  0.987205, 0.991415, 0.994985, 0.998237
+};
+static const double MAG32_CHROMA_420[] = {
+  0.985068, 0.970006, 0.969893, 0.973192,
+  0.973444, 0.975881, 0.979601, 0.981070,
+  0.984989, 0.987520, 0.988830, 0.990983,
+  0.992376, 0.992884, 0.993447, 0.993381,
+  0.993712, 0.994060, 0.993294, 0.992392,
+  0.991338, 0.992410, 0.992051, 0.993874,
+  0.993488, 0.994162, 0.995318, 0.995925,
+  0.997475, 0.999027, 0.998303, 1.001413
+};
+static const double MAG64_CHROMA_420[] = {
+  0.992453, 0.984930, 0.985137, 0.985029,
+  0.985514, 0.985784, 0.986269, 0.986854,
+  0.989932, 0.987780, 0.988269, 0.989175,
+  0.989951, 0.990466, 0.991145, 0.991839,
+  0.990773, 0.993191, 0.993618, 0.994221,
+  0.994662, 0.995259, 0.995826, 0.995996,
+  0.999070, 0.996624, 0.996835, 0.996948,
+  0.997022, 0.996973, 0.996993, 0.996996,
+  0.996871, 0.996828, 0.996598, 0.996688,
+  0.996845, 0.996407, 0.996327, 0.996435,
+  0.999173, 0.996216, 0.995981, 0.996173,
+  0.996595, 0.996334, 0.996512, 0.996627,
+  0.994976, 0.997113, 0.997248, 0.997548,
+  0.997943, 0.998121, 0.998291, 0.998687,
+  1.001696, 0.999133, 0.999315, 0.999621,
+  0.999745, 0.999905, 0.999936, 1.000075
+};
+
+const double *OD_BASIS_MAG[2][OD_NBSIZES + 1] = {
+  {
+    MAG4, MAG8, MAG16, MAG32, MAG64
+  },
+  {
+    MAG4_CHROMA_420, MAG8_CHROMA_420, MAG16_CHROMA_420, MAG32_CHROMA_420,
+    MAG64_CHROMA_420
+  }
+};
+
+/* Quantization matrices for 8x8. For other block sizes, we currently just do
+   resampling. */
+/* Flat quantization, i.e. optimize for PSNR. */
+const int OD_QM8_Q4_FLAT[] = {
+  16, 16, 16, 16, 16, 16, 16, 16,
+  16, 16, 16, 16, 16, 16, 16, 16,
+  16, 16, 16, 16, 16, 16, 16, 16,
+  16, 16, 16, 16, 16, 16, 16, 16,
+  16, 16, 16, 16, 16, 16, 16, 16,
+  16, 16, 16, 16, 16, 16, 16, 16,
+  16, 16, 16, 16, 16, 16, 16, 16,
+  16, 16, 16, 16, 16, 16, 16, 16
+};
+# if 0
+/* M1: MPEG2 matrix for inter (which has a dead zone). */
+const int OD_QM8_Q4[] = {
+  16, 17, 18, 19, 20, 21, 22, 23,
+  17, 18, 19, 20, 21, 22, 23, 24,
+  18, 19, 20, 21, 22, 23, 24, 25,
+  19, 20, 21, 22, 23, 24, 26, 27,
+  20, 21, 22, 23, 25, 26, 27, 28,
+  21, 22, 23, 24, 26, 27, 28, 30,
+  22, 23, 24, 26, 27, 28, 30, 31,
+  23, 24, 25, 27, 28, 30, 31, 33};
+# endif
+# if 0
+/* M2: MPEG2 matrix for intra (no dead zone). */
+const int OD_QM8_Q4[] = {
+  16, 16, 19, 22, 22, 26, 26, 27,
+  16, 16, 22, 22, 26, 27, 27, 29,
+  19, 22, 26, 26, 27, 29, 29, 35,
+  22, 24, 27, 27, 29, 32, 34, 38,
+  26, 27, 29, 29, 32, 35, 38, 46,
+  27, 29, 34, 34, 35, 40, 46, 56,
+  29, 34, 34, 37, 40, 48, 56, 69,
+  34, 37, 38, 40, 48, 58, 69, 83
+};
+# endif
+# if 0
+/* M3: Taken from dump_psnrhvs. */
+const int OD_QM8_Q4[] = {
+  16, 16, 17, 20, 24, 29, 36, 42,
+  16, 17, 17, 19, 22, 26, 31, 37,
+  17, 17, 21, 23, 26, 30, 34, 40,
+  20, 19, 23, 28, 31, 35, 39, 45,
+  24, 22, 26, 31, 36, 41, 46, 51,
+  29, 26, 30, 35, 41, 47, 52, 58,
+  36, 31, 34, 39, 46, 52, 59, 66,
+  42, 37, 40, 45, 51, 58, 66, 73
+};
+# endif
+# if 1
+/* M4: a compromise equal to .5*(M3 + .5*(M2+transpose(M2))) */
+const int OD_QM8_Q4_HVS[] = {
+  16, 16, 18, 21, 24, 28, 32, 36,
+  16, 17, 20, 21, 24, 27, 31, 35,
+  18, 20, 24, 25, 27, 31, 33, 38,
+  21, 21, 25, 28, 30, 34, 37, 42,
+  24, 24, 27, 30, 34, 38, 43, 49,
+  28, 27, 31, 34, 38, 44, 50, 58,
+  32, 31, 33, 37, 43, 50, 58, 68,
+  36, 35, 38, 42, 49, 58, 68, 78
+};
+#endif
+
+/* Constants for the beta parameter, which controls how activity masking is
+   used.
+   beta = 1 / (1 - alpha), so when beta is 1, alpha is 0 and activity
+   masking is disabled. When beta is 1.5, activity masking is used. Note that
+   activity masking is neither used for 4x4 blocks nor for chroma. */
+#define OD_BETA(b) OD_QCONST32(b, OD_BETA_SHIFT)
+static const od_val16 OD_PVQ_BETA4_LUMA[1] = {OD_BETA(1.)};
+static const od_val16 OD_PVQ_BETA8_LUMA[4] = {OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.)};
+static const od_val16 OD_PVQ_BETA16_LUMA[7] = {OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.)};
+static const od_val16 OD_PVQ_BETA32_LUMA[10] = {OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.)};
+static const od_val16 OD_PVQ_BETA64_LUMA[13] = {OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.)};
+
+static const od_val16 OD_PVQ_BETA4_LUMA_MASKING[1] = {OD_BETA(1.)};
+static const od_val16 OD_PVQ_BETA8_LUMA_MASKING[4] = {OD_BETA(1.5),
+ OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5)};
+static const od_val16 OD_PVQ_BETA16_LUMA_MASKING[7] = {OD_BETA(1.5),
+ OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5),
+ OD_BETA(1.5)};
+static const od_val16 OD_PVQ_BETA32_LUMA_MASKING[10] = {OD_BETA(1.5),
+ OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5),
+ OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5)};
+static const od_val16 OD_PVQ_BETA64_LUMA_MASKING[13] = {OD_BETA(1.5),
+ OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5),
+ OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5), OD_BETA(1.5),
+ OD_BETA(1.5), OD_BETA(1.5)};
+
+static const od_val16 OD_PVQ_BETA4_CHROMA[1] = {OD_BETA(1.)};
+static const od_val16 OD_PVQ_BETA8_CHROMA[4] = {OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.)};
+static const od_val16 OD_PVQ_BETA16_CHROMA[7] = {OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.)};
+static const od_val16 OD_PVQ_BETA32_CHROMA[10] = {OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.)};
+static const od_val16 OD_PVQ_BETA64_CHROMA[13] = {OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.),
+ OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.), OD_BETA(1.)};
+
+const od_val16 *const OD_PVQ_BETA[2][OD_NPLANES_MAX][OD_NBSIZES + 1] = {
+ {{OD_PVQ_BETA4_LUMA, OD_PVQ_BETA8_LUMA,
+   OD_PVQ_BETA16_LUMA, OD_PVQ_BETA32_LUMA,
+   OD_PVQ_BETA64_LUMA},
+  {OD_PVQ_BETA4_CHROMA, OD_PVQ_BETA8_CHROMA,
+   OD_PVQ_BETA16_CHROMA, OD_PVQ_BETA32_CHROMA,
+   OD_PVQ_BETA64_CHROMA},
+  {OD_PVQ_BETA4_CHROMA, OD_PVQ_BETA8_CHROMA,
+   OD_PVQ_BETA16_CHROMA, OD_PVQ_BETA32_CHROMA,
+   OD_PVQ_BETA64_CHROMA},
+  {OD_PVQ_BETA4_CHROMA, OD_PVQ_BETA8_CHROMA,
+   OD_PVQ_BETA16_CHROMA, OD_PVQ_BETA32_CHROMA,
+   OD_PVQ_BETA64_CHROMA}},
+ {{OD_PVQ_BETA4_LUMA_MASKING, OD_PVQ_BETA8_LUMA_MASKING,
+   OD_PVQ_BETA16_LUMA_MASKING, OD_PVQ_BETA32_LUMA_MASKING,
+   OD_PVQ_BETA64_LUMA_MASKING},
+  {OD_PVQ_BETA4_CHROMA, OD_PVQ_BETA8_CHROMA,
+   OD_PVQ_BETA16_CHROMA, OD_PVQ_BETA32_CHROMA,
+   OD_PVQ_BETA64_CHROMA},
+  {OD_PVQ_BETA4_CHROMA, OD_PVQ_BETA8_CHROMA,
+   OD_PVQ_BETA16_CHROMA, OD_PVQ_BETA32_CHROMA,
+   OD_PVQ_BETA64_CHROMA},
+  {OD_PVQ_BETA4_CHROMA, OD_PVQ_BETA8_CHROMA,
+   OD_PVQ_BETA16_CHROMA, OD_PVQ_BETA32_CHROMA,
+   OD_PVQ_BETA64_CHROMA}}
+};
+
+void od_adapt_pvq_ctx_reset(od_pvq_adapt_ctx *state, int is_keyframe) {
+  od_pvq_codeword_ctx *ctx;
+  int i;
+  int pli;
+  int bs;
+  ctx = &state->pvq_codeword_ctx;
+  generic_model_init(&state->pvq_param_model[0]);
+  generic_model_init(&state->pvq_param_model[1]);
+  generic_model_init(&state->pvq_param_model[2]);
+  for (i = 0; i < 2*OD_NBSIZES; i++) {
+    ctx->pvq_adapt[4*i + OD_ADAPT_K_Q8] = 384;
+    ctx->pvq_adapt[4*i + OD_ADAPT_SUM_EX_Q8] = 256;
+    ctx->pvq_adapt[4*i + OD_ADAPT_COUNT_Q8] = 104;
+    ctx->pvq_adapt[4*i + OD_ADAPT_COUNT_EX_Q8] = 128;
+  }
+  ctx->pvq_k1_increment = 128;
+  OD_CDFS_INIT(ctx->pvq_k1_cdf, ctx->pvq_k1_increment);
+  for (pli = 0; pli < OD_NPLANES_MAX; pli++) {
+    for (bs = 0; bs < OD_NBSIZES; bs++)
+    for (i = 0; i < PVQ_MAX_PARTITIONS; i++) {
+      state->pvq_exg[pli][bs][i] = 2 << 16;
+    }
+  }
+  for (i = 0; i < OD_NBSIZES*PVQ_MAX_PARTITIONS; i++) {
+    state->pvq_ext[i] = is_keyframe ? 24576 : 2 << 16;
+  }
+  state->pvq_gaintheta_increment = 128;
+  OD_CDFS_INIT(state->pvq_gaintheta_cdf, state->pvq_gaintheta_increment >> 2);
+  state->pvq_skip_dir_increment = 128;
+  OD_CDFS_INIT(state->pvq_skip_dir_cdf, state->pvq_skip_dir_increment >> 2);
+  ctx->pvq_split_increment = 128;
+  OD_CDFS_INIT(ctx->pvq_split_cdf, ctx->pvq_split_increment >> 1);
+}
+
+/* QMs are arranged from smallest to largest blocksizes, first for
+   blocks with decimation=0, followed by blocks with decimation=1.*/
+int od_qm_offset(int bs, int xydec)
+{
+    return xydec*OD_QM_STRIDE + OD_QM_OFFSET(bs);
+}
+
+/* Initialize the quantization matrix with the magnitude compensation applied.
+   We need to compensate for the magnitude because lapping causes some basis
+   functions to be smaller, so they would end up being quantized too finely
+   (the same error in the quantized domain would result in a smaller pixel
+   domain error). */
+// Note: When varying scan orders for hybrid transform is used by PVQ,
+// since AOM does not use magnitude compensation (i.e. simplay x16 for all coeffs),
+// we don't need seperate qm and qm_inv for each transform type.
+void od_init_qm(int16_t *x, int16_t *x_inv, const int *qm) {
+  int i;
+  int j;
+  int16_t y[OD_BSIZE_MAX*OD_BSIZE_MAX];
+  int16_t y_inv[OD_BSIZE_MAX*OD_BSIZE_MAX];
+  int16_t *x1;
+  int16_t *x1_inv;
+  int off;
+  int bs;
+  int xydec;
+  for (bs = 0; bs < OD_NBSIZES; bs++) {
+    for (xydec = 0; xydec < 2; xydec++) {
+      off = od_qm_offset(bs, xydec);
+      x1 = x + off;
+      x1_inv = x_inv + off;
+      for (i = 0; i < 4 << bs; i++) {
+        for (j = 0; j < 4 << bs; j++) {
+          double mag;
+#if OD_DEBLOCKING || OD_DISABLE_FILTER
+          mag = 1.0;
+#else
+          mag = OD_BASIS_MAG[xydec][bs][i]*OD_BASIS_MAG[xydec][bs][j];
+#endif
+          if (i == 0 && j == 0) {
+            mag = 1.0;
+          }
+          else {
+            mag /= 0.0625*qm[(i << 1 >> bs)*8 + (j << 1 >> bs)];
+            OD_ASSERT(mag > 0.0);
+          }
+          /*Convert to fit in 16 bits.*/
+          y[i*(4 << bs) + j] = (int16_t)OD_MINI(OD_QM_SCALE_MAX,
+           (int32_t)floor(.5 + mag*OD_QM_SCALE));
+          y_inv[i*(4 << bs) + j] = (int16_t)floor(.5
+           + OD_QM_SCALE*OD_QM_INV_SCALE/(double)y[i*(4 << bs) + j]);
+        }
+      }
+      od_raster_to_coding_order_16(x1, 4 << bs, y, 4 << bs);
+      od_raster_to_coding_order_16(x1_inv, 4 << bs, y_inv, 4 << bs);
+    }
+  }
+}
+
+/* Maps each possible size (n) in the split k-tokenizer to a different value.
+   Possible values of n are:
+   2, 3, 4, 7, 8, 14, 15, 16, 31, 32, 63, 64, 127, 128
+   Since we don't care about the order (even in the bit-stream) the simplest
+   ordering (implemented here) is:
+   14, 2, 3, 4, 7, 8, 15, 16, 31, 32, 63, 64, 127, 128 */
+int od_pvq_size_ctx(int n) {
+  int logn;
+  int odd;
+  logn = OD_ILOG(n - 1);
+  odd = n & 1;
+  return 2*logn - 1 - odd - 7*(n == 14);
+}
+
+/* Maps a length n to a context for the (k=1, n<=16) coder, with a special
+   case when n is the original length (orig_length=1) of the vector (i.e. we
+   haven't split it yet). For orig_length=0, we use the same mapping as
+   od_pvq_size_ctx() up to n=16. When orig_length=1, we map lengths
+   7, 8, 14, 15 to contexts 8 to 11. */
+int od_pvq_k1_ctx(int n, int orig_length) {
+  if (orig_length) return 8 + 2*(n > 8) + (n & 1);
+  else return od_pvq_size_ctx(n);
+}
+
+/* Indexing for the packed quantization matrices. */
+int od_qm_get_index(int bs, int band) {
+  /* The -band/3 term is due to the fact that we force corresponding horizontal
+     and vertical bands to have the same quantization. */
+  OD_ASSERT(bs >= 0 && bs < OD_NBSIZES);
+  return bs*(bs + 1) + band - band/3;
+}
+
+#if !defined(OD_FLOAT_PVQ)
+/*See celt/mathops.c in Opus and tools/cos_search.c.*/
+static int16_t od_pvq_cos_pi_2(int16_t x)
+{
+  int16_t x2;
+  x2 = OD_MULT16_16_Q15(x, x);
+  return OD_MINI(32767, (1073758164 - x*x + x2*(-7654 + OD_MULT16_16_Q16(x2,
+   16573 + OD_MULT16_16_Q16(-2529, x2)))) >> 15);
+}
+#endif
+
+/*Approximates cos(x) for -pi < x < pi.
+  Input is in OD_THETA_SCALE.*/
+od_val16 od_pvq_cos(od_val32 x) {
+#if defined(OD_FLOAT_PVQ)
+  return cos(x);
+#else
+  /*Wrap x around by masking, since cos is periodic.*/
+  x = x & 0x0001ffff;
+  if (x > (1 << 16)) {
+    x = (1 << 17) - x;
+  }
+  if (x & 0x00007fff) {
+    if (x < (1 << 15)) {
+       return od_pvq_cos_pi_2((int16_t)x);
+    }
+    else {
+      return -od_pvq_cos_pi_2((int16_t)(65536 - x));
+    }
+  }
+  else {
+    if (x & 0x0000ffff) {
+      return 0;
+    }
+    else if (x & 0x0001ffff) {
+      return -32767;
+    }
+    else {
+      return 32767;
+    }
+  }
+#endif
+}
+
+/*Approximates sin(x) for 0 <= x < pi.
+  Input is in OD_THETA_SCALE.*/
+od_val16 od_pvq_sin(od_val32 x) {
+#if defined(OD_FLOAT_PVQ)
+  return sin(x);
+#else
+  return od_pvq_cos(32768 - x);
+#endif
+}
+
+#if !defined(OD_FLOAT_PVQ)
+/* Computes an upper-bound on the number of bits required to store the L2 norm
+   of a vector (excluding sign). */
+int od_vector_log_mag(const od_coeff *x, int n) {
+  int i;
+  int32_t sum;
+  sum = 0;
+  for (i = 0; i < n; i++) {
+    int16_t tmp;
+    tmp = x[i] >> 8;
+    sum += tmp*(int32_t)tmp;
+  }
+  /* We add one full bit (instead of rounding OD_ILOG() up) for safety because
+     the >> 8 above causes the sum to be slightly underestimated. */
+  return 8 + 1 + OD_ILOG(n + sum)/2;
+}
+#endif
+
+/** Computes Householder reflection that aligns the reference r to the
+ *  dimension in r with the greatest absolute value. The reflection
+ *  vector is returned in r.
+ *
+ * @param [in,out]  r      reference vector to be reflected, reflection
+ *                         also returned in r
+ * @param [in]      n      number of dimensions in r
+ * @param [in]      gr     gain of reference vector
+ * @param [out]     sign   sign of reflection
+ * @return                 dimension number to which reflection aligns
+ **/
+int od_compute_householder(od_val16 *r, int n, od_val32 gr, int *sign,
+ int shift) {
+  int m;
+  int i;
+  int s;
+  od_val16 maxr;
+  OD_UNUSED(shift);
+  /* Pick component with largest magnitude. Not strictly
+   * necessary, but it helps numerical stability */
+  m = 0;
+  maxr = 0;
+  for (i = 0; i < n; i++) {
+    if (OD_ABS(r[i]) > maxr) {
+      maxr = OD_ABS(r[i]);
+      m = i;
+    }
+  }
+  s = r[m] > 0 ? 1 : -1;
+  /* This turns r into a Householder reflection vector that would reflect
+   * the original r[] to e_m */
+  r[m] += OD_SHR_ROUND(gr*s, shift);
+  *sign = s;
+  return m;
+}
+
+#if !defined(OD_FLOAT_PVQ)
+#define OD_RCP_INSHIFT 15
+#define OD_RCP_OUTSHIFT 14
+static od_val16 od_rcp(od_val16 x)
+{
+  int i;
+  od_val16 n;
+  od_val16 r;
+  i = OD_ILOG(x) - 1;
+  /*n is Q15 with range [0,1).*/
+  n = OD_VSHR_ROUND(x, i - OD_RCP_INSHIFT) - (1 << OD_RCP_INSHIFT);
+  /*Start with a linear approximation:
+    r = 1.8823529411764706-0.9411764705882353*n.
+    The coefficients and the result are Q14 in the range [15420,30840].*/
+  r = 30840 + OD_MULT16_16_Q15(-15420, n);
+  /*Perform two Newton iterations:
+    r -= r*((r*n)-1.Q15)
+       = r*((r*n)+(r-1.Q15)).*/
+  r = r - OD_MULT16_16_Q15(r, (OD_MULT16_16_Q15(r, n) + r - 32768));
+  /*We subtract an extra 1 in the second iteration to avoid overflow; it also
+     neatly compensates for truncation error in the rest of the process.*/
+  r = r - (1 + OD_MULT16_16_Q15(r, OD_MULT16_16_Q15(r, n) + r - 32768));
+  /*r is now the Q15 solution to 2/(n+1), with a maximum relative error
+     of 7.05346E-5, a (relative) RMSE of 2.14418E-5, and a peak absolute
+     error of 1.24665/32768.*/
+  return OD_VSHR_ROUND(r, i - OD_RCP_OUTSHIFT);
+}
+#endif
+
+/** Applies Householder reflection from compute_householder(). The
+ * reflection is its own inverse.
+ *
+ * @param [out]     out    reflected vector
+ * @param [in]      x      vector to be reflected
+ * @param [in]      r      reflection
+ * @param [in]      n      number of dimensions in x,r
+ */
+void od_apply_householder(od_val16 *out, const od_val16 *x, const od_val16 *r,
+ int n) {
+  int i;
+  od_val32 proj;
+  od_val16 proj_1;
+  od_val32 l2r;
+#if !defined(OD_FLOAT_PVQ)
+  od_val16 proj_norm;
+  od_val16 l2r_norm;
+  od_val16 rcp;
+  int proj_shift;
+  int l2r_shift;
+  int outshift;
+#endif
+  /*FIXME: Can we get l2r and/or l2r_shift from an earlier computation?*/
+  l2r = 0;
+  for (i = 0; i < n; i++) {
+    l2r += OD_MULT16_16(r[i], r[i]);
+  }
+  /* Apply Householder reflection */
+  proj = 0;
+  for (i = 0; i < n; i++) {
+    proj += OD_MULT16_16(r[i], x[i]);
+  }
+#if defined(OD_FLOAT_PVQ)
+  proj_1 = proj*2./(1e-100 + l2r);
+  for (i = 0; i < n; i++) {
+    out[i] = x[i] - r[i]*proj_1;
+  }
+#else
+  /*l2r_norm is [0.5, 1.0[ in Q15.*/
+  l2r_shift = (OD_ILOG(l2r) - 1) - 14;
+  l2r_norm = OD_VSHR_ROUND(l2r, l2r_shift);
+  rcp = od_rcp(l2r_norm);
+  proj_shift = (OD_ILOG(abs(proj)) - 1) - 14;
+  /*proj_norm is [0.5, 1.0[ in Q15.*/
+  proj_norm = OD_VSHR_ROUND(proj, proj_shift);
+  proj_1 = OD_MULT16_16_Q15(proj_norm, rcp);
+  /*The proj*2. in the float code becomes -1 in the final outshift.
+    The sign of l2r_shift is positive since we're taking the reciprocal of
+     l2r_norm and this is a right shift.*/
+  outshift = OD_MINI(30, OD_RCP_OUTSHIFT - proj_shift - 1 + l2r_shift);
+  if (outshift >= 0) {
+    for (i = 0; i < n; i++) {
+      int32_t tmp;
+      tmp = OD_MULT16_16(r[i], proj_1);
+      tmp = OD_SHR_ROUND(tmp, outshift);
+      out[i] = x[i] - tmp;
+    }
+  }
+  else {
+    /*FIXME: Can we make this case impossible?
+      Right now, if r[] is all zeros except for 1, 2, or 3 ones, and
+       if x[] is all zeros except for large values at the same position as the
+       ones in r[], then we can end up with a shift of -1.*/
+    for (i = 0; i < n; i++) {
+      int32_t tmp;
+      tmp = OD_MULT16_16(r[i], proj_1);
+      tmp = OD_SHL(tmp, -outshift);
+      out[i] = x[i] - tmp;
+    }
+  }
+#endif
+}
+
+#if !defined(OD_FLOAT_PVQ)
+#define OD_EXP2_INSHIFT 15
+#define OD_EXP2_FRACSHIFT 15
+#define OD_EXP2_OUTSHIFT 15
+static const int32_t OD_EXP2_C[5] = {32768, 22709, 7913, 1704, 443};
+/*Output is [1.0, 2.0) in Q(OD_EXP2_FRACSHIFT).
+  It does not include the integer offset, which is added in od_exp2 after the
+   final shift).*/
+static int32_t od_exp2_frac(int32_t x)
+{
+  return OD_MULT16_16_Q15(x, (OD_EXP2_C[1] + OD_MULT16_16_Q15(x,
+   (OD_EXP2_C[2] + OD_MULT16_16_Q15(x, (OD_EXP2_C[3]
+   + OD_MULT16_16_Q15(x, OD_EXP2_C[4])))))));
+}
+
+/** Base-2 exponential approximation (2^x) with Q15 input and output.*/
+static int32_t od_exp2(int32_t x)
+{
+  int integer;
+  int32_t frac;
+  integer = x >> OD_EXP2_INSHIFT;
+  if (integer > 14)
+    return 0x7f000000;
+  else if (integer < -15)
+    return 0;
+  frac = od_exp2_frac(x - OD_SHL(integer, OD_EXP2_INSHIFT));
+  return OD_VSHR_ROUND(OD_EXP2_C[0] + frac, -integer) + 1;
+}
+
+#define OD_LOG2_INSHIFT 15
+#define OD_LOG2_OUTSHIFT 15
+#define OD_LOG2_INSCALE_1 (1./(1 << OD_LOG2_INSHIFT))
+#define OD_LOG2_OUTSCALE (1 << OD_LOG2_OUTSHIFT)
+static int16_t od_log2(int16_t x)
+{
+  return x + OD_MULT16_16_Q15(x, (14482 + OD_MULT16_16_Q15(x, (-23234
+   + OD_MULT16_16_Q15(x, (13643 + OD_MULT16_16_Q15(x, (-6403
+   + OD_MULT16_16_Q15(x, 1515)))))))));
+}
+
+static int32_t od_pow(int32_t x, od_val16 beta)
+{
+  int16_t t;
+  int xshift;
+  int log2_x;
+  od_val32 logr;
+  /*FIXME: this conditional is to avoid doing log2(0).*/
+  if (x == 0)
+    return 0;
+  log2_x = (OD_ILOG(x) - 1);
+  xshift = log2_x - OD_LOG2_INSHIFT;
+  /*t should be in range [0.0, 1.0[ in Q(OD_LOG2_INSHIFT).*/
+  t = OD_VSHR(x, xshift) - (1 << OD_LOG2_INSHIFT);
+  /*log2(g/OD_COMPAND_SCALE) = log2(x) - OD_COMPAND_SHIFT in
+     Q(OD_LOG2_OUTSHIFT).*/
+  logr = od_log2(t) + (log2_x - OD_COMPAND_SHIFT)*OD_LOG2_OUTSCALE;
+  logr = OD_MULT16_32_QBETA(beta, logr);
+  return od_exp2(logr);
+}
+#endif
+
+/** Gain companding: raises gain to the power 1/beta for activity masking.
+ *
+ * @param [in]  g     real (uncompanded) gain
+ * @param [in]  q0    uncompanded quality parameter
+ * @param [in]  beta  activity masking beta param (exponent)
+ * @return            g^(1/beta)
+ */
+static od_val32 od_gain_compand(od_val32 g, int q0, od_val16 beta) {
+#if defined(OD_FLOAT_PVQ)
+  if (beta == 1) return OD_ROUND32(OD_CGAIN_SCALE*g/(double)q0);
+  else {
+    return OD_ROUND32(OD_CGAIN_SCALE*OD_COMPAND_SCALE*pow(g*OD_COMPAND_SCALE_1,
+     1./beta)/(double)q0);
+  }
+#else
+  if (beta == OD_BETA(1)) return (OD_CGAIN_SCALE*g + (q0 >> 1))/q0;
+  else {
+    int32_t expr;
+    /*FIXME: This is 1/beta in Q(BETA_SHIFT), should use od_rcp() instead.*/
+    expr = od_pow(g, OD_ROUND16((1 << (2*OD_BETA_SHIFT))/(double)beta));
+    expr <<= OD_CGAIN_SHIFT + OD_COMPAND_SHIFT - OD_EXP2_OUTSHIFT;
+    return (expr + (q0 >> 1))/q0;
+  }
+#endif
+}
+
+#if !defined(OD_FLOAT_PVQ)
+#define OD_SQRT_INSHIFT 16
+#define OD_SQRT_OUTSHIFT 15
+static int16_t od_rsqrt_norm(int16_t x);
+
+static int16_t od_sqrt_norm(int32_t x)
+{
+  OD_ASSERT(x < 65536);
+  return OD_MINI(OD_SHR_ROUND(x*od_rsqrt_norm(x), OD_SQRT_OUTSHIFT), 32767);
+}
+
+static int16_t od_sqrt(int32_t x, int *sqrt_shift)
+{
+  int k;
+  int s;
+  int32_t t;
+  if (x == 0) {
+    *sqrt_shift = 0;
+     return 0;
+  }
+  OD_ASSERT(x < (1 << 30));
+  k = ((OD_ILOG(x) - 1) >> 1);
+  /*t is x in the range [0.25, 1) in QINSHIFT, or x*2^(-s).
+    Shift by log2(x) - log2(0.25*(1 << INSHIFT)) to ensure 0.25 lower bound.*/
+  s = 2*k - (OD_SQRT_INSHIFT - 2);
+  t = OD_VSHR(x, s);
+  /*We want to express od_sqrt() in terms of od_sqrt_norm(), which is
+     defined as (2^OUTSHIFT)*sqrt(t*(2^-INSHIFT)) with t=x*(2^-s).
+    This simplifies to 2^(OUTSHIFT-(INSHIFT/2)-(s/2))*sqrt(x), so the caller
+     needs to shift right by OUTSHIFT - INSHIFT/2 - s/2.*/
+  *sqrt_shift = OD_SQRT_OUTSHIFT - ((s + OD_SQRT_INSHIFT) >> 1);
+  return od_sqrt_norm(t);
+}
+#endif
+
+/** Gain expanding: raises gain to the power beta for activity masking.
+ *
+ * @param [in]  cg    companded gain
+ * @param [in]  q0    uncompanded quality parameter
+ * @param [in]  beta  activity masking beta param (exponent)
+ * @return            g^beta
+ */
+od_val32 od_gain_expand(od_val32 cg0, int q0, od_val16 beta) {
+  if (beta == OD_BETA(1)) {
+    /*The multiply fits into 28 bits because the expanded gain has a range from
+       0 to 2^20.*/
+    return OD_SHR_ROUND(cg0*q0, OD_CGAIN_SHIFT);
+  }
+  else if (beta == OD_BETA(1.5)) {
+#if defined(OD_FLOAT_PVQ)
+    double cg;
+    cg = cg0*OD_CGAIN_SCALE_1;
+    cg *= q0*OD_COMPAND_SCALE_1;
+    return OD_ROUND32(OD_COMPAND_SCALE*cg*sqrt(cg));
+#else
+    int32_t irt;
+    int64_t tmp;
+    int sqrt_inshift;
+    int sqrt_outshift;
+    /*cg0 is in Q(OD_CGAIN_SHIFT) and we need to divide it by
+       2^OD_COMPAND_SHIFT.*/
+    irt = od_sqrt(cg0*q0, &sqrt_outshift);
+    sqrt_inshift = (OD_CGAIN_SHIFT + OD_COMPAND_SHIFT) >> 1;
+    /*tmp is in Q(OD_CGAIN_SHIFT + OD_COMPAND_SHIFT).*/
+    tmp = cg0*q0*(int64_t)irt;
+    /*Expanded gain must be in Q(OD_COMPAND_SHIFT), thus OD_COMPAND_SHIFT is
+       not included here.*/
+    return OD_VSHR_ROUND(tmp, OD_CGAIN_SHIFT + sqrt_outshift + sqrt_inshift);
+#endif
+  }
+  else {
+#if defined(OD_FLOAT_PVQ)
+    /*Expanded gain must be in Q(OD_COMPAND_SHIFT), hence the multiply by
+       OD_COMPAND_SCALE.*/
+    double cg;
+    cg = cg0*OD_CGAIN_SCALE_1;
+    return OD_ROUND32(OD_COMPAND_SCALE*pow(cg*q0*OD_COMPAND_SCALE_1, beta));
+#else
+    int32_t expr;
+    int32_t cg;
+    cg = OD_SHR_ROUND(cg0*q0, OD_CGAIN_SHIFT);
+    expr = od_pow(cg, beta);
+    /*Expanded gain must be in Q(OD_COMPAND_SHIFT), hence the subtraction by
+       OD_COMPAND_SHIFT.*/
+    return OD_SHR_ROUND(expr, OD_EXP2_OUTSHIFT - OD_COMPAND_SHIFT);
+#endif
+  }
+}
+
+/** Computes the raw and quantized/companded gain of a given input
+ * vector
+ *
+ * @param [in]      x      vector of input data
+ * @param [in]      n      number of elements in vector x
+ * @param [in]      q0     quantizer
+ * @param [out]     g      raw gain
+ * @param [in]      beta   activity masking beta param
+ * @param [in]      bshift shift to be applied to raw gain
+ * @return                 quantized/companded gain
+ */
+od_val32 od_pvq_compute_gain(const od_val16 *x, int n, int q0, od_val32 *g,
+ od_val16 beta, int bshift) {
+  int i;
+  od_val32 acc;
+#if !defined(OD_FLOAT_PVQ)
+  od_val32 irt;
+  int sqrt_shift;
+#else
+  OD_UNUSED(bshift);
+#endif
+  acc = 0;
+  for (i = 0; i < n; i++) {
+    acc += x[i]*(od_val32)x[i];
+  }
+#if defined(OD_FLOAT_PVQ)
+  *g = sqrt(acc);
+#else
+  irt = od_sqrt(acc, &sqrt_shift);
+  *g = OD_VSHR_ROUND(irt, sqrt_shift - bshift);
+#endif
+  /* Normalize gain by quantization step size and apply companding
+     (if ACTIVITY != 1). */
+  return od_gain_compand(*g, q0, beta);
+}
+
+static od_val16 od_beta_rcp(od_val16 beta){
+  if (beta == OD_BETA(1.))
+    return OD_BETA(1.);
+  else if (beta == OD_BETA(1.5))
+    return OD_BETA(1./1.5);
+  else {
+    od_val16 rcp_beta;
+    /*Shift by 1 less, transposing beta to range [.5, .75] and thus < 32768.*/
+    rcp_beta = od_rcp(beta << (OD_RCP_INSHIFT - 1 - OD_BETA_SHIFT));
+    return OD_SHR_ROUND(rcp_beta, OD_RCP_OUTSHIFT + 1 - OD_BETA_SHIFT);
+  }
+}
+
+/** Compute theta quantization range from quantized/companded gain
+ *
+ * @param [in]      qcg    quantized companded gain value
+ * @param [in]      beta   activity masking beta param
+ * @return                 max theta value
+ */
+int od_pvq_compute_max_theta(od_val32 qcg, od_val16 beta){
+  /* Set angular resolution (in ra) to match the encoded gain */
+#if defined(OD_FLOAT_PVQ)
+  int ts = (int)floor(.5 + qcg*OD_CGAIN_SCALE_1*M_PI/(2*beta));
+#else
+  int ts = OD_SHR_ROUND(qcg*OD_MULT16_16_QBETA(OD_QCONST32(M_PI/2,
+   OD_CGAIN_SHIFT), od_beta_rcp(beta)), OD_CGAIN_SHIFT*2);
+#endif
+  /* Special case for low gains -- will need to be tuned anyway */
+  if (qcg < OD_QCONST32(1.4, OD_CGAIN_SHIFT)) ts = 1;
+  return ts;
+}
+
+/** Decode quantized theta value from coded value
+ *
+ * @param [in]      t          quantized companded gain value
+ * @param [in]      max_theta  maximum theta value
+ * @return                     decoded theta value
+ */
+od_val32 od_pvq_compute_theta(int t, int max_theta) {
+  if (max_theta != 0) {
+#if defined(OD_FLOAT_PVQ)
+    return OD_MINI(t, max_theta - 1)*.5*M_PI/max_theta;
+#else
+    return (OD_MAX_THETA_SCALE*OD_MINI(t, max_theta - 1)
+     + (max_theta >> 1))/max_theta;
+#endif
+  }
+  else return 0;
+}
+
+#define OD_ITHETA_SHIFT 15
+/** Compute the number of pulses used for PVQ encoding a vector from
+ * available metrics (encode and decode side)
+ *
+ * @param [in]      qcg        quantized companded gain value
+ * @param [in]      itheta     quantized PVQ error angle theta
+ * @param [in]      theta      PVQ error angle theta
+ * @param [in]      noref      indicates present or lack of reference
+ *                             (prediction)
+ * @param [in]      n          number of elements to be coded
+ * @param [in]      beta       activity masking beta param
+ * @param [in]      nodesync   do not use info that depends on the reference
+ * @return                     number of pulses to use for coding
+ */
+int od_pvq_compute_k(od_val32 qcg, int itheta, od_val32 theta, int noref, int n,
+ od_val16 beta, int nodesync) {
+  if (noref) {
+    if (qcg == 0) return 0;
+    if (n == 15 && qcg == OD_CGAIN_SCALE && beta > OD_BETA(1.25)) {
+      return 1;
+    }
+    else {
+#if defined(OD_FLOAT_PVQ)
+      return OD_MAXI(1, (int)floor(.5 + (qcg*OD_CGAIN_SCALE_1 - .2)*
+       sqrt((n + 3)/2)/beta));
+#else
+      od_val32 rt;
+      int sqrt_shift;
+      rt = od_sqrt((n + 3) >> 1, &sqrt_shift);
+      /*FIXME: get rid of 64-bit mul.*/
+      return OD_MAXI(1, OD_SHR_ROUND((int64_t)((qcg
+       - (int64_t)OD_QCONST32(.2, OD_CGAIN_SHIFT))*rt/(beta*OD_BETA_SCALE_1)),
+       OD_CGAIN_SHIFT + sqrt_shift));
+#endif
+    }
+  }
+  else {
+    if (itheta == 0) return 0;
+    /* Sets K according to gain and theta, based on the high-rate
+       PVQ distortion curves (see PVQ document). Low-rate will have to be
+       perceptually tuned anyway. We subtract 0.2 from the radius as an
+       approximation for the fact that the coefficients aren't identically
+       distributed within a band so at low gain the number of dimensions that
+       are likely to have a pulse is less than n. */
+    if (nodesync) {
+#if defined(OD_FLOAT_PVQ)
+      return OD_MAXI(1, (int)floor(.5 + (itheta - .2)*sqrt((n + 2)/2)));
+#else
+      od_val32 rt;
+      int sqrt_outshift;
+      rt = od_sqrt((n + 2)/2, &sqrt_outshift);
+      /*FIXME: get rid of 64-bit mul.*/
+      return OD_MAXI(1, OD_VSHR_ROUND(((OD_SHL(itheta, OD_ITHETA_SHIFT)
+       - OD_QCONST32(.2, OD_ITHETA_SHIFT)))*(int64_t)rt,
+       sqrt_outshift + OD_ITHETA_SHIFT));
+#endif
+    }
+    else {
+      return OD_MAXI(1, (int)floor(.5 + (qcg*OD_CGAIN_SCALE_1*
+       od_pvq_sin(theta)*OD_TRIG_SCALE_1 - .2)*sqrt((n
+       + 2)/2)/(beta*OD_BETA_SCALE_1)));
+    }
+  }
+}
+
+#if !defined(OD_FLOAT_PVQ)
+#define OD_RSQRT_INSHIFT 16
+#define OD_RSQRT_OUTSHIFT 14
+/** Reciprocal sqrt approximation where the input is in the range [0.25,1) in
+     Q16 and the output is in the range (1.0, 2.0] in Q14).
+    Error is always within +/1 of round(1/sqrt(t))*/
+static int16_t od_rsqrt_norm(int16_t t)
+{
+  int16_t n;
+  int32_t r;
+  int32_t r2;
+  int32_t ry;
+  int32_t y;
+  int32_t ret;
+  /* Range of n is [-16384,32767] ([-0.5,1) in Q15).*/
+  n = t - 32768;
+  OD_ASSERT(n >= -16384);
+  /*Get a rough initial guess for the root.
+    The optimal minimax quadratic approximation (using relative error) is
+     r = 1.437799046117536+n*(-0.823394375837328+n*0.4096419668459485).
+    Coefficients here, and the final result r, are Q14.*/
+  r = (23565 + OD_MULT16_16_Q15(n, (-13481 + OD_MULT16_16_Q15(n, 6711))));
+  /*We want y = t*r*r-1 in Q15, but t is 32-bit Q16 and r is Q14.
+    We can compute the result from n and r using Q15 multiplies with some
+     adjustment, carefully done to avoid overflow.*/
+  r2 = r*r;
+  y = (((r2 >> 15)*n + r2) >> 12) - 131077;
+  ry = r*y;
+  /*Apply a 2nd-order Householder iteration: r += r*y*(y*0.375-0.5).
+    This yields the Q14 reciprocal square root of the Q16 t, with a maximum
+     relative error of 1.04956E-4, a (relative) RMSE of 2.80979E-5, and a peak
+     absolute error of 2.26591/16384.*/
+  ret = r + ((((ry >> 16)*(3*y) >> 3) - ry) >> 18);
+  OD_ASSERT(ret >= 16384 && ret < 32768);
+  return (int16_t)ret;
+}
+
+static int16_t od_rsqrt(int32_t x, int *rsqrt_shift)
+{
+   int k;
+   int s;
+   int16_t t;
+   k = (OD_ILOG(x) - 1) >> 1;
+  /*t is x in the range [0.25, 1) in QINSHIFT, or x*2^(-s).
+    Shift by log2(x) - log2(0.25*(1 << INSHIFT)) to ensure 0.25 lower bound.*/
+   s = 2*k - (OD_RSQRT_INSHIFT - 2);
+   t = OD_VSHR(x, s);
+   /*We want to express od_rsqrt() in terms of od_rsqrt_norm(), which is
+      defined as (2^OUTSHIFT)/sqrt(t*(2^-INSHIFT)) with t=x*(2^-s).
+     This simplifies to 2^(OUTSHIFT+(INSHIFT/2)+(s/2))/sqrt(x), so the caller
+      needs to shift right by OUTSHIFT + INSHIFT/2 + s/2.*/
+   *rsqrt_shift = OD_RSQRT_OUTSHIFT + ((s + OD_RSQRT_INSHIFT) >> 1);
+   return od_rsqrt_norm(t);
+}
+#endif
+
+/** Synthesizes one parition of coefficient values from a PVQ-encoded
+ * vector.  This 'partial' version is called by the encode loop where
+ * the Householder reflection has already been computed and there's no
+ * need to recompute it.
+ *
+ * @param [out]     xcoeff  output coefficient partition (x in math doc)
+ * @param [in]      ypulse  PVQ-encoded values (y in the math doc); in
+ *                          the noref case, this vector has n entries,
+ *                          in the reference case it contains n-1 entries
+ *                          (the m-th entry is not included)
+ * @param [in]      r       reference vector (prediction)
+ * @param [in]      n       number of elements in this partition
+ * @param [in]      noref   indicates presence or lack of prediction
+ * @param [in]      g       decoded quantized vector gain
+ * @param [in]      theta   decoded theta (prediction error)
+ * @param [in]      m       alignment dimension of Householder reflection
+ * @param [in]      s       sign of Householder reflection
+ * @param [in]      qm_inv  inverse of the QM with magnitude compensation
+ */
+void od_pvq_synthesis_partial(od_coeff *xcoeff, const od_coeff *ypulse,
+ const od_val16 *r16, int n, int noref, od_val32 g, od_val32 theta, int m, int s,
+ const int16_t *qm_inv) {
+  int i;
+  int yy;
+  od_val32 scale;
+  int nn;
+  int gshift;
+  int qshift;
+  OD_ASSERT(g != 0);
+  nn = n-(!noref); /* when noref==0, vector in is sized n-1 */
+  yy = 0;
+  for (i = 0; i < nn; i++)
+    yy += ypulse[i]*(int32_t)ypulse[i];
+  /* Shift required for the magnitude of the pre-qm synthesis to be guaranteed
+     to fit in 16 bits. In practice, the range will be 8192-16384 after scaling
+     most of the time. */
+  gshift = OD_MAXI(0, OD_ILOG(g) - 14);
+  /*scale is g/sqrt(yy) in Q(16-gshift) so that x[]*scale has a norm that fits
+     in 16 bits.*/
+  if (yy == 0) scale = 0;
+#if defined(OD_FLOAT_PVQ)
+  else {
+    scale = g/sqrt(yy);
+  }
+  OD_UNUSED(gshift);
+  OD_UNUSED(qshift);
+#else
+  else {
+    int rsqrt_shift;
+    int16_t rsqrt;
+    /*FIXME: should be < int64_t*/
+    int64_t tmp;
+    rsqrt = od_rsqrt(yy, &rsqrt_shift);
+    tmp = rsqrt*(int64_t)g;
+    scale = OD_VSHR_ROUND(tmp, rsqrt_shift + gshift - 16);
+  }
+  /* Shift to apply after multiplying by the inverse QM, taking into account
+     gshift. */
+  qshift = OD_QM_INV_SHIFT - gshift;
+#endif
+  if (noref) {
+    for (i = 0; i < n; i++) {
+      od_val32 x;
+      /* This multiply doesn't round, so it introduces some bias.
+         It would be nice (but not critical) to fix this. */
+      x = OD_MULT16_32_Q16(ypulse[i], scale);
+#if defined(OD_FLOAT_PVQ)
+      xcoeff[i] = (od_coeff)floor(.5
+       + x*(qm_inv[i]*OD_QM_INV_SCALE_1));
+#else
+      xcoeff[i] = OD_SHR_ROUND(x*qm_inv[i], qshift);
+#endif
+    }
+  }
+  else{
+    od_val16 x[MAXN];
+    scale = OD_ROUND32(scale*OD_TRIG_SCALE_1*od_pvq_sin(theta));
+    /* The following multiply doesn't round, but it's probably OK since
+       the Householder reflection is likely to undo most of the resulting
+       bias. */
+    for (i = 0; i < m; i++)
+      x[i] = OD_MULT16_32_Q16(ypulse[i], scale);
+    x[m] = OD_ROUND16(-s*(OD_SHR_ROUND(g, gshift))*OD_TRIG_SCALE_1*
+     od_pvq_cos(theta));
+    for (i = m; i < nn; i++)
+      x[i+1] = OD_MULT16_32_Q16(ypulse[i], scale);
+    od_apply_householder(x, x, r16, n);
+    for (i = 0; i < n; i++) {
+#if defined(OD_FLOAT_PVQ)
+      xcoeff[i] = (od_coeff)floor(.5 + (x[i]*(qm_inv[i]*OD_QM_INV_SCALE_1)));
+#else
+      xcoeff[i] = OD_SHR_ROUND(x[i]*qm_inv[i], qshift);
+#endif
+    }
+  }
+}
diff --git a/av1/common/pvq.h b/av1/common/pvq.h
new file mode 100644
index 0000000..e0fa7d9
--- /dev/null
+++ b/av1/common/pvq.h
@@ -0,0 +1,181 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#if !defined(_pvq_H)
+# define _pvq_H (1)
+# include "generic_code.h"
+# include "odintrin.h"
+
+extern const double *OD_BASIS_MAG[2][OD_NBSIZES + 1];
+extern const int OD_QM8_Q4_FLAT[];
+extern const int OD_QM8_Q4_HVS[];
+
+extern const uint16_t EXP_CDF_TABLE[][16];
+extern const uint16_t LAPLACE_OFFSET[];
+
+# define PVQ_MAX_PARTITIONS (1 + 3*(OD_NBSIZES-1))
+
+# define OD_NOREF_ADAPT_SPEED (4)
+/* Normalized lambda for PVQ quantizer. Since we normalize the gain by q, the
+   distortion is normalized by q^2 and lambda does not need the q^2 factor.
+   At high rate, this would be log(2)/6, but we're using a slightly more
+   aggressive value, closer to:
+   Li, Xiang, et al. "Laplace distribution based Lagrangian rate distortion
+   optimization for hybrid video coding." Circuits and Systems for Video
+   Technology, IEEE Transactions on 19.2 (2009): 193-205.
+   */
+# define OD_PVQ_LAMBDA (.1146)
+
+#define OD_PVQ_SKIP_ZERO 1
+#define OD_PVQ_SKIP_COPY 2
+
+/* Maximum size for coding a PVQ band. */
+#define OD_MAX_PVQ_SIZE (1024)
+
+#if defined(OD_FLOAT_PVQ)
+#define OD_QM_SHIFT (15)
+#else
+#define OD_QM_SHIFT (11)
+#endif
+#define OD_QM_SCALE (1 << OD_QM_SHIFT)
+#if defined(OD_FLOAT_PVQ)
+#define OD_QM_SCALE_1 (1./OD_QM_SCALE)
+#endif
+#define OD_QM_SCALE_MAX 32767
+#define OD_QM_INV_SHIFT (12)
+#define OD_QM_INV_SCALE (1 << OD_QM_INV_SHIFT)
+#if defined(OD_FLOAT_PVQ)
+#define OD_QM_INV_SCALE_1 (1./OD_QM_INV_SCALE)
+#endif
+#define OD_QM_OFFSET(bs) ((((1 << 2*bs) - 1) << 2*OD_LOG_BSIZE0)/3)
+#define OD_QM_STRIDE (OD_QM_OFFSET(OD_NBSIZES))
+#define OD_QM_BUFFER_SIZE (2*OD_QM_STRIDE)
+
+#if !defined(OD_FLOAT_PVQ)
+#define OD_THETA_SHIFT (15)
+#define OD_THETA_SCALE ((1 << OD_THETA_SHIFT)*2./M_PI)
+#define OD_MAX_THETA_SCALE (1 << OD_THETA_SHIFT)
+#define OD_TRIG_SCALE (32768)
+#define OD_BETA_SHIFT (12)
+#define OD_BETA_SCALE_1 (1./(1 << OD_BETA_SHIFT))
+/*Multiplies 16-bit a by 32-bit b and keeps bits [16:64-OD_BETA_SHIFT-1].*/
+#define OD_MULT16_32_QBETA(a, b) \
+ ((int16_t)(a)*(int64_t)(int32_t)(b) >> OD_BETA_SHIFT)
+# define OD_MULT16_16_QBETA(a, b) \
+  ((((int16_t)(a))*((int32_t)(int16_t)(b))) >> OD_BETA_SHIFT)
+#define OD_CGAIN_SHIFT (8)
+#define OD_CGAIN_SCALE (1 << OD_CGAIN_SHIFT)
+#else
+#define OD_BETA_SCALE_1 (1.)
+#define OD_THETA_SCALE (1)
+#define OD_TRIG_SCALE (1)
+#define OD_CGAIN_SCALE (1)
+#endif
+#define OD_THETA_SCALE_1 (1./OD_THETA_SCALE)
+#define OD_TRIG_SCALE_1 (1./OD_TRIG_SCALE)
+#define OD_CGAIN_SCALE_1 (1./OD_CGAIN_SCALE)
+#define OD_CGAIN_SCALE_2 (OD_CGAIN_SCALE_1*OD_CGAIN_SCALE_1)
+
+/* Largest PVQ partition is half the coefficients of largest block size. */
+#define MAXN (OD_BSIZE_MAX*OD_BSIZE_MAX/2)
+
+#define OD_COMPAND_SHIFT (8 + OD_COEFF_SHIFT)
+#define OD_COMPAND_SCALE (1 << OD_COMPAND_SHIFT)
+#define OD_COMPAND_SCALE_1 (1./OD_COMPAND_SCALE)
+
+#define OD_QM_SIZE (OD_NBSIZES*(OD_NBSIZES + 1))
+
+#define OD_FLAT_QM 0
+#define OD_HVS_QM  1
+
+# define OD_NSB_ADAPT_CTXS (4)
+
+# define OD_ADAPT_K_Q8        0
+# define OD_ADAPT_SUM_EX_Q8   1
+# define OD_ADAPT_COUNT_Q8    2
+# define OD_ADAPT_COUNT_EX_Q8 3
+
+# define OD_ADAPT_NO_VALUE (-2147483647-1)
+
+typedef struct od_pvq_adapt_ctx  od_pvq_adapt_ctx;
+typedef struct od_pvq_codeword_ctx od_pvq_codeword_ctx;
+
+struct od_pvq_codeword_ctx {
+  int                 pvq_adapt[2*OD_NBSIZES*OD_NSB_ADAPT_CTXS];
+  int                 pvq_k1_increment;
+  /* CDFs are size 16 despite the fact that we're using less than that. */
+  uint16_t            pvq_k1_cdf[12][16];
+  uint16_t            pvq_split_cdf[22*7][8];
+  int                 pvq_split_increment;
+};
+
+struct od_pvq_adapt_ctx {
+  od_pvq_codeword_ctx pvq_codeword_ctx;
+  generic_encoder     pvq_param_model[3];
+  int                 pvq_ext[OD_NBSIZES*PVQ_MAX_PARTITIONS];
+  int                 pvq_exg[OD_NPLANES_MAX][OD_NBSIZES][PVQ_MAX_PARTITIONS];
+  int                 pvq_gaintheta_increment;
+  uint16_t        pvq_gaintheta_cdf[2*OD_NBSIZES*PVQ_MAX_PARTITIONS][16];
+  int                 pvq_skip_dir_increment;
+  uint16_t        pvq_skip_dir_cdf[2*(OD_NBSIZES-1)][7];
+};
+
+void od_adapt_pvq_ctx_reset(od_pvq_adapt_ctx *state, int is_keyframe);
+int od_pvq_size_ctx(int n);
+int od_pvq_k1_ctx(int n, int orig_size);
+
+od_val16 od_pvq_sin(od_val32 x);
+od_val16 od_pvq_cos(od_val32 x);
+#if !defined(OD_FLOAT_PVQ)
+int od_vector_log_mag(const od_coeff *x, int n);
+#endif
+
+int od_qm_get_index(int bs, int band);
+
+extern const od_val16 *const OD_PVQ_BETA[2][OD_NPLANES_MAX][OD_NBSIZES + 1];
+
+void od_init_qm(int16_t *x, int16_t *x_inv, const int *qm);
+int od_compute_householder(od_val16 *r, int n, od_val32 gr, int *sign,
+ int shift);
+void od_apply_householder(od_val16 *out, const od_val16 *x, const od_val16 *r,
+ int n);
+void od_pvq_synthesis_partial(od_coeff *xcoeff, const od_coeff *ypulse,
+                                  const od_val16 *r, int n,
+                                  int noref, od_val32 g,
+                                  od_val32 theta, int m, int s,
+                                  const int16_t *qm_inv);
+od_val32 od_gain_expand(od_val32 cg, int q0, od_val16 beta);
+od_val32 od_pvq_compute_gain(const od_val16 *x, int n, int q0, od_val32 *g,
+ od_val16 beta, int bshift);
+int od_pvq_compute_max_theta(od_val32 qcg, od_val16 beta);
+od_val32 od_pvq_compute_theta(int t, int max_theta);
+int od_pvq_compute_k(od_val32 qcg, int itheta, od_val32 theta, int noref,
+ int n, od_val16 beta, int nodesync);
+
+int od_vector_is_null(const od_coeff *x, int len);
+int od_qm_offset(int bs, int xydec);
+
+#endif
diff --git a/av1/common/state.c b/av1/common/state.c
new file mode 100644
index 0000000..b370c94
--- /dev/null
+++ b/av1/common/state.c
@@ -0,0 +1,42 @@
+#include "av1/common/state.h"
+#include "av1/common/odintrin.h"
+
+void od_adapt_ctx_reset(od_adapt_ctx *adapt, int is_keyframe) {
+  int i;
+  int pli;
+  od_adapt_pvq_ctx_reset(&adapt->pvq, is_keyframe);
+  adapt->skip_increment = 128;
+  OD_CDFS_INIT(adapt->skip_cdf, adapt->skip_increment >> 2);
+  for (pli = 0; pli < OD_NPLANES_MAX; pli++) {
+    generic_model_init(&adapt->model_dc[pli]);
+    for (i = 0; i < OD_NBSIZES; i++) {
+      adapt->ex_g[pli][i] = 8;
+    }
+    for (i = 0; i < 4; i++) {
+      int j;
+      for (j = 0; j < 3; j++) {
+        adapt->ex_dc[pli][i][j] = pli > 0 ? 8 : 32768;
+      }
+    }
+  }
+}
+
+void od_init_skipped_coeffs(int16_t *d, int16_t *pred, int is_keyframe, int bo,
+                            int n, int w) {
+  int i;
+  int j;
+  if (is_keyframe) {
+    for (i = 0; i < n; i++) {
+      for (j = 0; j < n; j++) {
+        /* skip DC */
+        if (i || j) d[bo + i * w + j] = 0;
+      }
+    }
+  } else {
+    for (i = 0; i < n; i++) {
+      for (j = 0; j < n; j++) {
+        d[bo + i * w + j] = pred[i * n + j];
+      }
+    }
+  }
+}
diff --git a/av1/common/state.h b/av1/common/state.h
new file mode 100644
index 0000000..c654538
--- /dev/null
+++ b/av1/common/state.h
@@ -0,0 +1,66 @@
+/*Daala video codec
+Copyright (c) 2006-2010 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#if !defined(_state_H)
+# define _state_H (1)
+
+typedef struct od_state     od_state;
+typedef struct od_adapt_ctx od_adapt_ctx;
+
+# include "generic_code.h"
+# include "odintrin.h"
+# include "pvq.h"
+# include <stdio.h>
+
+/*Adaptation speed of scalar Laplace encoding.*/
+# define OD_SCALAR_ADAPT_SPEED (4)
+
+struct od_adapt_ctx {
+  /* Support for PVQ encode/decode */
+  od_pvq_adapt_ctx pvq;
+
+  generic_encoder model_dc[OD_NPLANES_MAX];
+
+  int ex_dc[OD_NPLANES_MAX][OD_NBSIZES][3];
+  int ex_g[OD_NPLANES_MAX][OD_NBSIZES];
+
+  /* Joint skip flag for DC and AC */
+  uint16_t skip_cdf[OD_NBSIZES*2][4];
+  int skip_increment;
+};
+
+struct od_state {
+  od_adapt_ctx adapt;
+  //unsigned char pvq_qm_q4[OD_NPLANES_MAX][OD_QM_SIZE];
+  int16_t *qm;
+  int16_t *qm_inv;
+};
+
+void od_adapt_ctx_reset(od_adapt_ctx *state, int is_keyframe);
+void od_init_skipped_coeffs(int16_t *d, int16_t *pred, int is_keyframe,
+ int bo, int n, int w);
+
+#endif
diff --git a/av1/common/zigzag.h b/av1/common/zigzag.h
new file mode 100644
index 0000000..c9be8fa
--- /dev/null
+++ b/av1/common/zigzag.h
@@ -0,0 +1,49 @@
+/*Daala video codec
+Copyright (c) 2015 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#if !defined(_zigzag_H)
+# define _zigzag_H (1)
+
+extern const unsigned char OD_ZIGZAG4_DCT_DCT[15][2];
+extern const unsigned char OD_ZIGZAG4_ADST_DCT[15][2];
+extern const unsigned char OD_ZIGZAG4_DCT_ADST[15][2];
+extern const unsigned char OD_ZIGZAG4_ADST_ADST[15][2];
+
+extern const unsigned char OD_ZIGZAG8_DCT_DCT[48][2];
+extern const unsigned char OD_ZIGZAG8_ADST_DCT[48][2];
+extern const unsigned char OD_ZIGZAG8_DCT_ADST[48][2];
+extern const unsigned char OD_ZIGZAG8_ADST_ADST[48][2];
+
+extern const unsigned char OD_ZIGZAG16_DCT_DCT[192][2];
+extern const unsigned char OD_ZIGZAG16_ADST_DCT[192][2];
+extern const unsigned char OD_ZIGZAG16_DCT_ADST[192][2];
+extern const unsigned char OD_ZIGZAG16_ADST_ADST[192][2];
+
+extern const unsigned char OD_ZIGZAG32_DCT_DCT[768][2];
+
+extern const unsigned char OD_ZIGZAG64_DCT_DCT[3072][2];
+
+#endif
diff --git a/av1/common/zigzag16.c b/av1/common/zigzag16.c
new file mode 100644
index 0000000..94c3487
--- /dev/null
+++ b/av1/common/zigzag16.c
@@ -0,0 +1,208 @@
+/* This file is generated by gen_zigzag16.m */
+
+/* clang-format off */
+
+#include "odintrin.h"
+OD_EXTERN const unsigned char OD_ZIGZAG16_DCT_DCT[192][2] = {
+  {8, 0}, {8, 1}, {8, 2}, {9, 0},
+  {8, 3}, {9, 1}, {9, 2}, {10, 0},
+  {9, 3}, {10, 1}, {10, 2}, {11, 0},
+  {10, 3}, {11, 1}, {11, 2}, {11, 3},
+  {12, 0}, {12, 1}, {13, 0}, {12, 2},
+  {12, 3}, {13, 1}, {13, 2}, {14, 0},
+  {13, 3}, {14, 1}, {15, 0}, {14, 2},
+  {14, 3}, {15, 1}, {15, 2}, {15, 3},
+  {0, 8}, {1, 8}, {0, 9}, {2, 8},
+  {1, 9}, {3, 8}, {0, 10}, {2, 9},
+  {1, 10}, {3, 9}, {0, 11}, {2, 10},
+  {1, 11}, {3, 10}, {0, 12}, {2, 11},
+  {1, 12}, {3, 11}, {0, 13}, {2, 12},
+  {1, 13}, {0, 14}, {3, 12}, {2, 13},
+  {1, 14}, {3, 13}, {0, 15}, {2, 14},
+  {1, 15}, {3, 14}, {2, 15}, {3, 15},
+  {4, 8}, {5, 8}, {4, 9}, {8, 4},
+  {8, 5}, {6, 8}, {5, 9}, {4, 10},
+  {9, 4}, {8, 6}, {7, 8}, {9, 5},
+  {5, 10}, {8, 7}, {6, 9}, {4, 11},
+  {10, 4}, {9, 6}, {7, 9}, {8, 8},
+  {10, 5}, {6, 10}, {5, 11}, {9, 7},
+  {8, 9}, {10, 6}, {7, 10}, {4, 12},
+  {11, 4}, {9, 8}, {6, 11}, {10, 7},
+  {11, 5}, {5, 12}, {8, 10}, {7, 11},
+  {9, 9}, {4, 13}, {10, 8}, {11, 6},
+  {11, 7}, {6, 12}, {8, 11}, {9, 10},
+  {12, 4}, {5, 13}, {10, 9}, {12, 5},
+  {7, 12}, {11, 8}, {4, 14}, {6, 13},
+  {10, 10}, {9, 11}, {12, 6}, {13, 4},
+  {11, 9}, {8, 12}, {5, 14}, {12, 7},
+  {7, 13}, {4, 15}, {13, 5}, {10, 11},
+  {11, 10}, {9, 12}, {13, 6}, {12, 8},
+  {6, 14}, {8, 13}, {5, 15}, {13, 7},
+  {14, 4}, {12, 9}, {7, 14}, {11, 11},
+  {10, 12}, {9, 13}, {14, 5}, {6, 15},
+  {13, 8}, {8, 14}, {12, 10}, {14, 6},
+  {7, 15}, {13, 9}, {15, 4}, {10, 13},
+  {11, 12}, {14, 7}, {9, 14}, {12, 11},
+  {8, 15}, {15, 5}, {13, 10}, {14, 8},
+  {11, 13}, {15, 6}, {9, 15}, {10, 14},
+  {14, 9}, {15, 7}, {13, 11}, {12, 12},
+  {10, 15}, {11, 14}, {15, 8}, {14, 10},
+  {12, 13}, {13, 12}, {15, 9}, {11, 15},
+  {14, 11}, {13, 13}, {15, 10}, {12, 14},
+  {13, 14}, {15, 11}, {14, 12}, {12, 15},
+  {14, 13}, {13, 15}, {15, 12}, {14, 14},
+  {15, 13}, {14, 15}, {15, 14}, {15, 15}
+  };
+
+OD_EXTERN const unsigned char OD_ZIGZAG16_ADST_DCT[192][2] = {
+  {8, 0}, {9, 0}, {10, 0}, {8, 1},
+  {11, 0}, {9, 1}, {8, 2}, {12, 0},
+  {10, 1}, {9, 2}, {8, 3}, {13, 0},
+  {11, 1}, {10, 2}, {9, 3}, {14, 0},
+  {12, 1}, {10, 3}, {15, 0}, {11, 2},
+  {13, 1}, {11, 3}, {12, 2}, {14, 1},
+  {12, 3}, {13, 2}, {15, 1}, {13, 3},
+  {14, 2}, {14, 3}, {15, 2}, {15, 3},
+  {0, 8}, {1, 8}, {2, 8}, {0, 9},
+  {3, 8}, {1, 9}, {2, 9}, {0, 10},
+  {3, 9}, {1, 10}, {2, 10}, {0, 11},
+  {3, 10}, {1, 11}, {2, 11}, {0, 12},
+  {3, 11}, {1, 12}, {2, 12}, {0, 13},
+  {3, 12}, {1, 13}, {0, 14}, {2, 13},
+  {0, 15}, {1, 14}, {3, 13}, {2, 14},
+  {1, 15}, {3, 14}, {2, 15}, {3, 15},
+  {8, 4}, {9, 4}, {8, 5}, {4, 8},
+  {10, 4}, {9, 5}, {5, 8}, {8, 6},
+  {4, 9}, {10, 5}, {9, 6}, {6, 8},
+  {8, 7}, {11, 4}, {7, 8}, {5, 9},
+  {9, 7}, {11, 5}, {10, 6}, {4, 10},
+  {6, 9}, {8, 8}, {5, 10}, {7, 9},
+  {12, 4}, {10, 7}, {9, 8}, {11, 6},
+  {8, 9}, {4, 11}, {6, 10}, {7, 10},
+  {12, 5}, {5, 11}, {10, 8}, {11, 7},
+  {9, 9}, {4, 12}, {13, 4}, {8, 10},
+  {6, 11}, {12, 6}, {5, 12}, {10, 9},
+  {7, 11}, {9, 10}, {11, 8}, {13, 5},
+  {8, 11}, {4, 13}, {6, 12}, {10, 10},
+  {12, 7}, {11, 9}, {7, 12}, {14, 4},
+  {5, 13}, {9, 11}, {13, 6}, {8, 12},
+  {4, 14}, {12, 8}, {6, 13}, {11, 10},
+  {10, 11}, {12, 9}, {5, 14}, {13, 7},
+  {14, 5}, {9, 12}, {4, 15}, {7, 13},
+  {8, 13}, {6, 14}, {13, 8}, {11, 11},
+  {10, 12}, {15, 4}, {12, 10}, {14, 6},
+  {13, 9}, {5, 15}, {9, 13}, {7, 14},
+  {15, 5}, {6, 15}, {8, 14}, {14, 7},
+  {11, 12}, {7, 15}, {9, 14}, {13, 10},
+  {10, 13}, {14, 8}, {15, 6}, {14, 9},
+  {12, 11}, {8, 15}, {15, 7}, {10, 14},
+  {11, 13}, {9, 15}, {13, 11}, {12, 12},
+  {15, 8}, {14, 10}, {15, 9}, {10, 15},
+  {11, 14}, {13, 12}, {12, 13}, {15, 10},
+  {14, 11}, {11, 15}, {13, 13}, {15, 11},
+  {14, 12}, {12, 14}, {15, 12}, {13, 14},
+  {12, 15}, {14, 13}, {13, 15}, {15, 13},
+  {14, 14}, {15, 14}, {14, 15}, {15, 15}
+  };
+
+OD_EXTERN const unsigned char OD_ZIGZAG16_DCT_ADST[192][2] = {
+  {8, 0}, {8, 1}, {8, 2}, {8, 3},
+  {9, 0}, {9, 1}, {9, 2}, {9, 3},
+  {10, 0}, {10, 1}, {10, 2}, {10, 3},
+  {11, 0}, {11, 1}, {11, 2}, {11, 3},
+  {12, 0}, {12, 1}, {12, 2}, {12, 3},
+  {13, 0}, {13, 1}, {13, 2}, {13, 3},
+  {14, 0}, {15, 0}, {14, 1}, {14, 2},
+  {14, 3}, {15, 1}, {15, 2}, {15, 3},
+  {0, 8}, {0, 9}, {0, 10}, {1, 8},
+  {0, 11}, {1, 9}, {2, 8}, {0, 12},
+  {1, 10}, {2, 9}, {0, 13}, {1, 11},
+  {3, 8}, {2, 10}, {0, 14}, {1, 12},
+  {3, 9}, {0, 15}, {2, 11}, {3, 10},
+  {1, 13}, {2, 12}, {3, 11}, {1, 14},
+  {2, 13}, {1, 15}, {3, 12}, {2, 14},
+  {3, 13}, {2, 15}, {3, 14}, {3, 15},
+  {4, 8}, {4, 9}, {5, 8}, {4, 10},
+  {5, 9}, {4, 11}, {6, 8}, {5, 10},
+  {8, 4}, {6, 9}, {4, 12}, {5, 11},
+  {8, 5}, {6, 10}, {7, 8}, {8, 6},
+  {4, 13}, {7, 9}, {5, 12}, {8, 7},
+  {9, 4}, {6, 11}, {8, 8}, {7, 10},
+  {5, 13}, {9, 5}, {4, 14}, {9, 6},
+  {8, 9}, {6, 12}, {9, 7}, {7, 11},
+  {4, 15}, {8, 10}, {9, 8}, {5, 14},
+  {10, 4}, {6, 13}, {10, 5}, {9, 9},
+  {7, 12}, {8, 11}, {10, 6}, {5, 15},
+  {10, 7}, {6, 14}, {9, 10}, {7, 13},
+  {8, 12}, {10, 8}, {9, 11}, {6, 15},
+  {11, 4}, {11, 5}, {10, 9}, {8, 13},
+  {7, 14}, {11, 6}, {9, 12}, {11, 7},
+  {10, 10}, {7, 15}, {8, 14}, {12, 4},
+  {11, 8}, {12, 5}, {9, 13}, {10, 11},
+  {8, 15}, {11, 9}, {12, 6}, {12, 7},
+  {10, 12}, {9, 14}, {11, 10}, {13, 4},
+  {12, 8}, {9, 15}, {13, 5}, {11, 11},
+  {12, 9}, {10, 13}, {13, 6}, {13, 7},
+  {12, 10}, {14, 4}, {11, 12}, {13, 8},
+  {10, 14}, {14, 5}, {12, 11}, {13, 9},
+  {14, 6}, {10, 15}, {11, 13}, {15, 4},
+  {14, 7}, {12, 12}, {13, 10}, {14, 8},
+  {15, 5}, {13, 11}, {15, 6}, {11, 14},
+  {14, 9}, {12, 13}, {11, 15}, {15, 7},
+  {14, 10}, {15, 8}, {13, 12}, {12, 14},
+  {15, 9}, {14, 11}, {13, 13}, {12, 15},
+  {15, 10}, {14, 12}, {13, 14}, {15, 11},
+  {13, 15}, {14, 13}, {14, 14}, {15, 12},
+  {14, 15}, {15, 13}, {15, 14}, {15, 15}
+  };
+
+OD_EXTERN const unsigned char OD_ZIGZAG16_ADST_ADST[192][2] = {
+  {8, 0}, {8, 1}, {8, 2}, {9, 0},
+  {8, 3}, {9, 1}, {9, 2}, {10, 0},
+  {9, 3}, {10, 1}, {10, 2}, {11, 0},
+  {10, 3}, {11, 1}, {11, 2}, {11, 3},
+  {12, 0}, {12, 1}, {13, 0}, {12, 2},
+  {12, 3}, {13, 1}, {13, 2}, {14, 0},
+  {13, 3}, {14, 1}, {15, 0}, {14, 2},
+  {14, 3}, {15, 1}, {15, 2}, {15, 3},
+  {0, 8}, {1, 8}, {0, 9}, {2, 8},
+  {1, 9}, {3, 8}, {0, 10}, {2, 9},
+  {1, 10}, {3, 9}, {0, 11}, {2, 10},
+  {1, 11}, {3, 10}, {0, 12}, {2, 11},
+  {1, 12}, {3, 11}, {0, 13}, {2, 12},
+  {1, 13}, {0, 14}, {3, 12}, {2, 13},
+  {1, 14}, {3, 13}, {0, 15}, {2, 14},
+  {1, 15}, {3, 14}, {2, 15}, {3, 15},
+  {4, 8}, {5, 8}, {4, 9}, {8, 4},
+  {8, 5}, {6, 8}, {5, 9}, {4, 10},
+  {9, 4}, {8, 6}, {7, 8}, {9, 5},
+  {5, 10}, {8, 7}, {6, 9}, {4, 11},
+  {10, 4}, {9, 6}, {7, 9}, {8, 8},
+  {10, 5}, {6, 10}, {5, 11}, {9, 7},
+  {8, 9}, {10, 6}, {7, 10}, {4, 12},
+  {11, 4}, {9, 8}, {6, 11}, {10, 7},
+  {11, 5}, {5, 12}, {8, 10}, {7, 11},
+  {9, 9}, {4, 13}, {10, 8}, {11, 6},
+  {11, 7}, {6, 12}, {8, 11}, {9, 10},
+  {12, 4}, {5, 13}, {10, 9}, {12, 5},
+  {7, 12}, {11, 8}, {4, 14}, {6, 13},
+  {10, 10}, {9, 11}, {12, 6}, {13, 4},
+  {11, 9}, {8, 12}, {5, 14}, {12, 7},
+  {7, 13}, {4, 15}, {13, 5}, {10, 11},
+  {11, 10}, {9, 12}, {13, 6}, {12, 8},
+  {6, 14}, {8, 13}, {5, 15}, {13, 7},
+  {14, 4}, {12, 9}, {7, 14}, {11, 11},
+  {10, 12}, {9, 13}, {14, 5}, {6, 15},
+  {13, 8}, {8, 14}, {12, 10}, {14, 6},
+  {7, 15}, {13, 9}, {15, 4}, {10, 13},
+  {11, 12}, {14, 7}, {9, 14}, {12, 11},
+  {8, 15}, {15, 5}, {13, 10}, {14, 8},
+  {11, 13}, {15, 6}, {9, 15}, {10, 14},
+  {14, 9}, {15, 7}, {13, 11}, {12, 12},
+  {10, 15}, {11, 14}, {15, 8}, {14, 10},
+  {12, 13}, {13, 12}, {15, 9}, {11, 15},
+  {14, 11}, {13, 13}, {15, 10}, {12, 14},
+  {13, 14}, {15, 11}, {14, 12}, {12, 15},
+  {14, 13}, {13, 15}, {15, 12}, {14, 14},
+  {15, 13}, {14, 15}, {15, 14}, {15, 15}
+  };
diff --git a/av1/common/zigzag32.c b/av1/common/zigzag32.c
new file mode 100644
index 0000000..cb3b9bc
--- /dev/null
+++ b/av1/common/zigzag32.c
@@ -0,0 +1,199 @@
+/* This file is generated by gen_zigzag32.m */
+
+/* clang-format off */
+
+#include "odintrin.h"
+OD_EXTERN const unsigned char OD_ZIGZAG32_DCT_DCT[768][2] = {
+  { 16, 0 }, { 17, 0 }, { 18, 0 }, { 19, 0 },
+  { 16, 1 }, { 17, 1 }, { 20, 0 }, { 16, 2 },
+  { 18, 1 }, { 21, 0 }, { 17, 2 }, { 16, 3 },
+  { 19, 1 }, { 22, 0 }, { 18, 2 }, { 17, 3 },
+  { 20, 1 }, { 16, 4 }, { 23, 0 }, { 19, 2 },
+  { 24, 0 }, { 16, 5 }, { 21, 1 }, { 17, 4 },
+  { 18, 3 }, { 20, 2 }, { 17, 5 }, { 16, 6 },
+  { 19, 3 }, { 18, 4 }, { 25, 0 }, { 22, 1 },
+  { 16, 7 }, { 21, 2 }, { 17, 6 }, { 20, 3 },
+  { 26, 0 }, { 18, 5 }, { 19, 4 }, { 17, 7 },
+  { 23, 1 }, { 22, 2 }, { 18, 6 }, { 27, 0 },
+  { 19, 5 }, { 24, 1 }, { 21, 3 }, { 28, 0 },
+  { 20, 4 }, { 18, 7 }, { 19, 6 }, { 23, 2 },
+  { 29, 0 }, { 25, 1 }, { 21, 4 }, { 30, 0 },
+  { 20, 5 }, { 22, 3 }, { 31, 0 }, { 19, 7 },
+  { 24, 2 }, { 26, 1 }, { 20, 6 }, { 21, 5 },
+  { 22, 4 }, { 23, 3 }, { 27, 1 }, { 25, 2 },
+  { 20, 7 }, { 28, 1 }, { 24, 3 }, { 21, 6 },
+  { 22, 5 }, { 23, 4 }, { 26, 2 }, { 21, 7 },
+  { 29, 1 }, { 25, 3 }, { 30, 1 }, { 27, 2 },
+  { 22, 6 }, { 23, 5 }, { 31, 1 }, { 24, 4 },
+  { 26, 3 }, { 28, 2 }, { 22, 7 }, { 23, 6 },
+  { 25, 4 }, { 24, 5 }, { 29, 2 }, { 30, 2 },
+  { 27, 3 }, { 23, 7 }, { 31, 2 }, { 24, 6 },
+  { 26, 4 }, { 25, 5 }, { 28, 3 }, { 24, 7 },
+  { 27, 4 }, { 29, 3 }, { 25, 6 }, { 26, 5 },
+  { 30, 3 }, { 31, 3 }, { 28, 4 }, { 27, 5 },
+  { 25, 7 }, { 29, 4 }, { 26, 6 }, { 28, 5 },
+  { 30, 4 }, { 26, 7 }, { 27, 6 }, { 31, 4 },
+  { 29, 5 }, { 27, 7 }, { 30, 5 }, { 28, 6 },
+  { 31, 5 }, { 29, 6 }, { 28, 7 }, { 30, 6 },
+  { 31, 6 }, { 29, 7 }, { 30, 7 }, { 31, 7 },
+  { 0, 16 }, { 0, 17 }, { 1, 16 }, { 0, 18 },
+  { 1, 17 }, { 0, 19 }, { 2, 16 }, { 1, 18 },
+  { 0, 20 }, { 2, 17 }, { 3, 16 }, { 1, 19 },
+  { 2, 18 }, { 0, 21 }, { 3, 17 }, { 4, 16 },
+  { 1, 20 }, { 2, 19 }, { 0, 22 }, { 3, 18 },
+  { 4, 17 }, { 5, 16 }, { 0, 23 }, { 3, 19 },
+  { 2, 20 }, { 1, 21 }, { 4, 18 }, { 6, 16 },
+  { 5, 17 }, { 3, 20 }, { 2, 21 }, { 1, 22 },
+  { 0, 24 }, { 0, 25 }, { 4, 19 }, { 7, 16 },
+  { 6, 17 }, { 5, 18 }, { 0, 26 }, { 3, 21 },
+  { 2, 22 }, { 1, 23 }, { 4, 20 }, { 5, 19 },
+  { 6, 18 }, { 1, 24 }, { 7, 17 }, { 0, 27 },
+  { 2, 23 }, { 3, 22 }, { 4, 21 }, { 1, 25 },
+  { 5, 20 }, { 7, 18 }, { 0, 28 }, { 6, 19 },
+  { 2, 24 }, { 1, 26 }, { 0, 29 }, { 4, 22 },
+  { 3, 23 }, { 2, 25 }, { 5, 21 }, { 0, 31 },
+  { 7, 19 }, { 6, 20 }, { 0, 30 }, { 1, 27 },
+  { 3, 24 }, { 2, 26 }, { 4, 23 }, { 5, 22 },
+  { 7, 20 }, { 1, 28 }, { 6, 21 }, { 3, 25 },
+  { 2, 27 }, { 1, 29 }, { 4, 24 }, { 2, 28 },
+  { 1, 30 }, { 7, 21 }, { 5, 23 }, { 3, 26 },
+  { 6, 22 }, { 1, 31 }, { 4, 25 }, { 7, 22 },
+  { 3, 27 }, { 2, 29 }, { 2, 30 }, { 5, 24 },
+  { 2, 31 }, { 6, 23 }, { 4, 26 }, { 3, 28 },
+  { 5, 25 }, { 3, 29 }, { 6, 24 }, { 7, 23 },
+  { 3, 30 }, { 4, 27 }, { 3, 31 }, { 5, 26 },
+  { 6, 25 }, { 4, 28 }, { 7, 24 }, { 4, 29 },
+  { 5, 27 }, { 4, 30 }, { 4, 31 }, { 6, 26 },
+  { 5, 28 }, { 7, 25 }, { 6, 27 }, { 5, 29 },
+  { 7, 26 }, { 5, 30 }, { 5, 31 }, { 6, 28 },
+  { 7, 27 }, { 6, 29 }, { 6, 30 }, { 7, 28 },
+  { 6, 31 }, { 7, 29 }, { 7, 30 }, { 7, 31 },
+  { 8, 16 }, { 9, 16 }, { 8, 17 }, { 10, 16 },
+  { 9, 17 }, { 16, 8 }, { 8, 18 }, { 16, 9 },
+  { 10, 17 }, { 11, 16 }, { 17, 8 }, { 9, 18 },
+  { 8, 19 }, { 16, 10 }, { 11, 17 }, { 12, 16 },
+  { 10, 18 }, { 17, 9 }, { 9, 19 }, { 16, 11 },
+  { 8, 20 }, { 18, 8 }, { 17, 10 }, { 10, 19 },
+  { 12, 17 }, { 11, 18 }, { 9, 20 }, { 16, 12 },
+  { 18, 9 }, { 8, 21 }, { 13, 16 }, { 17, 11 },
+  { 19, 8 }, { 18, 10 }, { 13, 17 }, { 16, 13 },
+  { 11, 19 }, { 12, 18 }, { 10, 20 }, { 17, 12 },
+  { 9, 21 }, { 19, 9 }, { 8, 22 }, { 14, 16 },
+  { 18, 11 }, { 11, 20 }, { 10, 21 }, { 20, 8 },
+  { 13, 18 }, { 16, 14 }, { 12, 19 }, { 17, 13 },
+  { 19, 10 }, { 14, 17 }, { 9, 22 }, { 18, 12 },
+  { 8, 23 }, { 17, 14 }, { 20, 9 }, { 15, 16 },
+  { 16, 15 }, { 13, 19 }, { 10, 22 }, { 19, 11 },
+  { 11, 21 }, { 14, 18 }, { 12, 20 }, { 18, 13 },
+  { 20, 10 }, { 21, 8 }, { 15, 17 }, { 9, 23 },
+  { 19, 12 }, { 11, 22 }, { 8, 24 }, { 21, 9 },
+  { 17, 15 }, { 16, 16 }, { 14, 19 }, { 18, 14 },
+  { 12, 21 }, { 13, 20 }, { 20, 11 }, { 10, 23 },
+  { 19, 13 }, { 15, 18 }, { 16, 17 }, { 21, 10 },
+  { 22, 8 }, { 9, 24 }, { 8, 25 }, { 20, 12 },
+  { 15, 19 }, { 11, 23 }, { 17, 16 }, { 18, 15 },
+  { 14, 20 }, { 12, 22 }, { 10, 24 }, { 22, 9 },
+  { 21, 11 }, { 19, 14 }, { 13, 21 }, { 16, 18 },
+  { 9, 25 }, { 17, 17 }, { 8, 26 }, { 20, 13 },
+  { 23, 8 }, { 12, 23 }, { 13, 22 }, { 22, 10 },
+  { 19, 15 }, { 15, 20 }, { 16, 19 }, { 21, 12 },
+  { 11, 24 }, { 14, 21 }, { 8, 27 }, { 18, 16 },
+  { 10, 25 }, { 9, 26 }, { 22, 11 }, { 20, 14 },
+  { 23, 9 }, { 18, 17 }, { 17, 18 }, { 17, 19 },
+  { 19, 16 }, { 21, 13 }, { 10, 26 }, { 12, 24 },
+  { 23, 10 }, { 24, 8 }, { 8, 28 }, { 16, 20 },
+  { 9, 27 }, { 15, 21 }, { 22, 12 }, { 14, 22 },
+  { 13, 23 }, { 20, 15 }, { 11, 25 }, { 24, 9 },
+  { 18, 18 }, { 19, 17 }, { 23, 11 }, { 10, 27 },
+  { 8, 29 }, { 12, 25 }, { 9, 28 }, { 8, 30 },
+  { 21, 14 }, { 13, 24 }, { 11, 26 }, { 25, 8 },
+  { 24, 10 }, { 20, 16 }, { 19, 18 }, { 14, 23 },
+  { 22, 13 }, { 8, 31 }, { 17, 20 }, { 9, 29 },
+  { 23, 12 }, { 15, 22 }, { 25, 9 }, { 11, 27 },
+  { 10, 28 }, { 20, 17 }, { 21, 15 }, { 18, 19 },
+  { 16, 21 }, { 24, 11 }, { 9, 30 }, { 12, 26 },
+  { 10, 29 }, { 22, 14 }, { 14, 24 }, { 9, 31 },
+  { 26, 8 }, { 13, 25 }, { 25, 10 }, { 18, 20 },
+  { 19, 19 }, { 11, 28 }, { 15, 23 }, { 20, 18 },
+  { 10, 30 }, { 12, 27 }, { 17, 21 }, { 23, 13 },
+  { 24, 12 }, { 21, 16 }, { 16, 22 }, { 26, 9 },
+  { 27, 8 }, { 13, 26 }, { 22, 15 }, { 10, 31 },
+  { 14, 25 }, { 12, 28 }, { 25, 11 }, { 21, 17 },
+  { 26, 10 }, { 20, 19 }, { 11, 29 }, { 15, 24 },
+  { 23, 14 }, { 27, 9 }, { 11, 30 }, { 13, 27 },
+  { 19, 20 }, { 24, 13 }, { 28, 8 }, { 11, 31 },
+  { 22, 16 }, { 17, 22 }, { 16, 23 }, { 25, 12 },
+  { 18, 21 }, { 12, 29 }, { 21, 18 }, { 28, 9 },
+  { 27, 10 }, { 26, 11 }, { 29, 8 }, { 14, 26 },
+  { 15, 25 }, { 13, 28 }, { 12, 30 }, { 23, 15 },
+  { 30, 8 }, { 16, 24 }, { 13, 29 }, { 25, 13 },
+  { 24, 14 }, { 20, 20 }, { 31, 8 }, { 12, 31 },
+  { 14, 27 }, { 28, 10 }, { 26, 12 }, { 22, 17 },
+  { 21, 19 }, { 17, 23 }, { 18, 22 }, { 29, 9 },
+  { 27, 11 }, { 19, 21 }, { 27, 12 }, { 30, 9 },
+  { 31, 9 }, { 13, 30 }, { 24, 15 }, { 23, 16 },
+  { 15, 26 }, { 14, 28 }, { 29, 10 }, { 28, 11 },
+  { 26, 13 }, { 17, 24 }, { 13, 31 }, { 25, 14 },
+  { 22, 18 }, { 16, 25 }, { 30, 10 }, { 14, 29 },
+  { 15, 27 }, { 19, 22 }, { 21, 20 }, { 20, 21 },
+  { 27, 13 }, { 29, 11 }, { 18, 23 }, { 23, 17 },
+  { 16, 26 }, { 31, 10 }, { 24, 16 }, { 14, 30 },
+  { 22, 19 }, { 14, 31 }, { 28, 12 }, { 26, 14 },
+  { 30, 11 }, { 15, 28 }, { 25, 15 }, { 17, 25 },
+  { 23, 18 }, { 18, 24 }, { 15, 30 }, { 29, 12 },
+  { 31, 11 }, { 16, 27 }, { 24, 17 }, { 28, 13 },
+  { 19, 23 }, { 15, 29 }, { 25, 16 }, { 17, 26 },
+  { 27, 14 }, { 22, 20 }, { 15, 31 }, { 20, 22 },
+  { 21, 21 }, { 16, 28 }, { 17, 27 }, { 30, 12 },
+  { 26, 15 }, { 19, 24 }, { 18, 25 }, { 23, 19 },
+  { 29, 13 }, { 31, 12 }, { 24, 18 }, { 26, 16 },
+  { 25, 17 }, { 16, 29 }, { 28, 14 }, { 20, 23 },
+  { 18, 26 }, { 21, 22 }, { 19, 25 }, { 22, 21 },
+  { 27, 15 }, { 17, 28 }, { 16, 30 }, { 26, 17 },
+  { 23, 20 }, { 16, 31 }, { 25, 18 }, { 27, 16 },
+  { 20, 24 }, { 24, 19 }, { 31, 13 }, { 30, 13 },
+  { 29, 14 }, { 18, 27 }, { 28, 15 }, { 17, 29 },
+  { 19, 26 }, { 17, 30 }, { 21, 23 }, { 22, 22 },
+  { 30, 14 }, { 20, 25 }, { 23, 21 }, { 17, 31 },
+  { 18, 28 }, { 25, 19 }, { 24, 20 }, { 28, 16 },
+  { 31, 14 }, { 26, 18 }, { 19, 27 }, { 29, 15 },
+  { 27, 17 }, { 30, 15 }, { 21, 24 }, { 22, 23 },
+  { 26, 19 }, { 23, 22 }, { 28, 17 }, { 29, 16 },
+  { 18, 30 }, { 24, 21 }, { 25, 20 }, { 18, 31 },
+  { 18, 29 }, { 20, 26 }, { 19, 28 }, { 27, 18 },
+  { 31, 15 }, { 20, 27 }, { 30, 16 }, { 19, 29 },
+  { 29, 17 }, { 31, 16 }, { 27, 19 }, { 21, 25 },
+  { 28, 18 }, { 26, 20 }, { 22, 24 }, { 25, 21 },
+  { 19, 30 }, { 24, 22 }, { 30, 17 }, { 21, 26 },
+  { 23, 23 }, { 19, 31 }, { 20, 28 }, { 31, 17 },
+  { 28, 19 }, { 27, 20 }, { 21, 27 }, { 29, 18 },
+  { 30, 18 }, { 25, 22 }, { 26, 21 }, { 20, 29 },
+  { 22, 25 }, { 24, 23 }, { 29, 19 }, { 23, 24 },
+  { 20, 31 }, { 20, 30 }, { 28, 20 }, { 21, 28 },
+  { 22, 26 }, { 31, 18 }, { 27, 21 }, { 30, 19 },
+  { 22, 27 }, { 29, 20 }, { 23, 25 }, { 24, 24 },
+  { 26, 22 }, { 21, 29 }, { 25, 23 }, { 31, 19 },
+  { 21, 30 }, { 23, 26 }, { 28, 21 }, { 21, 31 },
+  { 22, 28 }, { 30, 20 }, { 25, 24 }, { 27, 22 },
+  { 29, 21 }, { 26, 23 }, { 24, 25 }, { 31, 20 },
+  { 23, 27 }, { 22, 29 }, { 30, 21 }, { 28, 22 },
+  { 24, 26 }, { 25, 25 }, { 27, 23 }, { 22, 30 },
+  { 23, 28 }, { 22, 31 }, { 26, 24 }, { 31, 21 },
+  { 24, 27 }, { 29, 22 }, { 27, 24 }, { 30, 22 },
+  { 25, 26 }, { 28, 23 }, { 23, 30 }, { 23, 29 },
+  { 24, 28 }, { 25, 27 }, { 31, 22 }, { 23, 31 },
+  { 26, 25 }, { 28, 24 }, { 29, 23 }, { 24, 29 },
+  { 24, 30 }, { 27, 25 }, { 25, 28 }, { 26, 26 },
+  { 30, 23 }, { 26, 27 }, { 31, 23 }, { 28, 25 },
+  { 27, 26 }, { 25, 29 }, { 24, 31 }, { 29, 24 },
+  { 30, 24 }, { 27, 27 }, { 29, 25 }, { 26, 28 },
+  { 31, 24 }, { 25, 30 }, { 25, 31 }, { 28, 26 },
+  { 27, 28 }, { 26, 29 }, { 30, 25 }, { 29, 26 },
+  { 28, 27 }, { 26, 30 }, { 31, 25 }, { 27, 29 },
+  { 26, 31 }, { 30, 26 }, { 28, 28 }, { 31, 26 },
+  { 29, 27 }, { 27, 30 }, { 28, 29 }, { 27, 31 },
+  { 30, 27 }, { 31, 27 }, { 28, 30 }, { 29, 28 },
+  { 30, 28 }, { 29, 29 }, { 30, 29 }, { 31, 28 },
+  { 28, 31 }, { 29, 30 }, { 29, 31 }, { 31, 29 },
+  { 30, 30 }, { 30, 31 }, { 31, 30 }, { 31, 31 }
+};
diff --git a/av1/common/zigzag4.c b/av1/common/zigzag4.c
new file mode 100644
index 0000000..7ccc160
--- /dev/null
+++ b/av1/common/zigzag4.c
@@ -0,0 +1,28 @@
+/* This file is generated by gen_zigzag4.m */
+
+/* clang-format off */
+
+#include "odintrin.h"
+OD_EXTERN const unsigned char OD_ZIGZAG4_DCT_DCT[15][2] = {
+  {0, 1}, {1, 0}, {1, 1}, {0, 2},
+  {2, 0}, {0, 3}, {1, 2}, {3, 0},
+  {2, 1}, {1, 3}, {2, 2}, {3, 1},
+  {2, 3}, {3, 2}, {3, 3} };
+
+OD_EXTERN const unsigned char OD_ZIGZAG4_ADST_DCT[15][2] = {
+  {1, 0}, {0, 1}, {2, 0}, {1, 1},
+  {3, 0}, {2, 1}, {0, 2}, {1, 2},
+  {3, 1}, {0, 3}, {2, 2}, {1, 3},
+  {3, 2}, {2, 3}, {3, 3} };
+
+OD_EXTERN const unsigned char OD_ZIGZAG4_DCT_ADST[15][2] = {
+  {0, 1}, {0, 2}, {1, 0}, {0, 3},
+  {1, 1}, {1, 2}, {2, 0}, {1, 3},
+  {2, 1}, {2, 2}, {3, 0}, {3, 1},
+  {2, 3}, {3, 2}, {3, 3} };
+
+OD_EXTERN const unsigned char OD_ZIGZAG4_ADST_ADST[15][2] = {
+  {0, 1}, {1, 0}, {1, 1}, {0, 2},
+  {2, 0}, {0, 3}, {1, 2}, {3, 0},
+  {2, 1}, {1, 3}, {2, 2}, {3, 1},
+  {2, 3}, {3, 2}, {3, 3} };
diff --git a/av1/common/zigzag64.c b/av1/common/zigzag64.c
new file mode 100644
index 0000000..51c7ed3
--- /dev/null
+++ b/av1/common/zigzag64.c
@@ -0,0 +1,778 @@
+/* This file is generated by gen_zigzag64.m */
+
+/* clang-format off */
+
+/* a=1:64;
+   y=0.33*sqrt(a'*a) .+ 0.67*(a'+a)./2;
+   gen_zigzag64(1./y(:),'zigzag64.c') */
+#include "odintrin.h"
+OD_EXTERN const unsigned char OD_ZIGZAG64_DCT_DCT[3072][2] = {
+  { 32, 0 }, { 33, 0 }, { 34, 0 }, { 35, 0 },
+  { 32, 1 }, { 36, 0 }, { 33, 1 }, { 37, 0 },
+  { 34, 1 }, { 32, 2 }, { 38, 0 }, { 35, 1 },
+  { 33, 2 }, { 39, 0 }, { 36, 1 }, { 34, 2 },
+  { 40, 0 }, { 32, 3 }, { 37, 1 }, { 35, 2 },
+  { 41, 0 }, { 33, 3 }, { 38, 1 }, { 36, 2 },
+  { 42, 0 }, { 32, 4 }, { 34, 3 }, { 39, 1 },
+  { 37, 2 }, { 43, 0 }, { 35, 3 }, { 33, 4 },
+  { 40, 1 }, { 44, 0 }, { 38, 2 }, { 32, 5 },
+  { 36, 3 }, { 41, 1 }, { 34, 4 }, { 45, 0 },
+  { 39, 2 }, { 33, 5 }, { 42, 1 }, { 37, 3 },
+  { 35, 4 }, { 46, 0 }, { 40, 2 }, { 32, 6 },
+  { 43, 1 }, { 34, 5 }, { 38, 3 }, { 36, 4 },
+  { 47, 0 }, { 41, 2 }, { 33, 6 }, { 44, 1 },
+  { 39, 3 }, { 35, 5 }, { 37, 4 }, { 48, 0 },
+  { 32, 7 }, { 42, 2 }, { 34, 6 }, { 45, 1 },
+  { 40, 3 }, { 36, 5 }, { 38, 4 }, { 49, 0 },
+  { 33, 7 }, { 43, 2 }, { 46, 1 }, { 35, 6 },
+  { 41, 3 }, { 37, 5 }, { 39, 4 }, { 32, 8 },
+  { 50, 0 }, { 44, 2 }, { 34, 7 }, { 47, 1 },
+  { 36, 6 }, { 42, 3 }, { 38, 5 }, { 51, 0 },
+  { 40, 4 }, { 33, 8 }, { 45, 2 }, { 35, 7 },
+  { 48, 1 }, { 32, 9 }, { 37, 6 }, { 43, 3 },
+  { 52, 0 }, { 39, 5 }, { 41, 4 }, { 34, 8 },
+  { 46, 2 }, { 49, 1 }, { 36, 7 }, { 33, 9 },
+  { 44, 3 }, { 53, 0 }, { 38, 6 }, { 42, 4 },
+  { 40, 5 }, { 35, 8 }, { 32, 10 }, { 47, 2 },
+  { 50, 1 }, { 37, 7 }, { 54, 0 }, { 45, 3 },
+  { 34, 9 }, { 39, 6 }, { 43, 4 }, { 41, 5 },
+  { 48, 2 }, { 36, 8 }, { 51, 1 }, { 33, 10 },
+  { 55, 0 }, { 38, 7 }, { 46, 3 }, { 32, 11 },
+  { 40, 6 }, { 35, 9 }, { 44, 4 }, { 42, 5 },
+  { 49, 2 }, { 52, 1 }, { 37, 8 }, { 34, 10 },
+  { 56, 0 }, { 39, 7 }, { 47, 3 }, { 41, 6 },
+  { 33, 11 }, { 45, 4 }, { 36, 9 }, { 43, 5 },
+  { 50, 2 }, { 53, 1 }, { 32, 12 }, { 38, 8 },
+  { 57, 0 }, { 35, 10 }, { 48, 3 }, { 40, 7 },
+  { 42, 6 }, { 46, 4 }, { 44, 5 }, { 34, 11 },
+  { 37, 9 }, { 51, 2 }, { 54, 1 }, { 58, 0 },
+  { 39, 8 }, { 33, 12 }, { 36, 10 }, { 49, 3 },
+  { 41, 7 }, { 32, 13 }, { 47, 4 }, { 43, 6 },
+  { 45, 5 }, { 52, 2 }, { 55, 1 }, { 38, 9 },
+  { 35, 11 }, { 59, 0 }, { 40, 8 }, { 34, 12 },
+  { 50, 3 }, { 37, 10 }, { 42, 7 }, { 48, 4 },
+  { 44, 6 }, { 33, 13 }, { 56, 1 }, { 53, 2 },
+  { 46, 5 }, { 60, 0 }, { 39, 9 }, { 36, 11 },
+  { 32, 14 }, { 41, 8 }, { 51, 3 }, { 35, 12 },
+  { 38, 10 }, { 43, 7 }, { 49, 4 }, { 57, 1 },
+  { 54, 2 }, { 45, 6 }, { 47, 5 }, { 61, 0 },
+  { 34, 13 }, { 40, 9 }, { 37, 11 }, { 33, 14 },
+  { 52, 3 }, { 42, 8 }, { 36, 12 }, { 32, 15 },
+  { 39, 10 }, { 44, 7 }, { 58, 1 }, { 50, 4 },
+  { 55, 2 }, { 62, 0 }, { 46, 6 }, { 48, 5 },
+  { 35, 13 }, { 41, 9 }, { 38, 11 }, { 53, 3 },
+  { 34, 14 }, { 43, 8 }, { 59, 1 }, { 63, 0 },
+  { 56, 2 }, { 51, 4 }, { 37, 12 }, { 45, 7 },
+  { 40, 10 }, { 33, 15 }, { 47, 6 }, { 49, 5 },
+  { 36, 13 }, { 42, 9 }, { 39, 11 }, { 54, 3 },
+  { 44, 8 }, { 60, 1 }, { 35, 14 }, { 57, 2 },
+  { 52, 4 }, { 46, 7 }, { 41, 10 }, { 38, 12 },
+  { 50, 5 }, { 48, 6 }, { 34, 15 }, { 43, 9 },
+  { 37, 13 }, { 55, 3 }, { 40, 11 }, { 61, 1 },
+  { 45, 8 }, { 58, 2 }, { 53, 4 }, { 36, 14 },
+  { 47, 7 }, { 51, 5 }, { 42, 10 }, { 49, 6 },
+  { 39, 12 }, { 35, 15 }, { 56, 3 }, { 44, 9 },
+  { 38, 13 }, { 62, 1 }, { 41, 11 }, { 59, 2 },
+  { 46, 8 }, { 54, 4 }, { 48, 7 }, { 37, 14 },
+  { 52, 5 }, { 50, 6 }, { 43, 10 }, { 40, 12 },
+  { 36, 15 }, { 57, 3 }, { 45, 9 }, { 63, 1 },
+  { 39, 13 }, { 60, 2 }, { 42, 11 }, { 47, 8 },
+  { 55, 4 }, { 49, 7 }, { 53, 5 }, { 51, 6 },
+  { 38, 14 }, { 44, 10 }, { 41, 12 }, { 58, 3 },
+  { 37, 15 }, { 46, 9 }, { 61, 2 }, { 40, 13 },
+  { 56, 4 }, { 43, 11 }, { 48, 8 }, { 54, 5 },
+  { 50, 7 }, { 52, 6 }, { 39, 14 }, { 45, 10 },
+  { 59, 3 }, { 42, 12 }, { 62, 2 }, { 47, 9 },
+  { 38, 15 }, { 57, 4 }, { 41, 13 }, { 44, 11 },
+  { 49, 8 }, { 55, 5 }, { 51, 7 }, { 53, 6 },
+  { 60, 3 }, { 46, 10 }, { 40, 14 }, { 43, 12 },
+  { 63, 2 }, { 48, 9 }, { 58, 4 }, { 39, 15 },
+  { 50, 8 }, { 45, 11 }, { 42, 13 }, { 56, 5 },
+  { 52, 7 }, { 54, 6 }, { 61, 3 }, { 47, 10 },
+  { 41, 14 }, { 44, 12 }, { 49, 9 }, { 59, 4 },
+  { 40, 15 }, { 51, 8 }, { 57, 5 }, { 46, 11 },
+  { 43, 13 }, { 53, 7 }, { 55, 6 }, { 62, 3 },
+  { 48, 10 }, { 42, 14 }, { 45, 12 }, { 60, 4 },
+  { 50, 9 }, { 52, 8 }, { 58, 5 }, { 41, 15 },
+  { 47, 11 }, { 54, 7 }, { 56, 6 }, { 44, 13 },
+  { 63, 3 }, { 49, 10 }, { 43, 14 }, { 61, 4 },
+  { 46, 12 }, { 51, 9 }, { 59, 5 }, { 53, 8 },
+  { 42, 15 }, { 57, 6 }, { 55, 7 }, { 48, 11 },
+  { 45, 13 }, { 50, 10 }, { 62, 4 }, { 44, 14 },
+  { 47, 12 }, { 52, 9 }, { 60, 5 }, { 54, 8 },
+  { 58, 6 }, { 56, 7 }, { 49, 11 }, { 43, 15 },
+  { 46, 13 }, { 51, 10 }, { 63, 4 }, { 48, 12 },
+  { 45, 14 }, { 53, 9 }, { 61, 5 }, { 55, 8 },
+  { 59, 6 }, { 57, 7 }, { 50, 11 }, { 44, 15 },
+  { 47, 13 }, { 52, 10 }, { 54, 9 }, { 49, 12 },
+  { 62, 5 }, { 46, 14 }, { 56, 8 }, { 60, 6 },
+  { 58, 7 }, { 51, 11 }, { 45, 15 }, { 48, 13 },
+  { 53, 10 }, { 63, 5 }, { 55, 9 }, { 50, 12 },
+  { 47, 14 }, { 57, 8 }, { 61, 6 }, { 59, 7 },
+  { 52, 11 }, { 46, 15 }, { 49, 13 }, { 54, 10 },
+  { 56, 9 }, { 51, 12 }, { 62, 6 }, { 58, 8 },
+  { 48, 14 }, { 60, 7 }, { 53, 11 }, { 47, 15 },
+  { 50, 13 }, { 55, 10 }, { 57, 9 }, { 63, 6 },
+  { 52, 12 }, { 59, 8 }, { 61, 7 }, { 49, 14 },
+  { 54, 11 }, { 51, 13 }, { 48, 15 }, { 56, 10 },
+  { 58, 9 }, { 60, 8 }, { 53, 12 }, { 62, 7 },
+  { 50, 14 }, { 55, 11 }, { 52, 13 }, { 49, 15 },
+  { 57, 10 }, { 59, 9 }, { 61, 8 }, { 63, 7 },
+  { 54, 12 }, { 51, 14 }, { 56, 11 }, { 53, 13 },
+  { 58, 10 }, { 50, 15 }, { 60, 9 }, { 62, 8 },
+  { 55, 12 }, { 52, 14 }, { 57, 11 }, { 59, 10 },
+  { 54, 13 }, { 51, 15 }, { 61, 9 }, { 63, 8 },
+  { 56, 12 }, { 53, 14 }, { 58, 11 }, { 60, 10 },
+  { 55, 13 }, { 52, 15 }, { 62, 9 }, { 57, 12 },
+  { 54, 14 }, { 59, 11 }, { 61, 10 }, { 56, 13 },
+  { 63, 9 }, { 53, 15 }, { 58, 12 }, { 55, 14 },
+  { 60, 11 }, { 62, 10 }, { 57, 13 }, { 54, 15 },
+  { 59, 12 }, { 56, 14 }, { 61, 11 }, { 63, 10 },
+  { 58, 13 }, { 55, 15 }, { 60, 12 }, { 57, 14 },
+  { 62, 11 }, { 59, 13 }, { 56, 15 }, { 61, 12 },
+  { 63, 11 }, { 58, 14 }, { 60, 13 }, { 57, 15 },
+  { 62, 12 }, { 59, 14 }, { 61, 13 }, { 58, 15 },
+  { 63, 12 }, { 60, 14 }, { 62, 13 }, { 59, 15 },
+  { 61, 14 }, { 63, 13 }, { 60, 15 }, { 62, 14 },
+  { 61, 15 }, { 63, 14 }, { 62, 15 }, { 63, 15 },
+  { 0, 32 }, { 0, 33 }, { 0, 34 }, { 0, 35 },
+  { 1, 32 }, { 0, 36 }, { 1, 33 }, { 0, 37 },
+  { 1, 34 }, { 2, 32 }, { 0, 38 }, { 1, 35 },
+  { 2, 33 }, { 0, 39 }, { 1, 36 }, { 2, 34 },
+  { 0, 40 }, { 3, 32 }, { 1, 37 }, { 2, 35 },
+  { 0, 41 }, { 3, 33 }, { 1, 38 }, { 2, 36 },
+  { 0, 42 }, { 4, 32 }, { 3, 34 }, { 1, 39 },
+  { 2, 37 }, { 0, 43 }, { 3, 35 }, { 4, 33 },
+  { 1, 40 }, { 0, 44 }, { 2, 38 }, { 5, 32 },
+  { 3, 36 }, { 1, 41 }, { 4, 34 }, { 0, 45 },
+  { 2, 39 }, { 5, 33 }, { 1, 42 }, { 3, 37 },
+  { 4, 35 }, { 0, 46 }, { 2, 40 }, { 6, 32 },
+  { 1, 43 }, { 5, 34 }, { 3, 38 }, { 4, 36 },
+  { 0, 47 }, { 2, 41 }, { 6, 33 }, { 1, 44 },
+  { 3, 39 }, { 5, 35 }, { 4, 37 }, { 0, 48 },
+  { 7, 32 }, { 2, 42 }, { 6, 34 }, { 1, 45 },
+  { 3, 40 }, { 5, 36 }, { 4, 38 }, { 0, 49 },
+  { 7, 33 }, { 2, 43 }, { 1, 46 }, { 6, 35 },
+  { 3, 41 }, { 5, 37 }, { 4, 39 }, { 8, 32 },
+  { 0, 50 }, { 2, 44 }, { 7, 34 }, { 1, 47 },
+  { 6, 36 }, { 3, 42 }, { 5, 38 }, { 0, 51 },
+  { 4, 40 }, { 8, 33 }, { 2, 45 }, { 7, 35 },
+  { 1, 48 }, { 9, 32 }, { 6, 37 }, { 3, 43 },
+  { 0, 52 }, { 5, 39 }, { 4, 41 }, { 8, 34 },
+  { 2, 46 }, { 1, 49 }, { 7, 36 }, { 9, 33 },
+  { 3, 44 }, { 0, 53 }, { 6, 38 }, { 4, 42 },
+  { 5, 40 }, { 8, 35 }, { 10, 32 }, { 2, 47 },
+  { 1, 50 }, { 7, 37 }, { 0, 54 }, { 3, 45 },
+  { 9, 34 }, { 6, 39 }, { 4, 43 }, { 5, 41 },
+  { 2, 48 }, { 8, 36 }, { 1, 51 }, { 10, 33 },
+  { 0, 55 }, { 7, 38 }, { 3, 46 }, { 11, 32 },
+  { 6, 40 }, { 9, 35 }, { 4, 44 }, { 5, 42 },
+  { 2, 49 }, { 1, 52 }, { 8, 37 }, { 10, 34 },
+  { 0, 56 }, { 7, 39 }, { 3, 47 }, { 6, 41 },
+  { 11, 33 }, { 4, 45 }, { 9, 36 }, { 5, 43 },
+  { 2, 50 }, { 1, 53 }, { 12, 32 }, { 8, 38 },
+  { 0, 57 }, { 10, 35 }, { 3, 48 }, { 7, 40 },
+  { 6, 42 }, { 4, 46 }, { 5, 44 }, { 11, 34 },
+  { 9, 37 }, { 2, 51 }, { 1, 54 }, { 0, 58 },
+  { 8, 39 }, { 12, 33 }, { 10, 36 }, { 3, 49 },
+  { 7, 41 }, { 13, 32 }, { 4, 47 }, { 6, 43 },
+  { 5, 45 }, { 2, 52 }, { 1, 55 }, { 9, 38 },
+  { 11, 35 }, { 0, 59 }, { 8, 40 }, { 12, 34 },
+  { 3, 50 }, { 10, 37 }, { 7, 42 }, { 4, 48 },
+  { 6, 44 }, { 13, 33 }, { 1, 56 }, { 2, 53 },
+  { 5, 46 }, { 0, 60 }, { 9, 39 }, { 11, 36 },
+  { 14, 32 }, { 8, 41 }, { 3, 51 }, { 12, 35 },
+  { 10, 38 }, { 7, 43 }, { 4, 49 }, { 1, 57 },
+  { 2, 54 }, { 6, 45 }, { 5, 47 }, { 0, 61 },
+  { 13, 34 }, { 9, 40 }, { 11, 37 }, { 14, 33 },
+  { 3, 52 }, { 8, 42 }, { 12, 36 }, { 15, 32 },
+  { 10, 39 }, { 7, 44 }, { 1, 58 }, { 4, 50 },
+  { 2, 55 }, { 0, 62 }, { 6, 46 }, { 5, 48 },
+  { 13, 35 }, { 9, 41 }, { 11, 38 }, { 3, 53 },
+  { 14, 34 }, { 8, 43 }, { 1, 59 }, { 0, 63 },
+  { 2, 56 }, { 4, 51 }, { 12, 37 }, { 7, 45 },
+  { 10, 40 }, { 15, 33 }, { 6, 47 }, { 5, 49 },
+  { 13, 36 }, { 9, 42 }, { 11, 39 }, { 3, 54 },
+  { 8, 44 }, { 1, 60 }, { 14, 35 }, { 2, 57 },
+  { 4, 52 }, { 7, 46 }, { 10, 41 }, { 12, 38 },
+  { 5, 50 }, { 6, 48 }, { 15, 34 }, { 9, 43 },
+  { 13, 37 }, { 3, 55 }, { 11, 40 }, { 1, 61 },
+  { 8, 45 }, { 2, 58 }, { 4, 53 }, { 14, 36 },
+  { 7, 47 }, { 5, 51 }, { 10, 42 }, { 6, 49 },
+  { 12, 39 }, { 15, 35 }, { 3, 56 }, { 9, 44 },
+  { 13, 38 }, { 1, 62 }, { 11, 41 }, { 2, 59 },
+  { 8, 46 }, { 4, 54 }, { 7, 48 }, { 14, 37 },
+  { 5, 52 }, { 6, 50 }, { 10, 43 }, { 12, 40 },
+  { 15, 36 }, { 3, 57 }, { 9, 45 }, { 1, 63 },
+  { 13, 39 }, { 2, 60 }, { 11, 42 }, { 8, 47 },
+  { 4, 55 }, { 7, 49 }, { 5, 53 }, { 6, 51 },
+  { 14, 38 }, { 10, 44 }, { 12, 41 }, { 3, 58 },
+  { 15, 37 }, { 9, 46 }, { 2, 61 }, { 13, 40 },
+  { 4, 56 }, { 11, 43 }, { 8, 48 }, { 5, 54 },
+  { 7, 50 }, { 6, 52 }, { 14, 39 }, { 10, 45 },
+  { 3, 59 }, { 12, 42 }, { 2, 62 }, { 9, 47 },
+  { 15, 38 }, { 4, 57 }, { 13, 41 }, { 11, 44 },
+  { 8, 49 }, { 5, 55 }, { 7, 51 }, { 6, 53 },
+  { 3, 60 }, { 10, 46 }, { 14, 40 }, { 12, 43 },
+  { 2, 63 }, { 9, 48 }, { 4, 58 }, { 15, 39 },
+  { 8, 50 }, { 11, 45 }, { 13, 42 }, { 5, 56 },
+  { 7, 52 }, { 6, 54 }, { 3, 61 }, { 10, 47 },
+  { 14, 41 }, { 12, 44 }, { 9, 49 }, { 4, 59 },
+  { 15, 40 }, { 8, 51 }, { 5, 57 }, { 11, 46 },
+  { 13, 43 }, { 7, 53 }, { 6, 55 }, { 3, 62 },
+  { 10, 48 }, { 14, 42 }, { 12, 45 }, { 4, 60 },
+  { 9, 50 }, { 8, 52 }, { 5, 58 }, { 15, 41 },
+  { 11, 47 }, { 7, 54 }, { 6, 56 }, { 13, 44 },
+  { 3, 63 }, { 10, 49 }, { 14, 43 }, { 4, 61 },
+  { 12, 46 }, { 9, 51 }, { 5, 59 }, { 8, 53 },
+  { 15, 42 }, { 6, 57 }, { 7, 55 }, { 11, 48 },
+  { 13, 45 }, { 10, 50 }, { 4, 62 }, { 14, 44 },
+  { 12, 47 }, { 9, 52 }, { 5, 60 }, { 8, 54 },
+  { 6, 58 }, { 7, 56 }, { 11, 49 }, { 15, 43 },
+  { 13, 46 }, { 10, 51 }, { 4, 63 }, { 12, 48 },
+  { 14, 45 }, { 9, 53 }, { 5, 61 }, { 8, 55 },
+  { 6, 59 }, { 7, 57 }, { 11, 50 }, { 15, 44 },
+  { 13, 47 }, { 10, 52 }, { 9, 54 }, { 12, 49 },
+  { 5, 62 }, { 14, 46 }, { 8, 56 }, { 6, 60 },
+  { 7, 58 }, { 11, 51 }, { 15, 45 }, { 13, 48 },
+  { 10, 53 }, { 5, 63 }, { 9, 55 }, { 12, 50 },
+  { 14, 47 }, { 8, 57 }, { 6, 61 }, { 7, 59 },
+  { 11, 52 }, { 15, 46 }, { 13, 49 }, { 10, 54 },
+  { 9, 56 }, { 12, 51 }, { 6, 62 }, { 8, 58 },
+  { 14, 48 }, { 7, 60 }, { 11, 53 }, { 15, 47 },
+  { 13, 50 }, { 10, 55 }, { 9, 57 }, { 6, 63 },
+  { 12, 52 }, { 8, 59 }, { 7, 61 }, { 14, 49 },
+  { 11, 54 }, { 13, 51 }, { 15, 48 }, { 10, 56 },
+  { 9, 58 }, { 8, 60 }, { 12, 53 }, { 7, 62 },
+  { 14, 50 }, { 11, 55 }, { 13, 52 }, { 15, 49 },
+  { 10, 57 }, { 9, 59 }, { 8, 61 }, { 7, 63 },
+  { 12, 54 }, { 14, 51 }, { 11, 56 }, { 13, 53 },
+  { 10, 58 }, { 15, 50 }, { 9, 60 }, { 8, 62 },
+  { 12, 55 }, { 14, 52 }, { 11, 57 }, { 10, 59 },
+  { 13, 54 }, { 15, 51 }, { 9, 61 }, { 8, 63 },
+  { 12, 56 }, { 14, 53 }, { 11, 58 }, { 10, 60 },
+  { 13, 55 }, { 15, 52 }, { 9, 62 }, { 12, 57 },
+  { 14, 54 }, { 11, 59 }, { 10, 61 }, { 13, 56 },
+  { 9, 63 }, { 15, 53 }, { 12, 58 }, { 14, 55 },
+  { 11, 60 }, { 10, 62 }, { 13, 57 }, { 15, 54 },
+  { 12, 59 }, { 14, 56 }, { 11, 61 }, { 10, 63 },
+  { 13, 58 }, { 15, 55 }, { 12, 60 }, { 14, 57 },
+  { 11, 62 }, { 13, 59 }, { 15, 56 }, { 12, 61 },
+  { 11, 63 }, { 14, 58 }, { 13, 60 }, { 15, 57 },
+  { 12, 62 }, { 14, 59 }, { 13, 61 }, { 15, 58 },
+  { 12, 63 }, { 14, 60 }, { 13, 62 }, { 15, 59 },
+  { 14, 61 }, { 13, 63 }, { 15, 60 }, { 14, 62 },
+  { 15, 61 }, { 14, 63 }, { 15, 62 }, { 15, 63 },
+  { 32, 16 }, { 16, 32 }, { 33, 16 }, { 16, 33 },
+  { 32, 17 }, { 17, 32 }, { 34, 16 }, { 16, 34 },
+  { 33, 17 }, { 17, 33 }, { 32, 18 }, { 18, 32 },
+  { 35, 16 }, { 16, 35 }, { 34, 17 }, { 17, 34 },
+  { 33, 18 }, { 18, 33 }, { 32, 19 }, { 19, 32 },
+  { 36, 16 }, { 16, 36 }, { 35, 17 }, { 17, 35 },
+  { 34, 18 }, { 18, 34 }, { 33, 19 }, { 19, 33 },
+  { 32, 20 }, { 20, 32 }, { 37, 16 }, { 16, 37 },
+  { 36, 17 }, { 17, 36 }, { 35, 18 }, { 18, 35 },
+  { 34, 19 }, { 19, 34 }, { 33, 20 }, { 20, 33 },
+  { 38, 16 }, { 16, 38 }, { 32, 21 }, { 21, 32 },
+  { 37, 17 }, { 17, 37 }, { 36, 18 }, { 18, 36 },
+  { 35, 19 }, { 19, 35 }, { 39, 16 }, { 16, 39 },
+  { 34, 20 }, { 20, 34 }, { 33, 21 }, { 21, 33 },
+  { 38, 17 }, { 17, 38 }, { 32, 22 }, { 22, 32 },
+  { 37, 18 }, { 18, 37 }, { 36, 19 }, { 19, 36 },
+  { 40, 16 }, { 16, 40 }, { 35, 20 }, { 20, 35 },
+  { 34, 21 }, { 21, 34 }, { 39, 17 }, { 17, 39 },
+  { 33, 22 }, { 22, 33 }, { 32, 23 }, { 23, 32 },
+  { 38, 18 }, { 18, 38 }, { 37, 19 }, { 19, 37 },
+  { 41, 16 }, { 16, 41 }, { 36, 20 }, { 20, 36 },
+  { 35, 21 }, { 21, 35 }, { 40, 17 }, { 17, 40 },
+  { 34, 22 }, { 22, 34 }, { 33, 23 }, { 23, 33 },
+  { 39, 18 }, { 18, 39 }, { 32, 24 }, { 24, 32 },
+  { 38, 19 }, { 19, 38 }, { 42, 16 }, { 16, 42 },
+  { 37, 20 }, { 20, 37 }, { 41, 17 }, { 17, 41 },
+  { 36, 21 }, { 21, 36 }, { 35, 22 }, { 22, 35 },
+  { 40, 18 }, { 18, 40 }, { 34, 23 }, { 23, 34 },
+  { 33, 24 }, { 24, 33 }, { 32, 25 }, { 25, 32 },
+  { 39, 19 }, { 19, 39 }, { 43, 16 }, { 16, 43 },
+  { 38, 20 }, { 20, 38 }, { 42, 17 }, { 17, 42 },
+  { 37, 21 }, { 21, 37 }, { 36, 22 }, { 22, 36 },
+  { 41, 18 }, { 18, 41 }, { 35, 23 }, { 23, 35 },
+  { 34, 24 }, { 24, 34 }, { 40, 19 }, { 19, 40 },
+  { 44, 16 }, { 16, 44 }, { 33, 25 }, { 25, 33 },
+  { 32, 26 }, { 26, 32 }, { 39, 20 }, { 20, 39 },
+  { 43, 17 }, { 17, 43 }, { 38, 21 }, { 21, 38 },
+  { 37, 22 }, { 22, 37 }, { 42, 18 }, { 18, 42 },
+  { 36, 23 }, { 23, 36 }, { 45, 16 }, { 16, 45 },
+  { 41, 19 }, { 19, 41 }, { 35, 24 }, { 24, 35 },
+  { 34, 25 }, { 25, 34 }, { 33, 26 }, { 26, 33 },
+  { 40, 20 }, { 20, 40 }, { 32, 27 }, { 27, 32 },
+  { 44, 17 }, { 17, 44 }, { 39, 21 }, { 21, 39 },
+  { 43, 18 }, { 18, 43 }, { 38, 22 }, { 22, 38 },
+  { 37, 23 }, { 23, 37 }, { 46, 16 }, { 16, 46 },
+  { 42, 19 }, { 19, 42 }, { 36, 24 }, { 24, 36 },
+  { 35, 25 }, { 25, 35 }, { 41, 20 }, { 20, 41 },
+  { 34, 26 }, { 26, 34 }, { 45, 17 }, { 17, 45 },
+  { 33, 27 }, { 27, 33 }, { 32, 28 }, { 28, 32 },
+  { 40, 21 }, { 21, 40 }, { 44, 18 }, { 18, 44 },
+  { 39, 22 }, { 22, 39 }, { 38, 23 }, { 23, 38 },
+  { 47, 16 }, { 16, 47 }, { 43, 19 }, { 19, 43 },
+  { 37, 24 }, { 24, 37 }, { 36, 25 }, { 25, 36 },
+  { 42, 20 }, { 20, 42 }, { 46, 17 }, { 17, 46 },
+  { 35, 26 }, { 26, 35 }, { 34, 27 }, { 27, 34 },
+  { 33, 28 }, { 28, 33 }, { 41, 21 }, { 21, 41 },
+  { 32, 29 }, { 29, 32 }, { 45, 18 }, { 18, 45 },
+  { 40, 22 }, { 22, 40 }, { 48, 16 }, { 16, 48 },
+  { 39, 23 }, { 23, 39 }, { 44, 19 }, { 19, 44 },
+  { 38, 24 }, { 24, 38 }, { 43, 20 }, { 20, 43 },
+  { 47, 17 }, { 17, 47 }, { 37, 25 }, { 25, 37 },
+  { 36, 26 }, { 26, 36 }, { 35, 27 }, { 27, 35 },
+  { 42, 21 }, { 21, 42 }, { 34, 28 }, { 28, 34 },
+  { 46, 18 }, { 18, 46 }, { 33, 29 }, { 29, 33 },
+  { 32, 30 }, { 30, 32 }, { 41, 22 }, { 22, 41 },
+  { 49, 16 }, { 16, 49 }, { 45, 19 }, { 19, 45 },
+  { 40, 23 }, { 23, 40 }, { 39, 24 }, { 24, 39 },
+  { 48, 17 }, { 17, 48 }, { 44, 20 }, { 20, 44 },
+  { 38, 25 }, { 25, 38 }, { 37, 26 }, { 26, 37 },
+  { 43, 21 }, { 21, 43 }, { 36, 27 }, { 27, 36 },
+  { 47, 18 }, { 18, 47 }, { 35, 28 }, { 28, 35 },
+  { 34, 29 }, { 29, 34 }, { 42, 22 }, { 22, 42 },
+  { 33, 30 }, { 30, 33 }, { 50, 16 }, { 16, 50 },
+  { 32, 31 }, { 31, 32 }, { 46, 19 }, { 19, 46 },
+  { 41, 23 }, { 23, 41 }, { 40, 24 }, { 24, 40 },
+  { 49, 17 }, { 17, 49 }, { 45, 20 }, { 20, 45 },
+  { 39, 25 }, { 25, 39 }, { 38, 26 }, { 26, 38 },
+  { 44, 21 }, { 21, 44 }, { 48, 18 }, { 18, 48 },
+  { 37, 27 }, { 27, 37 }, { 36, 28 }, { 28, 36 },
+  { 51, 16 }, { 16, 51 }, { 43, 22 }, { 22, 43 },
+  { 35, 29 }, { 29, 35 }, { 34, 30 }, { 30, 34 },
+  { 33, 31 }, { 31, 33 }, { 32, 32 }, { 47, 19 },
+  { 19, 47 }, { 42, 23 }, { 23, 42 }, { 50, 17 },
+  { 17, 50 }, { 41, 24 }, { 24, 41 }, { 46, 20 },
+  { 20, 46 }, { 40, 25 }, { 25, 40 }, { 45, 21 },
+  { 21, 45 }, { 49, 18 }, { 18, 49 }, { 39, 26 },
+  { 26, 39 }, { 38, 27 }, { 27, 38 }, { 52, 16 },
+  { 16, 52 }, { 44, 22 }, { 22, 44 }, { 37, 28 },
+  { 28, 37 }, { 36, 29 }, { 29, 36 }, { 48, 19 },
+  { 19, 48 }, { 35, 30 }, { 30, 35 }, { 34, 31 },
+  { 31, 34 }, { 33, 32 }, { 32, 33 }, { 43, 23 },
+  { 23, 43 }, { 51, 17 }, { 17, 51 }, { 47, 20 },
+  { 20, 47 }, { 42, 24 }, { 24, 42 }, { 41, 25 },
+  { 25, 41 }, { 50, 18 }, { 18, 50 }, { 46, 21 },
+  { 21, 46 }, { 40, 26 }, { 26, 40 }, { 53, 16 },
+  { 16, 53 }, { 39, 27 }, { 27, 39 }, { 45, 22 },
+  { 22, 45 }, { 38, 28 }, { 28, 38 }, { 49, 19 },
+  { 19, 49 }, { 37, 29 }, { 29, 37 }, { 36, 30 },
+  { 30, 36 }, { 44, 23 }, { 23, 44 }, { 52, 17 },
+  { 17, 52 }, { 35, 31 }, { 31, 35 }, { 34, 32 },
+  { 32, 34 }, { 33, 33 }, { 48, 20 }, { 20, 48 },
+  { 43, 24 }, { 24, 43 }, { 42, 25 }, { 25, 42 },
+  { 51, 18 }, { 18, 51 }, { 47, 21 }, { 21, 47 },
+  { 54, 16 }, { 16, 54 }, { 41, 26 }, { 26, 41 },
+  { 40, 27 }, { 27, 40 }, { 46, 22 }, { 22, 46 },
+  { 50, 19 }, { 19, 50 }, { 39, 28 }, { 28, 39 },
+  { 38, 29 }, { 29, 38 }, { 53, 17 }, { 17, 53 },
+  { 45, 23 }, { 23, 45 }, { 37, 30 }, { 30, 37 },
+  { 36, 31 }, { 31, 36 }, { 49, 20 }, { 20, 49 },
+  { 35, 32 }, { 32, 35 }, { 34, 33 }, { 33, 34 },
+  { 44, 24 }, { 24, 44 }, { 52, 18 }, { 18, 52 },
+  { 43, 25 }, { 25, 43 }, { 48, 21 }, { 21, 48 },
+  { 55, 16 }, { 16, 55 }, { 42, 26 }, { 26, 42 },
+  { 47, 22 }, { 22, 47 }, { 51, 19 }, { 19, 51 },
+  { 41, 27 }, { 27, 41 }, { 40, 28 }, { 28, 40 },
+  { 54, 17 }, { 17, 54 }, { 46, 23 }, { 23, 46 },
+  { 39, 29 }, { 29, 39 }, { 50, 20 }, { 20, 50 },
+  { 38, 30 }, { 30, 38 }, { 37, 31 }, { 31, 37 },
+  { 45, 24 }, { 24, 45 }, { 36, 32 }, { 32, 36 },
+  { 35, 33 }, { 33, 35 }, { 34, 34 }, { 53, 18 },
+  { 18, 53 }, { 56, 16 }, { 16, 56 }, { 49, 21 },
+  { 21, 49 }, { 44, 25 }, { 25, 44 }, { 43, 26 },
+  { 26, 43 }, { 48, 22 }, { 22, 48 }, { 52, 19 },
+  { 19, 52 }, { 42, 27 }, { 27, 42 }, { 55, 17 },
+  { 17, 55 }, { 41, 28 }, { 28, 41 }, { 47, 23 },
+  { 23, 47 }, { 40, 29 }, { 29, 40 }, { 51, 20 },
+  { 20, 51 }, { 39, 30 }, { 30, 39 }, { 46, 24 },
+  { 24, 46 }, { 38, 31 }, { 31, 38 }, { 54, 18 },
+  { 18, 54 }, { 37, 32 }, { 32, 37 }, { 57, 16 },
+  { 16, 57 }, { 36, 33 }, { 33, 36 }, { 35, 34 },
+  { 34, 35 }, { 50, 21 }, { 21, 50 }, { 45, 25 },
+  { 25, 45 }, { 44, 26 }, { 26, 44 }, { 53, 19 },
+  { 19, 53 }, { 49, 22 }, { 22, 49 }, { 56, 17 },
+  { 17, 56 }, { 43, 27 }, { 27, 43 }, { 48, 23 },
+  { 23, 48 }, { 42, 28 }, { 28, 42 }, { 52, 20 },
+  { 20, 52 }, { 41, 29 }, { 29, 41 }, { 40, 30 },
+  { 30, 40 }, { 47, 24 }, { 24, 47 }, { 55, 18 },
+  { 18, 55 }, { 58, 16 }, { 16, 58 }, { 39, 31 },
+  { 31, 39 }, { 51, 21 }, { 21, 51 }, { 38, 32 },
+  { 32, 38 }, { 37, 33 }, { 33, 37 }, { 46, 25 },
+  { 25, 46 }, { 36, 34 }, { 34, 36 }, { 35, 35 },
+  { 54, 19 }, { 19, 54 }, { 45, 26 }, { 26, 45 },
+  { 50, 22 }, { 22, 50 }, { 57, 17 }, { 17, 57 },
+  { 44, 27 }, { 27, 44 }, { 49, 23 }, { 23, 49 },
+  { 53, 20 }, { 20, 53 }, { 43, 28 }, { 28, 43 },
+  { 42, 29 }, { 29, 42 }, { 56, 18 }, { 18, 56 },
+  { 59, 16 }, { 16, 59 }, { 48, 24 }, { 24, 48 },
+  { 41, 30 }, { 30, 41 }, { 52, 21 }, { 21, 52 },
+  { 40, 31 }, { 31, 40 }, { 39, 32 }, { 32, 39 },
+  { 47, 25 }, { 25, 47 }, { 38, 33 }, { 33, 38 },
+  { 37, 34 }, { 34, 37 }, { 36, 35 }, { 35, 36 },
+  { 55, 19 }, { 19, 55 }, { 51, 22 }, { 22, 51 },
+  { 46, 26 }, { 26, 46 }, { 58, 17 }, { 17, 58 },
+  { 45, 27 }, { 27, 45 }, { 50, 23 }, { 23, 50 },
+  { 54, 20 }, { 20, 54 }, { 44, 28 }, { 28, 44 },
+  { 57, 18 }, { 18, 57 }, { 60, 16 }, { 16, 60 },
+  { 43, 29 }, { 29, 43 }, { 49, 24 }, { 24, 49 },
+  { 53, 21 }, { 21, 53 }, { 42, 30 }, { 30, 42 },
+  { 41, 31 }, { 31, 41 }, { 48, 25 }, { 25, 48 },
+  { 40, 32 }, { 32, 40 }, { 56, 19 }, { 19, 56 },
+  { 39, 33 }, { 33, 39 }, { 59, 17 }, { 17, 59 },
+  { 52, 22 }, { 22, 52 }, { 38, 34 }, { 34, 38 },
+  { 37, 35 }, { 35, 37 }, { 36, 36 }, { 47, 26 },
+  { 26, 47 }, { 46, 27 }, { 27, 46 }, { 55, 20 },
+  { 20, 55 }, { 51, 23 }, { 23, 51 }, { 45, 28 },
+  { 28, 45 }, { 61, 16 }, { 16, 61 }, { 58, 18 },
+  { 18, 58 }, { 50, 24 }, { 24, 50 }, { 44, 29 },
+  { 29, 44 }, { 54, 21 }, { 21, 54 }, { 43, 30 },
+  { 30, 43 }, { 49, 25 }, { 25, 49 }, { 42, 31 },
+  { 31, 42 }, { 57, 19 }, { 19, 57 }, { 60, 17 },
+  { 17, 60 }, { 41, 32 }, { 32, 41 }, { 53, 22 },
+  { 22, 53 }, { 40, 33 }, { 33, 40 }, { 48, 26 },
+  { 26, 48 }, { 39, 34 }, { 34, 39 }, { 38, 35 },
+  { 35, 38 }, { 37, 36 }, { 36, 37 }, { 56, 20 },
+  { 20, 56 }, { 47, 27 }, { 27, 47 }, { 52, 23 },
+  { 23, 52 }, { 62, 16 }, { 16, 62 }, { 59, 18 },
+  { 18, 59 }, { 46, 28 }, { 28, 46 }, { 51, 24 },
+  { 24, 51 }, { 55, 21 }, { 21, 55 }, { 45, 29 },
+  { 29, 45 }, { 44, 30 }, { 30, 44 }, { 58, 19 },
+  { 19, 58 }, { 50, 25 }, { 25, 50 }, { 61, 17 },
+  { 17, 61 }, { 43, 31 }, { 31, 43 }, { 54, 22 },
+  { 22, 54 }, { 42, 32 }, { 32, 42 }, { 49, 26 },
+  { 26, 49 }, { 41, 33 }, { 33, 41 }, { 40, 34 },
+  { 34, 40 }, { 57, 20 }, { 20, 57 }, { 39, 35 },
+  { 35, 39 }, { 38, 36 }, { 36, 38 }, { 37, 37 },
+  { 53, 23 }, { 23, 53 }, { 48, 27 }, { 27, 48 },
+  { 63, 16 }, { 16, 63 }, { 60, 18 }, { 18, 60 },
+  { 47, 28 }, { 28, 47 }, { 52, 24 }, { 24, 52 },
+  { 56, 21 }, { 21, 56 }, { 46, 29 }, { 29, 46 },
+  { 59, 19 }, { 19, 59 }, { 62, 17 }, { 17, 62 },
+  { 45, 30 }, { 30, 45 }, { 51, 25 }, { 25, 51 },
+  { 55, 22 }, { 22, 55 }, { 44, 31 }, { 31, 44 },
+  { 43, 32 }, { 32, 43 }, { 50, 26 }, { 26, 50 },
+  { 42, 33 }, { 33, 42 }, { 58, 20 }, { 20, 58 },
+  { 41, 34 }, { 34, 41 }, { 54, 23 }, { 23, 54 },
+  { 61, 18 }, { 18, 61 }, { 40, 35 }, { 35, 40 },
+  { 49, 27 }, { 27, 49 }, { 39, 36 }, { 36, 39 },
+  { 38, 37 }, { 37, 38 }, { 48, 28 }, { 28, 48 },
+  { 57, 21 }, { 21, 57 }, { 53, 24 }, { 24, 53 },
+  { 47, 29 }, { 29, 47 }, { 60, 19 }, { 19, 60 },
+  { 63, 17 }, { 17, 63 }, { 52, 25 }, { 25, 52 },
+  { 46, 30 }, { 30, 46 }, { 56, 22 }, { 22, 56 },
+  { 45, 31 }, { 31, 45 }, { 51, 26 }, { 26, 51 },
+  { 44, 32 }, { 32, 44 }, { 59, 20 }, { 20, 59 },
+  { 62, 18 }, { 18, 62 }, { 43, 33 }, { 33, 43 },
+  { 55, 23 }, { 23, 55 }, { 42, 34 }, { 34, 42 },
+  { 50, 27 }, { 27, 50 }, { 41, 35 }, { 35, 41 },
+  { 40, 36 }, { 36, 40 }, { 39, 37 }, { 37, 39 },
+  { 38, 38 }, { 58, 21 }, { 21, 58 }, { 49, 28 },
+  { 28, 49 }, { 54, 24 }, { 24, 54 }, { 61, 19 },
+  { 19, 61 }, { 48, 29 }, { 29, 48 }, { 53, 25 },
+  { 25, 53 }, { 57, 22 }, { 22, 57 }, { 47, 30 },
+  { 30, 47 }, { 46, 31 }, { 31, 46 }, { 60, 20 },
+  { 20, 60 }, { 52, 26 }, { 26, 52 }, { 63, 18 },
+  { 18, 63 }, { 45, 32 }, { 32, 45 }, { 56, 23 },
+  { 23, 56 }, { 44, 33 }, { 33, 44 }, { 51, 27 },
+  { 27, 51 }, { 43, 34 }, { 34, 43 }, { 42, 35 },
+  { 35, 42 }, { 59, 21 }, { 21, 59 }, { 41, 36 },
+  { 36, 41 }, { 55, 24 }, { 24, 55 }, { 40, 37 },
+  { 37, 40 }, { 50, 28 }, { 28, 50 }, { 39, 38 },
+  { 38, 39 }, { 62, 19 }, { 19, 62 }, { 49, 29 },
+  { 29, 49 }, { 54, 25 }, { 25, 54 }, { 58, 22 },
+  { 22, 58 }, { 48, 30 }, { 30, 48 }, { 61, 20 },
+  { 20, 61 }, { 47, 31 }, { 31, 47 }, { 53, 26 },
+  { 26, 53 }, { 57, 23 }, { 23, 57 }, { 46, 32 },
+  { 32, 46 }, { 52, 27 }, { 27, 52 }, { 45, 33 },
+  { 33, 45 }, { 60, 21 }, { 21, 60 }, { 44, 34 },
+  { 34, 44 }, { 56, 24 }, { 24, 56 }, { 43, 35 },
+  { 35, 43 }, { 63, 19 }, { 19, 63 }, { 51, 28 },
+  { 28, 51 }, { 42, 36 }, { 36, 42 }, { 41, 37 },
+  { 37, 41 }, { 40, 38 }, { 38, 40 }, { 39, 39 },
+  { 50, 29 }, { 29, 50 }, { 55, 25 }, { 25, 55 },
+  { 59, 22 }, { 22, 59 }, { 49, 30 }, { 30, 49 },
+  { 62, 20 }, { 20, 62 }, { 54, 26 }, { 26, 54 },
+  { 48, 31 }, { 31, 48 }, { 58, 23 }, { 23, 58 },
+  { 47, 32 }, { 32, 47 }, { 53, 27 }, { 27, 53 },
+  { 46, 33 }, { 33, 46 }, { 61, 21 }, { 21, 61 },
+  { 57, 24 }, { 24, 57 }, { 45, 34 }, { 34, 45 },
+  { 52, 28 }, { 28, 52 }, { 44, 35 }, { 35, 44 },
+  { 43, 36 }, { 36, 43 }, { 42, 37 }, { 37, 42 },
+  { 41, 38 }, { 38, 41 }, { 40, 39 }, { 39, 40 },
+  { 60, 22 }, { 22, 60 }, { 51, 29 }, { 29, 51 },
+  { 56, 25 }, { 25, 56 }, { 63, 20 }, { 20, 63 },
+  { 50, 30 }, { 30, 50 }, { 55, 26 }, { 26, 55 },
+  { 59, 23 }, { 23, 59 }, { 49, 31 }, { 31, 49 },
+  { 48, 32 }, { 32, 48 }, { 54, 27 }, { 27, 54 },
+  { 62, 21 }, { 21, 62 }, { 47, 33 }, { 33, 47 },
+  { 58, 24 }, { 24, 58 }, { 46, 34 }, { 34, 46 },
+  { 53, 28 }, { 28, 53 }, { 45, 35 }, { 35, 45 },
+  { 44, 36 }, { 36, 44 }, { 61, 22 }, { 22, 61 },
+  { 57, 25 }, { 25, 57 }, { 52, 29 }, { 29, 52 },
+  { 43, 37 }, { 37, 43 }, { 42, 38 }, { 38, 42 },
+  { 41, 39 }, { 39, 41 }, { 40, 40 }, { 51, 30 },
+  { 30, 51 }, { 56, 26 }, { 26, 56 }, { 60, 23 },
+  { 23, 60 }, { 50, 31 }, { 31, 50 }, { 63, 21 },
+  { 21, 63 }, { 55, 27 }, { 27, 55 }, { 49, 32 },
+  { 32, 49 }, { 59, 24 }, { 24, 59 }, { 48, 33 },
+  { 33, 48 }, { 54, 28 }, { 28, 54 }, { 47, 34 },
+  { 34, 47 }, { 62, 22 }, { 22, 62 }, { 46, 35 },
+  { 35, 46 }, { 58, 25 }, { 25, 58 }, { 45, 36 },
+  { 36, 45 }, { 53, 29 }, { 29, 53 }, { 44, 37 },
+  { 37, 44 }, { 43, 38 }, { 38, 43 }, { 42, 39 },
+  { 39, 42 }, { 41, 40 }, { 40, 41 }, { 52, 30 },
+  { 30, 52 }, { 57, 26 }, { 26, 57 }, { 61, 23 },
+  { 23, 61 }, { 51, 31 }, { 31, 51 }, { 56, 27 },
+  { 27, 56 }, { 50, 32 }, { 32, 50 }, { 60, 24 },
+  { 24, 60 }, { 49, 33 }, { 33, 49 }, { 55, 28 },
+  { 28, 55 }, { 63, 22 }, { 22, 63 }, { 48, 34 },
+  { 34, 48 }, { 59, 25 }, { 25, 59 }, { 47, 35 },
+  { 35, 47 }, { 54, 29 }, { 29, 54 }, { 46, 36 },
+  { 36, 46 }, { 45, 37 }, { 37, 45 }, { 44, 38 },
+  { 38, 44 }, { 53, 30 }, { 30, 53 }, { 62, 23 },
+  { 23, 62 }, { 58, 26 }, { 26, 58 }, { 43, 39 },
+  { 39, 43 }, { 42, 40 }, { 40, 42 }, { 41, 41 },
+  { 52, 31 }, { 31, 52 }, { 57, 27 }, { 27, 57 },
+  { 61, 24 }, { 24, 61 }, { 51, 32 }, { 32, 51 },
+  { 50, 33 }, { 33, 50 }, { 56, 28 }, { 28, 56 },
+  { 49, 34 }, { 34, 49 }, { 60, 25 }, { 25, 60 },
+  { 48, 35 }, { 35, 48 }, { 55, 29 }, { 29, 55 },
+  { 47, 36 }, { 36, 47 }, { 63, 23 }, { 23, 63 },
+  { 46, 37 }, { 37, 46 }, { 59, 26 }, { 26, 59 },
+  { 54, 30 }, { 30, 54 }, { 45, 38 }, { 38, 45 },
+  { 44, 39 }, { 39, 44 }, { 43, 40 }, { 40, 43 },
+  { 42, 41 }, { 41, 42 }, { 53, 31 }, { 31, 53 },
+  { 58, 27 }, { 27, 58 }, { 62, 24 }, { 24, 62 },
+  { 52, 32 }, { 32, 52 }, { 57, 28 }, { 28, 57 },
+  { 51, 33 }, { 33, 51 }, { 61, 25 }, { 25, 61 },
+  { 50, 34 }, { 34, 50 }, { 56, 29 }, { 29, 56 },
+  { 49, 35 }, { 35, 49 }, { 48, 36 }, { 36, 48 },
+  { 60, 26 }, { 26, 60 }, { 55, 30 }, { 30, 55 },
+  { 47, 37 }, { 37, 47 }, { 46, 38 }, { 38, 46 },
+  { 45, 39 }, { 39, 45 }, { 44, 40 }, { 40, 44 },
+  { 54, 31 }, { 31, 54 }, { 43, 41 }, { 41, 43 },
+  { 42, 42 }, { 59, 27 }, { 27, 59 }, { 63, 24 },
+  { 24, 63 }, { 53, 32 }, { 32, 53 }, { 58, 28 },
+  { 28, 58 }, { 52, 33 }, { 33, 52 }, { 62, 25 },
+  { 25, 62 }, { 51, 34 }, { 34, 51 }, { 57, 29 },
+  { 29, 57 }, { 50, 35 }, { 35, 50 }, { 61, 26 },
+  { 26, 61 }, { 49, 36 }, { 36, 49 }, { 56, 30 },
+  { 30, 56 }, { 48, 37 }, { 37, 48 }, { 47, 38 },
+  { 38, 47 }, { 55, 31 }, { 31, 55 }, { 60, 27 },
+  { 27, 60 }, { 46, 39 }, { 39, 46 }, { 45, 40 },
+  { 40, 45 }, { 44, 41 }, { 41, 44 }, { 43, 42 },
+  { 42, 43 }, { 54, 32 }, { 32, 54 }, { 59, 28 },
+  { 28, 59 }, { 63, 25 }, { 25, 63 }, { 53, 33 },
+  { 33, 53 }, { 52, 34 }, { 34, 52 }, { 58, 29 },
+  { 29, 58 }, { 51, 35 }, { 35, 51 }, { 62, 26 },
+  { 26, 62 }, { 57, 30 }, { 30, 57 }, { 50, 36 },
+  { 36, 50 }, { 49, 37 }, { 37, 49 }, { 61, 27 },
+  { 27, 61 }, { 48, 38 }, { 38, 48 }, { 56, 31 },
+  { 31, 56 }, { 47, 39 }, { 39, 47 }, { 46, 40 },
+  { 40, 46 }, { 45, 41 }, { 41, 45 }, { 44, 42 },
+  { 42, 44 }, { 43, 43 }, { 55, 32 }, { 32, 55 },
+  { 60, 28 }, { 28, 60 }, { 54, 33 }, { 33, 54 },
+  { 59, 29 }, { 29, 59 }, { 53, 34 }, { 34, 53 },
+  { 63, 26 }, { 26, 63 }, { 52, 35 }, { 35, 52 },
+  { 58, 30 }, { 30, 58 }, { 51, 36 }, { 36, 51 },
+  { 50, 37 }, { 37, 50 }, { 62, 27 }, { 27, 62 },
+  { 57, 31 }, { 31, 57 }, { 49, 38 }, { 38, 49 },
+  { 48, 39 }, { 39, 48 }, { 47, 40 }, { 40, 47 },
+  { 56, 32 }, { 32, 56 }, { 46, 41 }, { 41, 46 },
+  { 61, 28 }, { 28, 61 }, { 45, 42 }, { 42, 45 },
+  { 44, 43 }, { 43, 44 }, { 55, 33 }, { 33, 55 },
+  { 60, 29 }, { 29, 60 }, { 54, 34 }, { 34, 54 },
+  { 53, 35 }, { 35, 53 }, { 59, 30 }, { 30, 59 },
+  { 52, 36 }, { 36, 52 }, { 63, 27 }, { 27, 63 },
+  { 51, 37 }, { 37, 51 }, { 58, 31 }, { 31, 58 },
+  { 50, 38 }, { 38, 50 }, { 49, 39 }, { 39, 49 },
+  { 57, 32 }, { 32, 57 }, { 62, 28 }, { 28, 62 },
+  { 48, 40 }, { 40, 48 }, { 47, 41 }, { 41, 47 },
+  { 46, 42 }, { 42, 46 }, { 45, 43 }, { 43, 45 },
+  { 44, 44 }, { 56, 33 }, { 33, 56 }, { 61, 29 },
+  { 29, 61 }, { 55, 34 }, { 34, 55 }, { 54, 35 },
+  { 35, 54 }, { 60, 30 }, { 30, 60 }, { 53, 36 },
+  { 36, 53 }, { 59, 31 }, { 31, 59 }, { 52, 37 },
+  { 37, 52 }, { 51, 38 }, { 38, 51 }, { 63, 28 },
+  { 28, 63 }, { 58, 32 }, { 32, 58 }, { 50, 39 },
+  { 39, 50 }, { 49, 40 }, { 40, 49 }, { 48, 41 },
+  { 41, 48 }, { 57, 33 }, { 33, 57 }, { 47, 42 },
+  { 42, 47 }, { 46, 43 }, { 43, 46 }, { 45, 44 },
+  { 44, 45 }, { 62, 29 }, { 29, 62 }, { 56, 34 },
+  { 34, 56 }, { 61, 30 }, { 30, 61 }, { 55, 35 },
+  { 35, 55 }, { 54, 36 }, { 36, 54 }, { 60, 31 },
+  { 31, 60 }, { 53, 37 }, { 37, 53 }, { 52, 38 },
+  { 38, 52 }, { 59, 32 }, { 32, 59 }, { 51, 39 },
+  { 39, 51 }, { 50, 40 }, { 40, 50 }, { 58, 33 },
+  { 33, 58 }, { 49, 41 }, { 41, 49 }, { 63, 29 },
+  { 29, 63 }, { 48, 42 }, { 42, 48 }, { 47, 43 },
+  { 43, 47 }, { 46, 44 }, { 44, 46 }, { 45, 45 },
+  { 57, 34 }, { 34, 57 }, { 62, 30 }, { 30, 62 },
+  { 56, 35 }, { 35, 56 }, { 55, 36 }, { 36, 55 },
+  { 61, 31 }, { 31, 61 }, { 54, 37 }, { 37, 54 },
+  { 60, 32 }, { 32, 60 }, { 53, 38 }, { 38, 53 },
+  { 52, 39 }, { 39, 52 }, { 51, 40 }, { 40, 51 },
+  { 59, 33 }, { 33, 59 }, { 50, 41 }, { 41, 50 },
+  { 49, 42 }, { 42, 49 }, { 48, 43 }, { 43, 48 },
+  { 58, 34 }, { 34, 58 }, { 47, 44 }, { 44, 47 },
+  { 46, 45 }, { 45, 46 }, { 63, 30 }, { 30, 63 },
+  { 57, 35 }, { 35, 57 }, { 62, 31 }, { 31, 62 },
+  { 56, 36 }, { 36, 56 }, { 55, 37 }, { 37, 55 },
+  { 61, 32 }, { 32, 61 }, { 54, 38 }, { 38, 54 },
+  { 53, 39 }, { 39, 53 }, { 60, 33 }, { 33, 60 },
+  { 52, 40 }, { 40, 52 }, { 51, 41 }, { 41, 51 },
+  { 50, 42 }, { 42, 50 }, { 59, 34 }, { 34, 59 },
+  { 49, 43 }, { 43, 49 }, { 48, 44 }, { 44, 48 },
+  { 47, 45 }, { 45, 47 }, { 46, 46 }, { 58, 35 },
+  { 35, 58 }, { 63, 31 }, { 31, 63 }, { 57, 36 },
+  { 36, 57 }, { 56, 37 }, { 37, 56 }, { 62, 32 },
+  { 32, 62 }, { 55, 38 }, { 38, 55 }, { 54, 39 },
+  { 39, 54 }, { 61, 33 }, { 33, 61 }, { 53, 40 },
+  { 40, 53 }, { 52, 41 }, { 41, 52 }, { 60, 34 },
+  { 34, 60 }, { 51, 42 }, { 42, 51 }, { 50, 43 },
+  { 43, 50 }, { 49, 44 }, { 44, 49 }, { 48, 45 },
+  { 45, 48 }, { 59, 35 }, { 35, 59 }, { 47, 46 },
+  { 46, 47 }, { 58, 36 }, { 36, 58 }, { 57, 37 },
+  { 37, 57 }, { 63, 32 }, { 32, 63 }, { 56, 38 },
+  { 38, 56 }, { 62, 33 }, { 33, 62 }, { 55, 39 },
+  { 39, 55 }, { 54, 40 }, { 40, 54 }, { 61, 34 },
+  { 34, 61 }, { 53, 41 }, { 41, 53 }, { 52, 42 },
+  { 42, 52 }, { 51, 43 }, { 43, 51 }, { 60, 35 },
+  { 35, 60 }, { 50, 44 }, { 44, 50 }, { 49, 45 },
+  { 45, 49 }, { 48, 46 }, { 46, 48 }, { 47, 47 },
+  { 59, 36 }, { 36, 59 }, { 58, 37 }, { 37, 58 },
+  { 57, 38 }, { 38, 57 }, { 63, 33 }, { 33, 63 },
+  { 56, 39 }, { 39, 56 }, { 55, 40 }, { 40, 55 },
+  { 62, 34 }, { 34, 62 }, { 54, 41 }, { 41, 54 },
+  { 53, 42 }, { 42, 53 }, { 61, 35 }, { 35, 61 },
+  { 52, 43 }, { 43, 52 }, { 51, 44 }, { 44, 51 },
+  { 50, 45 }, { 45, 50 }, { 49, 46 }, { 46, 49 },
+  { 48, 47 }, { 47, 48 }, { 60, 36 }, { 36, 60 },
+  { 59, 37 }, { 37, 59 }, { 58, 38 }, { 38, 58 },
+  { 57, 39 }, { 39, 57 }, { 56, 40 }, { 40, 56 },
+  { 63, 34 }, { 34, 63 }, { 55, 41 }, { 41, 55 },
+  { 54, 42 }, { 42, 54 }, { 62, 35 }, { 35, 62 },
+  { 53, 43 }, { 43, 53 }, { 52, 44 }, { 44, 52 },
+  { 51, 45 }, { 45, 51 }, { 61, 36 }, { 36, 61 },
+  { 50, 46 }, { 46, 50 }, { 49, 47 }, { 47, 49 },
+  { 48, 48 }, { 60, 37 }, { 37, 60 }, { 59, 38 },
+  { 38, 59 }, { 58, 39 }, { 39, 58 }, { 57, 40 },
+  { 40, 57 }, { 56, 41 }, { 41, 56 }, { 63, 35 },
+  { 35, 63 }, { 55, 42 }, { 42, 55 }, { 54, 43 },
+  { 43, 54 }, { 53, 44 }, { 44, 53 }, { 62, 36 },
+  { 36, 62 }, { 52, 45 }, { 45, 52 }, { 51, 46 },
+  { 46, 51 }, { 50, 47 }, { 47, 50 }, { 49, 48 },
+  { 48, 49 }, { 61, 37 }, { 37, 61 }, { 60, 38 },
+  { 38, 60 }, { 59, 39 }, { 39, 59 }, { 58, 40 },
+  { 40, 58 }, { 57, 41 }, { 41, 57 }, { 56, 42 },
+  { 42, 56 }, { 55, 43 }, { 43, 55 }, { 63, 36 },
+  { 36, 63 }, { 54, 44 }, { 44, 54 }, { 53, 45 },
+  { 45, 53 }, { 52, 46 }, { 46, 52 }, { 62, 37 },
+  { 37, 62 }, { 51, 47 }, { 47, 51 }, { 50, 48 },
+  { 48, 50 }, { 49, 49 }, { 61, 38 }, { 38, 61 },
+  { 60, 39 }, { 39, 60 }, { 59, 40 }, { 40, 59 },
+  { 58, 41 }, { 41, 58 }, { 57, 42 }, { 42, 57 },
+  { 56, 43 }, { 43, 56 }, { 55, 44 }, { 44, 55 },
+  { 54, 45 }, { 45, 54 }, { 63, 37 }, { 37, 63 },
+  { 53, 46 }, { 46, 53 }, { 52, 47 }, { 47, 52 },
+  { 51, 48 }, { 48, 51 }, { 50, 49 }, { 49, 50 },
+  { 62, 38 }, { 38, 62 }, { 61, 39 }, { 39, 61 },
+  { 60, 40 }, { 40, 60 }, { 59, 41 }, { 41, 59 },
+  { 58, 42 }, { 42, 58 }, { 57, 43 }, { 43, 57 },
+  { 56, 44 }, { 44, 56 }, { 55, 45 }, { 45, 55 },
+  { 54, 46 }, { 46, 54 }, { 53, 47 }, { 47, 53 },
+  { 52, 48 }, { 48, 52 }, { 63, 38 }, { 38, 63 },
+  { 51, 49 }, { 49, 51 }, { 50, 50 }, { 62, 39 },
+  { 39, 62 }, { 61, 40 }, { 40, 61 }, { 60, 41 },
+  { 41, 60 }, { 59, 42 }, { 42, 59 }, { 58, 43 },
+  { 43, 58 }, { 57, 44 }, { 44, 57 }, { 56, 45 },
+  { 45, 56 }, { 55, 46 }, { 46, 55 }, { 54, 47 },
+  { 47, 54 }, { 53, 48 }, { 48, 53 }, { 52, 49 },
+  { 49, 52 }, { 51, 50 }, { 50, 51 }, { 63, 39 },
+  { 39, 63 }, { 62, 40 }, { 40, 62 }, { 61, 41 },
+  { 41, 61 }, { 60, 42 }, { 42, 60 }, { 59, 43 },
+  { 43, 59 }, { 58, 44 }, { 44, 58 }, { 57, 45 },
+  { 45, 57 }, { 56, 46 }, { 46, 56 }, { 55, 47 },
+  { 47, 55 }, { 54, 48 }, { 48, 54 }, { 53, 49 },
+  { 49, 53 }, { 52, 50 }, { 50, 52 }, { 51, 51 },
+  { 63, 40 }, { 40, 63 }, { 62, 41 }, { 41, 62 },
+  { 61, 42 }, { 42, 61 }, { 60, 43 }, { 43, 60 },
+  { 59, 44 }, { 44, 59 }, { 58, 45 }, { 45, 58 },
+  { 57, 46 }, { 46, 57 }, { 56, 47 }, { 47, 56 },
+  { 55, 48 }, { 48, 55 }, { 54, 49 }, { 49, 54 },
+  { 53, 50 }, { 50, 53 }, { 52, 51 }, { 51, 52 },
+  { 63, 41 }, { 41, 63 }, { 62, 42 }, { 42, 62 },
+  { 61, 43 }, { 43, 61 }, { 60, 44 }, { 44, 60 },
+  { 59, 45 }, { 45, 59 }, { 58, 46 }, { 46, 58 },
+  { 57, 47 }, { 47, 57 }, { 56, 48 }, { 48, 56 },
+  { 55, 49 }, { 49, 55 }, { 54, 50 }, { 50, 54 },
+  { 53, 51 }, { 51, 53 }, { 52, 52 }, { 63, 42 },
+  { 42, 63 }, { 62, 43 }, { 43, 62 }, { 61, 44 },
+  { 44, 61 }, { 60, 45 }, { 45, 60 }, { 59, 46 },
+  { 46, 59 }, { 58, 47 }, { 47, 58 }, { 57, 48 },
+  { 48, 57 }, { 56, 49 }, { 49, 56 }, { 55, 50 },
+  { 50, 55 }, { 54, 51 }, { 51, 54 }, { 53, 52 },
+  { 52, 53 }, { 63, 43 }, { 43, 63 }, { 62, 44 },
+  { 44, 62 }, { 61, 45 }, { 45, 61 }, { 60, 46 },
+  { 46, 60 }, { 59, 47 }, { 47, 59 }, { 58, 48 },
+  { 48, 58 }, { 57, 49 }, { 49, 57 }, { 56, 50 },
+  { 50, 56 }, { 55, 51 }, { 51, 55 }, { 54, 52 },
+  { 52, 54 }, { 53, 53 }, { 63, 44 }, { 44, 63 },
+  { 62, 45 }, { 45, 62 }, { 61, 46 }, { 46, 61 },
+  { 60, 47 }, { 47, 60 }, { 59, 48 }, { 48, 59 },
+  { 58, 49 }, { 49, 58 }, { 57, 50 }, { 50, 57 },
+  { 56, 51 }, { 51, 56 }, { 55, 52 }, { 52, 55 },
+  { 54, 53 }, { 53, 54 }, { 63, 45 }, { 45, 63 },
+  { 62, 46 }, { 46, 62 }, { 61, 47 }, { 47, 61 },
+  { 60, 48 }, { 48, 60 }, { 59, 49 }, { 49, 59 },
+  { 58, 50 }, { 50, 58 }, { 57, 51 }, { 51, 57 },
+  { 56, 52 }, { 52, 56 }, { 55, 53 }, { 53, 55 },
+  { 54, 54 }, { 63, 46 }, { 46, 63 }, { 62, 47 },
+  { 47, 62 }, { 61, 48 }, { 48, 61 }, { 60, 49 },
+  { 49, 60 }, { 59, 50 }, { 50, 59 }, { 58, 51 },
+  { 51, 58 }, { 57, 52 }, { 52, 57 }, { 56, 53 },
+  { 53, 56 }, { 55, 54 }, { 54, 55 }, { 63, 47 },
+  { 47, 63 }, { 62, 48 }, { 48, 62 }, { 61, 49 },
+  { 49, 61 }, { 60, 50 }, { 50, 60 }, { 59, 51 },
+  { 51, 59 }, { 58, 52 }, { 52, 58 }, { 57, 53 },
+  { 53, 57 }, { 56, 54 }, { 54, 56 }, { 55, 55 },
+  { 63, 48 }, { 48, 63 }, { 62, 49 }, { 49, 62 },
+  { 61, 50 }, { 50, 61 }, { 60, 51 }, { 51, 60 },
+  { 59, 52 }, { 52, 59 }, { 58, 53 }, { 53, 58 },
+  { 57, 54 }, { 54, 57 }, { 56, 55 }, { 55, 56 },
+  { 63, 49 }, { 49, 63 }, { 62, 50 }, { 50, 62 },
+  { 61, 51 }, { 51, 61 }, { 60, 52 }, { 52, 60 },
+  { 59, 53 }, { 53, 59 }, { 58, 54 }, { 54, 58 },
+  { 57, 55 }, { 55, 57 }, { 56, 56 }, { 63, 50 },
+  { 50, 63 }, { 62, 51 }, { 51, 62 }, { 61, 52 },
+  { 52, 61 }, { 60, 53 }, { 53, 60 }, { 59, 54 },
+  { 54, 59 }, { 58, 55 }, { 55, 58 }, { 57, 56 },
+  { 56, 57 }, { 63, 51 }, { 51, 63 }, { 62, 52 },
+  { 52, 62 }, { 61, 53 }, { 53, 61 }, { 60, 54 },
+  { 54, 60 }, { 59, 55 }, { 55, 59 }, { 58, 56 },
+  { 56, 58 }, { 57, 57 }, { 63, 52 }, { 52, 63 },
+  { 62, 53 }, { 53, 62 }, { 61, 54 }, { 54, 61 },
+  { 60, 55 }, { 55, 60 }, { 59, 56 }, { 56, 59 },
+  { 58, 57 }, { 57, 58 }, { 63, 53 }, { 53, 63 },
+  { 62, 54 }, { 54, 62 }, { 61, 55 }, { 55, 61 },
+  { 60, 56 }, { 56, 60 }, { 59, 57 }, { 57, 59 },
+  { 58, 58 }, { 63, 54 }, { 54, 63 }, { 62, 55 },
+  { 55, 62 }, { 61, 56 }, { 56, 61 }, { 60, 57 },
+  { 57, 60 }, { 59, 58 }, { 58, 59 }, { 63, 55 },
+  { 55, 63 }, { 62, 56 }, { 56, 62 }, { 61, 57 },
+  { 57, 61 }, { 60, 58 }, { 58, 60 }, { 59, 59 },
+  { 63, 56 }, { 56, 63 }, { 62, 57 }, { 57, 62 },
+  { 61, 58 }, { 58, 61 }, { 60, 59 }, { 59, 60 },
+  { 63, 57 }, { 57, 63 }, { 62, 58 }, { 58, 62 },
+  { 61, 59 }, { 59, 61 }, { 60, 60 }, { 63, 58 },
+  { 58, 63 }, { 62, 59 }, { 59, 62 }, { 61, 60 },
+  { 60, 61 }, { 63, 59 }, { 59, 63 }, { 62, 60 },
+  { 60, 62 }, { 61, 61 }, { 63, 60 }, { 60, 63 },
+  { 62, 61 }, { 61, 62 }, { 63, 61 }, { 61, 63 },
+  { 62, 62 }, { 63, 62 }, { 62, 63 }, { 63, 63 }
+};
diff --git a/av1/common/zigzag8.c b/av1/common/zigzag8.c
new file mode 100644
index 0000000..1ef932f
--- /dev/null
+++ b/av1/common/zigzag8.c
@@ -0,0 +1,64 @@
+/* This file is generated by gen_zigzag8.m */
+
+/* clang-format off */
+
+#include "odintrin.h"
+OD_EXTERN const unsigned char OD_ZIGZAG8_DCT_DCT[48][2] = {
+  {4, 0}, {4, 1}, {5, 0}, {5, 1},
+  {6, 0}, {7, 0}, {6, 1}, {7, 1},
+  {0, 4}, {1, 4}, {0, 5}, {1, 5},
+  {0, 6}, {1, 6}, {0, 7}, {1, 7},
+  {2, 4}, {4, 2}, {3, 4}, {2, 5},
+  {4, 3}, {5, 2}, {4, 4}, {3, 5},
+  {5, 3}, {2, 6}, {4, 5}, {6, 2},
+  {5, 4}, {3, 6}, {2, 7}, {6, 3},
+  {5, 5}, {7, 2}, {4, 6}, {3, 7},
+  {6, 4}, {7, 3}, {4, 7}, {5, 6},
+  {6, 5}, {7, 4}, {5, 7}, {6, 6},
+  {7, 5}, {6, 7}, {7, 6}, {7, 7}
+  };
+
+OD_EXTERN const unsigned char OD_ZIGZAG8_ADST_DCT[48][2] = {
+  {4, 0}, {5, 0}, {4, 1}, {6, 0},
+  {5, 1}, {7, 0}, {6, 1}, {7, 1},
+  {0, 4}, {1, 4}, {0, 5}, {1, 5},
+  {0, 6}, {1, 6}, {0, 7}, {1, 7},
+  {4, 2}, {2, 4}, {5, 2}, {4, 3},
+  {3, 4}, {2, 5}, {5, 3}, {4, 4},
+  {6, 2}, {3, 5}, {5, 4}, {2, 6},
+  {4, 5}, {6, 3}, {7, 2}, {3, 6},
+  {2, 7}, {5, 5}, {6, 4}, {4, 6},
+  {7, 3}, {3, 7}, {5, 6}, {6, 5},
+  {4, 7}, {7, 4}, {5, 7}, {7, 5},
+  {6, 6}, {7, 6}, {6, 7}, {7, 7}
+  };
+
+OD_EXTERN const unsigned char OD_ZIGZAG8_DCT_ADST[48][2] = {
+  {4, 0}, {4, 1}, {5, 0}, {5, 1},
+  {6, 0}, {6, 1}, {7, 0}, {7, 1},
+  {0, 4}, {0, 5}, {1, 4}, {0, 6},
+  {1, 5}, {0, 7}, {1, 6}, {1, 7},
+  {2, 4}, {2, 5}, {3, 4}, {4, 2},
+  {2, 6}, {4, 3}, {3, 5}, {4, 4},
+  {2, 7}, {3, 6}, {5, 2}, {4, 5},
+  {5, 3}, {3, 7}, {5, 4}, {4, 6},
+  {6, 2}, {5, 5}, {4, 7}, {6, 3},
+  {6, 4}, {5, 6}, {7, 2}, {6, 5},
+  {7, 3}, {5, 7}, {7, 4}, {6, 6},
+  {7, 5}, {6, 7}, {7, 6}, {7, 7}
+  };
+
+OD_EXTERN const unsigned char OD_ZIGZAG8_ADST_ADST[48][2] = {
+  {4, 0}, {4, 1}, {5, 0}, {5, 1},
+  {6, 0}, {7, 0}, {6, 1}, {7, 1},
+  {0, 4}, {1, 4}, {0, 5}, {1, 5},
+  {0, 6}, {1, 6}, {0, 7}, {1, 7},
+  {2, 4}, {4, 2}, {3, 4}, {2, 5},
+  {4, 3}, {5, 2}, {4, 4}, {3, 5},
+  {5, 3}, {2, 6}, {4, 5}, {6, 2},
+  {5, 4}, {3, 6}, {2, 7}, {6, 3},
+  {5, 5}, {7, 2}, {4, 6}, {3, 7},
+  {6, 4}, {7, 3}, {4, 7}, {5, 6},
+  {6, 5}, {7, 4}, {5, 7}, {6, 6},
+  {7, 5}, {6, 7}, {7, 6}, {7, 7}
+  };
diff --git a/av1/decoder/decint.h b/av1/decoder/decint.h
new file mode 100644
index 0000000..076c8a3
--- /dev/null
+++ b/av1/decoder/decint.h
@@ -0,0 +1,45 @@
+/*Daala video codec
+Copyright (c) 2006-2013 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#if !defined(_decint_H)
+# define _decint_H (1)
+# include "av1/common/state.h"
+
+typedef struct daala_dec_ctx daala_dec_ctx;
+
+typedef struct daala_dec_ctx od_dec_ctx;
+
+/*Constants for the packet state machine specific to the decoder.*/
+/*Next packet to read: Data packet.*/
+# define OD_PACKET_DATA (0)
+
+struct daala_dec_ctx {
+  od_state state;
+  od_ec_dec *ec;
+  int qm;
+};
+
+#endif
diff --git a/av1/decoder/generic_decoder.c b/av1/decoder/generic_decoder.c
new file mode 100644
index 0000000..5c404b6
--- /dev/null
+++ b/av1/decoder/generic_decoder.c
@@ -0,0 +1,150 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#ifdef HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include <stdio.h>
+
+#include "aom_dsp/entdec.h"
+#include "av1/common/generic_code.h"
+#include "av1/common/odintrin.h"
+#include "pvq_decoder.h"
+
+/** Decodes a value from 0 to N-1 (with N up to 16) based on a cdf and adapts
+ * the cdf accordingly.
+ *
+ * @param [in,out] enc   range encoder
+ * @param [in,out] cdf   CDF of the variable (Q15)
+ * @param [in]     n     number of values possible
+ * @param [in,out] count number of symbols encoded with that cdf so far
+ * @param [in]     rate  adaptation rate shift (smaller is faster)
+ * @return decoded variable
+ */
+int od_decode_cdf_adapt_q15_(od_ec_dec *ec, uint16_t *cdf, int n,
+ int *count, int rate OD_ACC_STR) {
+  int val;
+  int i;
+  if (*count == 0) {
+    int ft;
+    ft = cdf[n - 1];
+    for (i = 0; i < n; i++) {
+      cdf[i] = cdf[i]*32768/ft;
+    }
+  }
+  val = od_ec_decode_cdf_q15(ec, cdf, n);
+  od_cdf_adapt_q15(val, cdf, n, count, rate);
+  return val;
+}
+
+/** Decodes a value from 0 to N-1 (with N up to 16) based on a cdf and adapts
+ * the cdf accordingly.
+ *
+ * @param [in,out] enc   range encoder
+ * @param [in]     cdf   CDF of the variable (Q15)
+ * @param [in]     n     number of values possible
+ * @param [in]     increment adaptation speed (Q15)
+ *
+ * @retval decoded variable
+ */
+int od_decode_cdf_adapt_(od_ec_dec *ec, uint16_t *cdf, int n,
+ int increment OD_ACC_STR) {
+  int i;
+  int val;
+  val = od_ec_decode_cdf_unscaled(ec, cdf, n);
+  if (cdf[n-1] + increment > 32767) {
+    for (i = 0; i < n; i++) {
+      /* Second term ensures that the pdf is non-null */
+      cdf[i] = (cdf[i] >> 1) + i + 1;
+    }
+  }
+  for (i = val; i < n; i++) cdf[i] += increment;
+  return val;
+}
+
+/** Encodes a random variable using a "generic" model, assuming that the
+ * distribution is one-sided (zero and up), has a single mode, and decays
+ * exponentially past the model.
+ *
+ * @param [in,out] dec   range decoder
+ * @param [in,out] model generic probability model
+ * @param [in]     x     variable being encoded
+ * @param [in,out] ExQ16 expectation of x (adapted)
+ * @param [in]     integration integration period of ExQ16 (leaky average over
+ * 1<<integration samples)
+ *
+ * @retval decoded variable x
+ */
+int generic_decode_(od_ec_dec *dec, generic_encoder *model, int max,
+ int *ex_q16, int integration OD_ACC_STR) {
+  int lg_q1;
+  int shift;
+  int id;
+  uint16_t *cdf;
+  int xs;
+  int lsb;
+  int x;
+  int ms;
+  lsb = 0;
+  if (max == 0) return 0;
+  lg_q1 = log_ex(*ex_q16);
+  /* If expectation is too large, shift x to ensure that
+     all we have past xs=15 is the exponentially decaying tail
+     of the distribution. */
+  shift = OD_MAXI(0, (lg_q1 - 5) >> 1);
+  /* Choose the cdf to use: we have two per "octave" of ExQ16. */
+  id = OD_MINI(GENERIC_TABLES - 1, lg_q1);
+  cdf = model->cdf[id];
+  ms = (max + (1 << shift >> 1)) >> shift;
+  if (max == -1) xs = od_ec_decode_cdf_unscaled(dec, cdf, 16);
+  else xs = od_ec_decode_cdf_unscaled(dec, cdf, OD_MINI(ms + 1, 16));
+  if (xs == 15) {
+    int e;
+    unsigned decay;
+    /* Estimate decay based on the assumption that the distribution is close
+       to Laplacian for large values. We should probably have an adaptive
+       estimate instead. Note: The 2* is a kludge that's not fully understood
+       yet. */
+    OD_ASSERT(*ex_q16 < INT_MAX >> 1);
+    e = ((2**ex_q16 >> 8) + (1 << shift >> 1)) >> shift;
+    decay = OD_MAXI(2, OD_MINI(254, 256*e/(e + 256)));
+    xs += laplace_decode_special(dec, decay, (max == -1) ? -1 : ms - 15, acc_str);
+  }
+  if (shift != 0) {
+    int special;
+    /* Because of the rounding, there's only half the number of possibilities
+       for xs=0 */
+    special = xs == 0;
+    if (shift - special > 0) lsb = od_ec_dec_bits(dec, shift - special, acc_str);
+    lsb -= !special << (shift - 1);
+  }
+  x = (xs << shift) + lsb;
+  generic_model_update(model, ex_q16, x, xs, id, integration);
+  OD_LOG((OD_LOG_ENTROPY_CODER, OD_LOG_DEBUG,
+   "dec: %d %d %d %d %d %x", *ex_q16, x, shift, id, xs, dec->rng));
+  return x;
+}
diff --git a/av1/decoder/laplace_decoder.c b/av1/decoder/laplace_decoder.c
new file mode 100644
index 0000000..a7e0f80
--- /dev/null
+++ b/av1/decoder/laplace_decoder.c
@@ -0,0 +1,337 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#ifdef HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include <stdio.h>
+
+#include "aom_dsp/entdec.h"
+#include "av1/common/pvq.h"
+#include "pvq_decoder.h"
+
+#if OD_ACCOUNTING
+# define od_decode_pvq_split(ec, adapt, sum, ctx, str) od_decode_pvq_split_(ec, adapt, sum, ctx, str)
+#else
+# define od_decode_pvq_split(ec, adapt, sum, ctx, str) od_decode_pvq_split_(ec, adapt, sum, ctx)
+#endif
+
+static int od_decode_pvq_split_(od_ec_dec *ec, od_pvq_codeword_ctx *adapt,
+ int sum, int ctx OD_ACC_STR) {
+  int shift;
+  int count;
+  int msbs;
+  int fctx;
+  count = 0;
+  if (sum == 0) return 0;
+  shift = OD_MAXI(0, OD_ILOG(sum) - 3);
+  fctx = 7*ctx + (sum >> shift) - 1;
+  msbs = od_decode_cdf_adapt(ec, adapt->pvq_split_cdf[fctx],
+   (sum >> shift) + 1, adapt->pvq_split_increment, acc_str);
+  if (shift) count = od_ec_dec_bits(ec, shift, acc_str);
+  count += msbs << shift;
+  if (count > sum) {
+    count = sum;
+    ec->error = 1;
+  }
+  return count;
+}
+
+void od_decode_band_pvq_splits(od_ec_dec *ec, od_pvq_codeword_ctx *adapt,
+ od_coeff *y, int n, int k, int level) {
+  int mid;
+  int count_right;
+  if (n == 1) {
+    y[0] = k;
+  }
+  else if (k == 0) {
+    OD_CLEAR(y, n);
+  }
+  else if (k == 1 && n <= 16) {
+    int cdf_id;
+    int pos;
+    cdf_id = od_pvq_k1_ctx(n, level == 0);
+    OD_CLEAR(y, n);
+    pos = od_decode_cdf_adapt(ec, adapt->pvq_k1_cdf[cdf_id], n,
+     adapt->pvq_k1_increment, "pvq:k1");
+    y[pos] = 1;
+  }
+  else {
+    mid = n >> 1;
+    count_right = od_decode_pvq_split(ec, adapt, k, od_pvq_size_ctx(n),
+     "pvq:split");
+    od_decode_band_pvq_splits(ec, adapt, y, mid, k - count_right, level + 1);
+    od_decode_band_pvq_splits(ec, adapt, y + mid, n - mid, count_right,
+     level + 1);
+  }
+}
+
+/** Decodes the tail of a Laplace-distributed variable, i.e. it doesn't
+ * do anything special for the zero case.
+ *
+ * @param [dec] range decoder
+ * @param [decay] decay factor of the distribution, i.e. pdf ~= decay^x
+ * @param [max] maximum possible value of x (used to truncate the pdf)
+ *
+ * @retval decoded variable x
+ */
+int laplace_decode_special_(od_ec_dec *dec, unsigned decay, int max OD_ACC_STR) {
+  int pos;
+  int shift;
+  int xs;
+  int ms;
+  int sym;
+  const uint16_t *cdf;
+  shift = 0;
+  if (max == 0) return 0;
+  /* We don't want a large decay value because that would require too many
+     symbols. However, it's OK if the max is below 15. */
+  while (((max >> shift) >= 15 || max == -1) && decay > 235) {
+    decay = (decay*decay + 128) >> 8;
+    shift++;
+  }
+  decay = OD_MINI(decay, 254);
+  decay = OD_MAXI(decay, 2);
+  ms = max >> shift;
+  cdf = EXP_CDF_TABLE[(decay + 1) >> 1];
+  OD_LOG((OD_LOG_PVQ, OD_LOG_DEBUG, "decay = %d\n", decay));
+  xs = 0;
+  do {
+    sym = OD_MINI(xs, 15);
+    {
+      int i;
+      OD_LOG((OD_LOG_PVQ, OD_LOG_DEBUG, "%d %d %d %d", xs, shift, sym, max));
+      for (i = 0; i < 16; i++) {
+        OD_LOG_PARTIAL((OD_LOG_PVQ, OD_LOG_DEBUG, "%d ", cdf[i]));
+      }
+      OD_LOG_PARTIAL((OD_LOG_PVQ, OD_LOG_DEBUG, "\n"));
+    }
+    if (ms > 0 && ms < 15) {
+      /* Simple way of truncating the pdf when we have a bound. */
+      sym = od_ec_decode_cdf_unscaled(dec, cdf, ms + 1);
+    }
+    else sym = od_ec_decode_cdf_q15(dec, cdf, 16);
+    xs += sym;
+    ms -= 15;
+  }
+  while (sym >= 15 && ms != 0);
+  if (shift) pos = (xs << shift) + od_ec_dec_bits(dec, shift, acc_str);
+  else pos = xs;
+  OD_ASSERT(pos >> shift <= max >> shift || max == -1);
+  if (max != -1 && pos > max) {
+    pos = max;
+    dec->error = 1;
+  }
+  OD_ASSERT(pos <= max || max == -1);
+  return pos;
+}
+
+/** Decodes a Laplace-distributed variable for use in PVQ.
+ *
+ * @param [in,out] dec  range decoder
+ * @param [in]     ExQ8 expectation of the absolute value of x
+ * @param [in]     K    maximum value of |x|
+ *
+ * @retval decoded variable (including sign)
+ */
+int laplace_decode_(od_ec_dec *dec, unsigned ex_q8, int k OD_ACC_STR) {
+  int j;
+  int shift;
+  uint16_t cdf[16];
+  int sym;
+  int lsb;
+  int decay;
+  int offset;
+  lsb = 0;
+  /* Shift down x if expectation is too high. */
+  shift = OD_ILOG(ex_q8) - 11;
+  if (shift < 0) shift = 0;
+  /* Apply the shift with rounding to Ex, K and xs. */
+  ex_q8 = (ex_q8 + (1 << shift >> 1)) >> shift;
+  k = (k + (1 << shift >> 1)) >> shift;
+  decay = OD_MINI(254, OD_DIVU(256*ex_q8, (ex_q8 + 256)));
+  offset = LAPLACE_OFFSET[(decay + 1) >> 1];
+  for (j = 0; j < 16; j++) {
+    cdf[j] = EXP_CDF_TABLE[(decay + 1) >> 1][j] - offset;
+  }
+  /* Simple way of truncating the pdf when we have a bound */
+  if (k == 0) sym = 0;
+  else sym = od_ec_decode_cdf_unscaled(dec, cdf, OD_MINI(k + 1, 16));
+  if (shift) {
+    int special;
+    /* Because of the rounding, there's only half the number of possibilities
+       for xs=0 */
+    special = (sym == 0);
+    if (shift - special > 0) lsb = od_ec_dec_bits(dec, shift - special, acc_str);
+    lsb -= (!special << (shift - 1));
+  }
+  /* Handle the exponentially-decaying tail of the distribution */
+  if (sym == 15) sym += laplace_decode_special(dec, decay, k - 15, acc_str);
+  return (sym << shift) + lsb;
+}
+
+#if OD_ACCOUNTING
+# define laplace_decode_vector_delta(dec, y, n, k, curr, means, str) laplace_decode_vector_delta_(dec, y, n, k, curr, means, str)
+#else
+# define laplace_decode_vector_delta(dec, y, n, k, curr, means, str) laplace_decode_vector_delta_(dec, y, n, k, curr, means)
+#endif
+
+static void laplace_decode_vector_delta_(od_ec_dec *dec, od_coeff *y, int n, int k,
+                                        int32_t *curr, const int32_t *means
+                                        OD_ACC_STR) {
+  int i;
+  int prev;
+  int sum_ex;
+  int sum_c;
+  int coef;
+  int pos;
+  int k0;
+  int sign;
+  int first;
+  int k_left;
+  prev = 0;
+  sum_ex = 0;
+  sum_c = 0;
+  coef = 256*means[OD_ADAPT_COUNT_Q8]/
+   (1 + means[OD_ADAPT_COUNT_EX_Q8]);
+  pos = 0;
+  sign = 0;
+  first = 1;
+  k_left = k;
+  for (i = 0; i < n; i++) y[i] = 0;
+  k0 = k_left;
+  coef = OD_MAXI(coef, 1);
+  for (i = 0; i < k0; i++) {
+    int count;
+    if (first) {
+      int decay;
+      int ex = coef*(n - prev)/k_left;
+      if (ex > 65280) decay = 255;
+      else {
+        decay = OD_MINI(255,
+         (int)((256*ex/(ex + 256) + (ex>>5)*ex/((n + 1)*(n - 1)*(n - 1)))));
+      }
+      /*Update mean position.*/
+      count = laplace_decode_special(dec, decay, n - 1, acc_str);
+      first = 0;
+    }
+    else count = laplace_decode(dec, coef*(n - prev)/k_left, n - prev - 1, acc_str);
+    sum_ex += 256*(n - prev);
+    sum_c += count*k_left;
+    pos += count;
+    OD_ASSERT(pos < n);
+    if (y[pos] == 0)
+      sign = od_ec_dec_bits(dec, 1, acc_str);
+    y[pos] += sign ? -1 : 1;
+    prev = pos;
+    k_left--;
+    if (k_left == 0) break;
+  }
+  if (k > 0) {
+    curr[OD_ADAPT_COUNT_Q8] = 256*sum_c;
+    curr[OD_ADAPT_COUNT_EX_Q8] = sum_ex;
+  }
+  else {
+    curr[OD_ADAPT_COUNT_Q8] = -1;
+    curr[OD_ADAPT_COUNT_EX_Q8] = 0;
+  }
+  curr[OD_ADAPT_K_Q8] = 0;
+  curr[OD_ADAPT_SUM_EX_Q8] = 0;
+}
+
+/** Decodes a vector of integers assumed to come from rounding a sequence of
+ * Laplace-distributed real values in decreasing order of variance.
+ *
+ * @param [in,out] dec range decoder
+ * @param [in]     y     decoded vector
+ * @param [in]     N     dimension of the vector
+ * @param [in]     K     sum of the absolute value of components of y
+ * @param [out]    curr  Adaptation context output, may alias means.
+ * @param [in]     means Adaptation context input.
+ */
+void laplace_decode_vector_(od_ec_dec *dec, od_coeff *y, int n, int k,
+                           int32_t *curr, const int32_t *means OD_ACC_STR) {
+  int i;
+  int sum_ex;
+  int kn;
+  int exp_q8;
+  int mean_k_q8;
+  int mean_sum_ex_q8;
+  int ran_delta;
+  ran_delta = 0;
+  if (k <= 1) {
+    laplace_decode_vector_delta(dec, y, n, k, curr, means, acc_str);
+    return;
+  }
+  if (k == 0) {
+    curr[OD_ADAPT_COUNT_Q8] = OD_ADAPT_NO_VALUE;
+    curr[OD_ADAPT_COUNT_EX_Q8] = OD_ADAPT_NO_VALUE;
+    curr[OD_ADAPT_K_Q8] = 0;
+    curr[OD_ADAPT_SUM_EX_Q8] = 0;
+    for (i = 0; i < n; i++) y[i] = 0;
+    return;
+  }
+  sum_ex = 0;
+  kn = k;
+  /* Estimates the factor relating pulses_left and positions_left to E(|x|).*/
+  mean_k_q8 = means[OD_ADAPT_K_Q8];
+  mean_sum_ex_q8 = means[OD_ADAPT_SUM_EX_Q8];
+  if (mean_k_q8 < 1 << 23) exp_q8 = 256*mean_k_q8/(1 + mean_sum_ex_q8);
+  else exp_q8 = mean_k_q8/(1 + (mean_sum_ex_q8 >> 8));
+  for (i = 0; i < n; i++) {
+    int ex;
+    int x;
+    if (kn == 0) break;
+    if (kn <= 1 && i != n - 1) {
+      laplace_decode_vector_delta(dec, y + i, n - i, kn, curr, means, acc_str);
+      ran_delta = 1;
+      i = n;
+      break;
+    }
+    /* Expected value of x (round-to-nearest) is
+       expQ8*pulses_left/positions_left. */
+    ex = (2*exp_q8*kn + (n - i))/(2*(n - i));
+    if (ex > kn*256) ex = kn*256;
+    sum_ex += (2*256*kn + (n - i))/(2*(n - i));
+    /* No need to encode the magnitude for the last bin. */
+    if (i != n - 1) x = laplace_decode(dec, ex, kn, acc_str);
+    else x = kn;
+    if (x != 0) {
+      if (od_ec_dec_bits(dec, 1, acc_str)) x = -x;
+    }
+    y[i] = x;
+    kn -= abs(x);
+  }
+  /* Adapting the estimates for expQ8. */
+  if (!ran_delta) {
+    curr[OD_ADAPT_COUNT_Q8] = OD_ADAPT_NO_VALUE;
+    curr[OD_ADAPT_COUNT_EX_Q8] = OD_ADAPT_NO_VALUE;
+  }
+  curr[OD_ADAPT_K_Q8] = k - kn;
+  curr[OD_ADAPT_SUM_EX_Q8] = sum_ex;
+  for (; i < n; i++) y[i] = 0;
+}
diff --git a/av1/decoder/pvq_decoder.c b/av1/decoder/pvq_decoder.c
new file mode 100644
index 0000000..f1e62fd
--- /dev/null
+++ b/av1/decoder/pvq_decoder.c
@@ -0,0 +1,382 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#ifdef HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include "./aom_config.h"
+#include "aom_dsp/entcode.h"
+#include "aom_dsp/entdec.h"
+#include "av1/common/odintrin.h"
+#include "av1/common/partition.h"
+#include "av1/common/state.h"
+#include "av1/decoder/decint.h"
+#include "pvq_decoder.h"
+#include <stdio.h>
+#include <stdlib.h>
+
+static void od_decode_pvq_codeword(od_ec_dec *ec, od_pvq_codeword_ctx *ctx,
+ od_coeff *y, int n, int k) {
+  int i;
+  od_decode_band_pvq_splits(ec, ctx, y, n, k, 0);
+  for (i = 0; i < n; i++) {
+    if (y[i] && od_ec_dec_bits(ec, 1, "pvq:sign")) y[i] = -y[i];
+  }
+}
+
+/** Inverse of neg_interleave; decodes the interleaved gain.
+ *
+ * @param [in]      x      quantized/interleaved gain to decode
+ * @param [in]      ref    quantized gain of the reference
+ * @return                 original quantized gain value
+ */
+static int neg_deinterleave(int x, int ref) {
+  if (x < 2*ref-1) {
+    if (x & 1) return ref - 1 - (x >> 1);
+    else return ref + (x >> 1);
+  }
+  else return x+1;
+}
+
+/** Synthesizes one parition of coefficient values from a PVQ-encoded
+ * vector.
+ *
+ * @param [out]     xcoeff  output coefficient partition (x in math doc)
+ * @param [in]      ypulse  PVQ-encoded values (y in math doc); in the noref
+ *                          case, this vector has n entries, in the
+ *                          reference case it contains n-1 entries
+ *                          (the m-th entry is not included)
+ * @param [in]      ref     reference vector (prediction)
+ * @param [in]      n       number of elements in this partition
+ * @param [in]      gr      gain of the reference vector (prediction)
+ * @param [in]      noref   indicates presence or lack of prediction
+ * @param [in]      g       decoded quantized vector gain
+ * @param [in]      theta   decoded theta (prediction error)
+ * @param [in]      qm      QM with magnitude compensation
+ * @param [in]      qm_inv  Inverse of QM with magnitude compensation
+ */
+static void pvq_synthesis(od_coeff *xcoeff, od_coeff *ypulse, od_val16 *r16,
+ int n, od_val32 gr, int noref, od_val32 g, od_val32 theta, const int16_t *qm_inv,
+ int shift) {
+  int s;
+  int m;
+  /* Sign of the Householder reflection vector */
+  s = 0;
+  /* Direction of the Householder reflection vector */
+  m = noref ? 0 : od_compute_householder(r16, n, gr, &s, shift);
+  od_pvq_synthesis_partial(xcoeff, ypulse, r16, n, noref, g, theta, m, s,
+   qm_inv);
+}
+
+typedef struct {
+  od_coeff *ref;
+  int nb_coeffs;
+  int allow_flip;
+} cfl_ctx;
+
+/** Decodes a single vector of integers (eg, a partition within a
+ *  coefficient block) encoded using PVQ
+ *
+ * @param [in,out] ec      range encoder
+ * @param [in]     q0      scale/quantizer
+ * @param [in]     n       number of coefficients in partition
+ * @param [in,out] model   entropy decoder state
+ * @param [in,out] adapt   adaptation context
+ * @param [in,out] exg     ExQ16 expectation of decoded gain value
+ * @param [in,out] ext     ExQ16 expectation of decoded theta value
+ * @param [in]     ref     'reference' (prediction) vector
+ * @param [out]    out     decoded partition
+ * @param [out]    noref   boolean indicating absence of reference
+ * @param [in]     beta    per-band activity masking beta param
+ * @param [in]     robust  stream is robust to error in the reference
+ * @param [in]     is_keyframe whether we're encoding a keyframe
+ * @param [in]     pli     plane index
+ * @param [in]     cdf_ctx selects which cdf context to use
+ * @param [in,out] skip_rest whether to skip further bands in each direction
+ * @param [in]     band    index of the band being decoded
+ * @param [in]     band    index of the band being decoded
+ * @param [out]    skip    skip flag with range [0,1]
+ * @param [in]     qm      QM with magnitude compensation
+ * @param [in]     qm_inv  Inverse of QM with magnitude compensation
+ */
+static void pvq_decode_partition(od_ec_dec *ec,
+                                 int q0,
+                                 int n,
+                                 generic_encoder model[3],
+                                 od_adapt_ctx *adapt,
+                                 int *exg,
+                                 int *ext,
+                                 od_coeff *ref,
+                                 od_coeff *out,
+                                 int *noref,
+                                 od_val16 beta,
+                                 int robust,
+                                 int is_keyframe,
+                                 int pli,
+                                 int cdf_ctx,
+                                 cfl_ctx *cfl,
+                                 int has_skip,
+                                 int *skip_rest,
+                                 int band,
+                                 int *skip,
+                                 const int16_t *qm,
+                                 const int16_t *qm_inv) {
+  int k;
+  od_val32 qcg;
+  int max_theta;
+  int itheta;
+  od_val32 theta;
+  od_val32 gr;
+  od_val32 gain_offset;
+  od_coeff y[MAXN];
+  int qg;
+  int nodesync;
+  int id;
+  int i;
+  od_val16 ref16[MAXN];
+  int rshift;
+  theta = 0;
+  gr = 0;
+  gain_offset = 0;
+  /* We always use the robust bitstream for keyframes to avoid having
+     PVQ and entropy decoding depending on each other, hurting parallelism. */
+  nodesync = robust || is_keyframe;
+  /* Skip is per-direction. For band=0, we can use any of the flags. */
+  if (skip_rest[(band + 2) % 3]) {
+    qg = 0;
+    if (is_keyframe) {
+      itheta = -1;
+      *noref = 1;
+    }
+    else {
+      itheta = 0;
+      *noref = 0;
+    }
+  }
+  else {
+    /* Jointly decode gain, itheta and noref for small values. Then we handle
+       larger gain. We need to wait for itheta because in the !nodesync case
+       it depends on max_theta, which depends on the gain. */
+    id = od_decode_cdf_adapt(ec, &adapt->pvq.pvq_gaintheta_cdf[cdf_ctx][0],
+     8 + 7*has_skip, adapt->pvq.pvq_gaintheta_increment,
+     "pvq:gaintheta");
+    if (!is_keyframe && id >= 10) id++;
+    if (is_keyframe && id >= 8) id++;
+    if (id >= 8) {
+      id -= 8;
+      skip_rest[0] = skip_rest[1] = skip_rest[2] = 1;
+    }
+    qg = id & 1;
+    itheta = (id >> 1) - 1;
+    *noref = (itheta == -1);
+  }
+  /* The CfL flip bit is only decoded on the first band that has noref=0. */
+  if (cfl->allow_flip && !*noref) {
+    int flip;
+    flip = od_ec_dec_bits(ec, 1, "cfl:flip");
+    if (flip) {
+      for (i = 0; i < cfl->nb_coeffs; i++) cfl->ref[i] = -cfl->ref[i];
+    }
+    cfl->allow_flip = 0;
+  }
+  if (qg > 0) {
+    int tmp;
+    tmp = *exg;
+    qg = 1 + generic_decode(ec, &model[!*noref], -1, &tmp, 2, "pvq:gain");
+    OD_IIR_DIADIC(*exg, qg << 16, 2);
+  }
+  *skip = 0;
+#if defined(OD_FLOAT_PVQ)
+  rshift = 0;
+#else
+  /* Shift needed to make the reference fit in 15 bits, so that the Householder
+     vector can fit in 16 bits. */
+  rshift = OD_MAXI(0, od_vector_log_mag(ref, n) - 14);
+#endif
+  for (i = 0; i < n; i++) {
+#if defined(OD_FLOAT_PVQ)
+    ref16[i] = ref[i]*(double)qm[i]*OD_QM_SCALE_1;
+#else
+    ref16[i] = OD_SHR_ROUND(ref[i]*qm[i], OD_QM_SHIFT + rshift);
+#endif
+  }
+  if(!*noref){
+    /* we have a reference; compute its gain */
+    od_val32 cgr;
+    int icgr;
+    int cfl_enabled;
+    cfl_enabled = pli != 0 && is_keyframe && !OD_DISABLE_CFL;
+    cgr = od_pvq_compute_gain(ref16, n, q0, &gr, beta, rshift);
+    if (cfl_enabled) cgr = OD_CGAIN_SCALE;
+#if defined(OD_FLOAT_PVQ)
+    icgr = (int)floor(.5 + cgr);
+#else
+    icgr = OD_SHR_ROUND(cgr, OD_CGAIN_SHIFT);
+#endif
+    /* quantized gain is interleave encoded when there's a reference;
+       deinterleave it now */
+    if (is_keyframe) qg = neg_deinterleave(qg, icgr);
+    else {
+      qg = neg_deinterleave(qg, icgr + 1) - 1;
+      if (qg == 0) *skip = (icgr ? OD_PVQ_SKIP_ZERO : OD_PVQ_SKIP_COPY);
+    }
+    if (qg == icgr && itheta == 0 && !cfl_enabled) *skip = OD_PVQ_SKIP_COPY;
+    gain_offset = cgr - OD_SHL(icgr, OD_CGAIN_SHIFT);
+    qcg = OD_SHL(qg, OD_CGAIN_SHIFT) + gain_offset;
+    /* read and decode first-stage PVQ error theta */
+    max_theta = od_pvq_compute_max_theta(qcg, beta);
+    if (itheta > 1 && (nodesync || max_theta > 3)) {
+      int tmp;
+      tmp = *ext;
+      itheta = 2 + generic_decode(ec, &model[2], nodesync ? -1 : max_theta - 3,
+       &tmp, 2, "pvq:theta");
+      OD_IIR_DIADIC(*ext, itheta << 16, 2);
+    }
+    theta = od_pvq_compute_theta(itheta, max_theta);
+  }
+  else{
+    itheta = 0;
+    if (!is_keyframe) qg++;
+    qcg = OD_SHL(qg, OD_CGAIN_SHIFT);
+    if (qg == 0) *skip = OD_PVQ_SKIP_ZERO;
+  }
+
+  k = od_pvq_compute_k(qcg, itheta, theta, *noref, n, beta, nodesync);
+  if (k != 0) {
+    /* when noref==0, y is actually size n-1 */
+    od_decode_pvq_codeword(ec, &adapt->pvq.pvq_codeword_ctx, y, n - !*noref,
+     k);
+  }
+  else {
+    OD_CLEAR(y, n);
+  }
+  if (*skip) {
+    if (*skip == OD_PVQ_SKIP_COPY) OD_COPY(out, ref, n);
+    else OD_CLEAR(out, n);
+  }
+  else {
+    od_val32 g;
+    g = od_gain_expand(qcg, q0, beta);
+    pvq_synthesis(out, y, ref16, n, gr, *noref, g, theta, qm_inv, rshift);
+  }
+  *skip = !!*skip;
+}
+
+/** Decodes a coefficient block (except for DC) encoded using PVQ
+ *
+ * @param [in,out] dec     daala decoder context
+ * @param [in]     ref     'reference' (prediction) vector
+ * @param [out]    out     decoded partition
+ * @param [in]     q0      quantizer
+ * @param [in]     pli     plane index
+ * @param [in]     bs      log of the block size minus two
+ * @param [in]     beta    per-band activity masking beta param
+ * @param [in]     robust  stream is robust to error in the reference
+ * @param [in]     is_keyframe whether we're encoding a keyframe
+ * @param [out]    flags   bitmask of the per band skip and noref flags
+ * @param [in]     block_skip skip flag for the block (range 0-3)
+ * @param [in]     qm      QM with magnitude compensation
+ * @param [in]     qm_inv  Inverse of QM with magnitude compensation
+ */
+void od_pvq_decode(daala_dec_ctx *dec,
+                   od_coeff *ref,
+                   od_coeff *out,
+                   int q0,
+                   int pli,
+                   int bs,
+                   const od_val16 *beta,
+                   int robust,
+                   int is_keyframe,
+                   unsigned int *flags,
+                   int block_skip,
+                   const int16_t *qm,
+                   const int16_t *qm_inv){
+
+  int noref[PVQ_MAX_PARTITIONS];
+  int skip[PVQ_MAX_PARTITIONS];
+  int *exg;
+  int *ext;
+  int nb_bands;
+  int i;
+  const int *off;
+  int size[PVQ_MAX_PARTITIONS];
+  generic_encoder *model;
+  int skip_rest[3] = {0};
+  cfl_ctx cfl;
+  /* const unsigned char *pvq_qm; */
+  /*Default to skip=1 and noref=0 for all bands.*/
+  for (i = 0; i < PVQ_MAX_PARTITIONS; i++) {
+    noref[i] = 0;
+    skip[i] = 1;
+  }
+  /*TODO: Enable this later, if pvq_qm_q4 is available in AOM.*/
+  /*pvq_qm = &dec->state.pvq_qm_q4[pli][0];*/
+  exg = &dec->state.adapt.pvq.pvq_exg[pli][bs][0];
+  ext = dec->state.adapt.pvq.pvq_ext + bs*PVQ_MAX_PARTITIONS;
+  model = dec->state.adapt.pvq.pvq_param_model;
+  nb_bands = OD_BAND_OFFSETS[bs][0];
+  off = &OD_BAND_OFFSETS[bs][1];
+  OD_ASSERT(block_skip < 4);
+  out[0] = block_skip & 1;
+  if (!(block_skip >> 1)) {
+    if (is_keyframe) for (i = 1; i < 1 << (2*bs + 4); i++) out[i] = 0;
+    else for (i = 1; i < 1 << (2*bs + 4); i++) out[i] = ref[i];
+  }
+  else {
+    for (i = 0; i < nb_bands; i++) size[i] = off[i+1] - off[i];
+    cfl.ref = ref;
+    cfl.nb_coeffs = off[nb_bands];
+    cfl.allow_flip = pli != 0 && is_keyframe;
+    for (i = 0; i < nb_bands; i++) {
+      int q;
+      /*TODO: Enable this later, if pvq_qm_q4 is available in AOM.*/
+      /*q = OD_MAXI(1, q0*pvq_qm[od_qm_get_index(bs, i + 1)] >> 4);*/
+      q = OD_MAXI(1, q0);
+      pvq_decode_partition(dec->ec, q, size[i],
+       model, &dec->state.adapt, exg + i, ext + i, ref + off[i], out + off[i],
+       &noref[i], beta[i], robust, is_keyframe, pli,
+       (pli != 0)*OD_NBSIZES*PVQ_MAX_PARTITIONS + bs*PVQ_MAX_PARTITIONS + i,
+       &cfl, i == 0 && (i < nb_bands - 1), skip_rest, i, &skip[i],
+       qm + off[i], qm_inv + off[i]);
+      if (i == 0 && !skip_rest[0] && bs > 0) {
+        int skip_dir;
+        int j;
+        skip_dir = od_decode_cdf_adapt(dec->ec,
+         &dec->state.adapt.pvq.pvq_skip_dir_cdf[(pli != 0) + 2*(bs - 1)][0], 7,
+         dec->state.adapt.pvq.pvq_skip_dir_increment, "pvq:skiprest");
+        for (j = 0; j < 3; j++) skip_rest[j] = !!(skip_dir & (1 << j));
+      }
+    }
+  }
+  *flags = 0;
+  for (i = nb_bands - 1; i >= 0; i--) {
+    *flags <<= 1;
+    *flags |= noref[i]&1;
+    *flags <<= 1;
+    *flags |= skip[i]&1;
+  }
+}
diff --git a/av1/decoder/pvq_decoder.h b/av1/decoder/pvq_decoder.h
new file mode 100644
index 0000000..8dd707e
--- /dev/null
+++ b/av1/decoder/pvq_decoder.h
@@ -0,0 +1,58 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#if !defined(_pvq_decoder_H)
+# define _pvq_decoder_H (1)
+# include "aom_dsp/entdec.h"
+# include "av1/common/pvq.h"
+# include "av1/decoder/decint.h"
+
+void od_decode_band_pvq_splits(od_ec_dec *ec, od_pvq_codeword_ctx *adapt,
+ od_coeff *y, int n, int k, int level);
+
+#if OD_ACCOUNTING
+# define laplace_decode_special(dec, decay, max, str) laplace_decode_special_(dec, decay, max, str)
+# define laplace_decode(dec, ex_q8, k, str) laplace_decode_(dec, ex_q8, k, str)
+#define laplace_decode_vector(dec, y, n, k, curr, means, str) laplace_decode_vector_(dec, y, n, k, curr, means, str)
+#else
+# define laplace_decode_special(dec, decay, max, str) laplace_decode_special_(dec, decay, max)
+# define laplace_decode(dec, ex_q8, k, str) laplace_decode_(dec, ex_q8, k)
+#define laplace_decode_vector(dec, y, n, k, curr, means, str) laplace_decode_vector_(dec, y, n, k, curr, means)
+#endif
+
+int laplace_decode_special_(od_ec_dec *dec, unsigned decay, int max OD_ACC_STR);
+int laplace_decode_(od_ec_dec *dec, unsigned ex_q8, int k OD_ACC_STR);
+void laplace_decode_vector_(od_ec_dec *dec, od_coeff *y, int n, int k,
+                                  int32_t *curr, const int32_t *means
+                                  OD_ACC_STR);
+
+
+void od_pvq_decode(daala_dec_ctx *dec, od_coeff *ref, od_coeff *out, int q0,
+ int pli, int bs, const od_val16 *beta, int robust, int is_keyframe,
+ unsigned int *flags, int block_skip, const int16_t *qm,
+ const int16_t *qm_inv);
+
+#endif
diff --git a/av1/encoder/daala_compat_enc.c b/av1/encoder/daala_compat_enc.c
new file mode 100644
index 0000000..3bdf9f6
--- /dev/null
+++ b/av1/encoder/daala_compat_enc.c
@@ -0,0 +1,11 @@
+#include "encint.h"
+
+void od_encode_checkpoint(const daala_enc_ctx *enc, od_rollback_buffer *rbuf) {
+  od_ec_enc_checkpoint(&rbuf->ec, &enc->ec);
+  OD_COPY(&rbuf->adapt, &enc->state.adapt, 1);
+}
+
+void od_encode_rollback(daala_enc_ctx *enc, const od_rollback_buffer *rbuf) {
+  od_ec_enc_rollback(&enc->ec, &rbuf->ec);
+  OD_COPY(&enc->state.adapt, &rbuf->adapt, 1);
+}
diff --git a/av1/encoder/encint.h b/av1/encoder/encint.h
new file mode 100644
index 0000000..f9683eb
--- /dev/null
+++ b/av1/encoder/encint.h
@@ -0,0 +1,62 @@
+/*Daala video codec
+Copyright (c) 2006-2013 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#if !defined(_encint_H)
+# define _encint_H (1)
+
+typedef struct daala_enc_ctx od_enc_ctx;
+typedef struct od_params_ctx od_params_ctx;
+typedef struct od_rollback_buffer od_rollback_buffer;
+
+# include "aom_dsp/entenc.h"
+# include "av1/common/odintrin.h"
+# include "av1/common/state.h"
+
+struct daala_enc_ctx{
+  od_state state;
+  od_ec_enc ec;
+  int use_activity_masking;
+  int qm;
+  /*Normalized PVQ lambda for use where we've already performed
+     quantization.*/
+  double pvq_norm_lambda;
+  double pvq_norm_lambda_dc;
+};
+
+// from daalaenc.h
+/**The encoder context.*/
+typedef struct daala_enc_ctx daala_enc_ctx;
+
+/** Holds important encoder information so we can roll back decisions */
+struct od_rollback_buffer {
+  od_ec_enc ec;
+  od_adapt_ctx adapt;
+};
+
+void od_encode_checkpoint(const daala_enc_ctx *enc, od_rollback_buffer *rbuf);
+void od_encode_rollback(daala_enc_ctx *enc, const od_rollback_buffer *rbuf);
+
+#endif
diff --git a/av1/encoder/generic_encoder.c b/av1/encoder/generic_encoder.c
new file mode 100644
index 0000000..f19bef5
--- /dev/null
+++ b/av1/encoder/generic_encoder.c
@@ -0,0 +1,213 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#ifdef HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include <stdio.h>
+
+#include "aom_dsp/entdec.h"
+#include "aom_dsp/entenc.h"
+#include "av1/common/generic_code.h"
+#include "av1/common/odintrin.h"
+#include "pvq_encoder.h"
+
+/** Encodes a value from 0 to N-1 (with N up to 16) based on a cdf and adapts
+ * the cdf accordingly.
+ *
+ * @param [in,out] enc   range encoder
+ * @param [in]     val   variable being encoded
+ * @param [in,out] cdf   CDF of the variable (Q15)
+ * @param [in]     n     number of values possible
+ * @param [in,out] count number of symbols encoded with that cdf so far
+ * @param [in]     rate  adaptation rate shift (smaller is faster)
+ */
+void od_encode_cdf_adapt_q15(od_ec_enc *ec, int val, uint16_t *cdf, int n,
+ int *count, int rate) {
+  int i;
+  if (*count == 0) {
+    /* On the first call, we normalize the cdf to (32768 - n). This should
+       eventually be moved to the state init, but for now it makes it much
+       easier to experiment and convert symbols to the Q15 adaptation.*/
+    int ft;
+    ft = cdf[n - 1];
+    for (i = 0; i < n; i++) {
+      cdf[i] = cdf[i]*32768/ft;
+    }
+  }
+  od_ec_encode_cdf_q15(ec, val, cdf, n);
+  od_cdf_adapt_q15(val, cdf, n, count, rate);
+}
+
+/** Encodes a value from 0 to N-1 (with N up to 16) based on a cdf and adapts
+ * the cdf accordingly.
+ *
+ * @param [in,out] enc   range encoder
+ * @param [in]     val   variable being encoded
+ * @param [in]     cdf   CDF of the variable (Q15)
+ * @param [in]     n     number of values possible
+ * @param [in]     increment adaptation speed (Q15)
+ */
+void od_encode_cdf_adapt(od_ec_enc *ec, int val, uint16_t *cdf, int n,
+ int increment) {
+  int i;
+  od_ec_encode_cdf_unscaled(ec, val, cdf, n);
+  if (cdf[n-1] + increment > 32767) {
+    for (i = 0; i < n; i++) {
+      /* Second term ensures that the pdf is non-null */
+      cdf[i] = (cdf[i] >> 1) + i + 1;
+    }
+  }
+  for (i = val; i < n; i++) cdf[i] += increment;
+}
+
+/** Encodes a random variable using a "generic" model, assuming that the
+ * distribution is one-sided (zero and up), has a single mode, and decays
+ * exponentially past the model.
+ *
+ * @param [in,out] enc   range encoder
+ * @param [in,out] model generic probability model
+ * @param [in]     x     variable being encoded
+ * @param [in]     max   largest value possible
+ * @param [in,out] ExQ16 expectation of x (adapted)
+ * @param [in]     integration integration period of ExQ16 (leaky average over
+ * 1<<integration samples)
+ */
+void generic_encode(od_ec_enc *enc, generic_encoder *model, int x, int max,
+ int *ex_q16, int integration) {
+  int lg_q1;
+  int shift;
+  int id;
+  uint16_t *cdf;
+  int xs;
+  int ms;
+  if (max == 0) return;
+  lg_q1 = log_ex(*ex_q16);
+  OD_LOG((OD_LOG_ENTROPY_CODER, OD_LOG_DEBUG,
+   "%d %d", *ex_q16, lg_q1));
+  /* If expectation is too large, shift x to ensure that
+     all we have past xs=15 is the exponentially decaying tail
+     of the distribution */
+  shift = OD_MAXI(0, (lg_q1 - 5) >> 1);
+  /* Choose the cdf to use: we have two per "octave" of ExQ16 */
+  id = OD_MINI(GENERIC_TABLES - 1, lg_q1);
+  cdf = model->cdf[id];
+  xs = (x + (1 << shift >> 1)) >> shift;
+  ms = (max + (1 << shift >> 1)) >> shift;
+  OD_ASSERT(max == -1 || xs <= ms);
+  if (max == -1) od_ec_encode_cdf_unscaled(enc, OD_MINI(15, xs), cdf, 16);
+  else {
+    od_ec_encode_cdf_unscaled(enc, OD_MINI(15, xs), cdf, OD_MINI(ms + 1, 16));
+  }
+  if (xs >= 15) {
+    int e;
+    unsigned decay;
+    /* Estimate decay based on the assumption that the distribution is close
+       to Laplacian for large values. We should probably have an adaptive
+       estimate instead. Note: The 2* is a kludge that's not fully understood
+       yet. */
+    OD_ASSERT(*ex_q16 < INT_MAX >> 1);
+    e = ((2**ex_q16 >> 8) + (1 << shift >> 1)) >> shift;
+    decay = OD_MAXI(2, OD_MINI(254, 256*e/(e + 256)));
+    /* Encode the tail of the distribution assuming exponential decay. */
+    laplace_encode_special(enc, xs - 15, decay, (max == -1) ? -1 : ms - 15);
+  }
+  if (shift != 0) {
+    int special;
+    /* Because of the rounding, there's only half the number of possibilities
+       for xs=0. */
+    special = xs == 0;
+    if (shift - special > 0) {
+      od_ec_enc_bits(enc, x - (xs << shift) + (!special << (shift - 1)),
+       shift - special);
+    }
+  }
+  generic_model_update(model, ex_q16, x, xs, id, integration);
+  OD_LOG((OD_LOG_ENTROPY_CODER, OD_LOG_DEBUG,
+   "enc: %d %d %d %d %d %x", *ex_q16, x, shift, id, xs, enc->rng));
+}
+
+/** Estimates the cost of encoding a value with generic_encode().
+ *
+ * @param [in,out] model generic probability model
+ * @param [in]     x     variable being encoded
+ * @param [in]     max   largest value possible
+ * @param [in,out] ExQ16 expectation of x (adapted)
+ * @return number of bits (approximation)
+ */
+double generic_encode_cost(generic_encoder *model, int x, int max,
+ int *ex_q16) {
+  int lg_q1;
+  int shift;
+  int id;
+  uint16_t *cdf;
+  int xs;
+  int ms;
+  int extra;
+  if (max == 0) return 0;
+  lg_q1 = log_ex(*ex_q16);
+  /* If expectation is too large, shift x to ensure that
+       all we have past xs=15 is the exponentially decaying tail
+       of the distribution */
+  shift = OD_MAXI(0, (lg_q1 - 5) >> 1);
+  /* Choose the cdf to use: we have two per "octave" of ExQ16 */
+  id = OD_MINI(GENERIC_TABLES - 1, lg_q1);
+  cdf = model->cdf[id];
+  xs = (x + (1 << shift >> 1)) >> shift;
+  ms = (max + (1 << shift >> 1)) >> shift;
+  OD_ASSERT(max == -1 || xs <= ms);
+  extra = 0;
+  if (shift) extra = shift - (xs == 0);
+  xs = OD_MINI(15, xs);
+  /* Shortcut: assume it's going to cost 2 bits for the Laplace coder. */
+  if (xs == 15) extra += 2;
+  if (max == -1) {
+    return extra - OD_LOG2((double)(cdf[xs] - (xs == 0 ? 0 : cdf[xs - 1]))/
+     cdf[15]);
+  }
+  else {
+    return extra - OD_LOG2((double)(cdf[xs] - (xs == 0 ? 0 : cdf[xs - 1]))/
+     cdf[OD_MINI(ms, 15)]);
+  }
+}
+
+/*Estimates the cost of encoding a value with a given CDF.*/
+double od_encode_cdf_cost(int val, uint16_t *cdf, int n) {
+  int total_prob;
+  int prev_prob;
+  double val_prob;
+  OD_ASSERT(n > 0);
+  total_prob = cdf[n - 1];
+  if (val == 0) {
+    prev_prob = 0;
+  }
+  else {
+    prev_prob = cdf[val - 1];
+  }
+  val_prob = (cdf[val] - prev_prob) / (double)total_prob;
+  return -OD_LOG2(val_prob);
+}
diff --git a/av1/encoder/laplace_encoder.c b/av1/encoder/laplace_encoder.c
new file mode 100644
index 0000000..c6d7761
--- /dev/null
+++ b/av1/encoder/laplace_encoder.c
@@ -0,0 +1,305 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#ifdef HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include <stdio.h>
+
+#include "aom_dsp/entdec.h"
+#include "aom_dsp/entenc.h"
+#include "av1/common/odintrin.h"
+#include "av1/common/pvq.h"
+#include "pvq_encoder.h"
+
+static void od_encode_pvq_split(od_ec_enc *ec, od_pvq_codeword_ctx *adapt,
+ int count, int sum, int ctx) {
+  int shift;
+  int rest;
+  int fctx;
+  if (sum == 0) return;
+  shift = OD_MAXI(0, OD_ILOG(sum) - 3);
+  if (shift) {
+    rest = count & ((1 << shift) - 1);
+    count >>= shift;
+    sum >>= shift;
+  }
+  fctx = 7*ctx + sum - 1;
+  od_encode_cdf_adapt(ec, count, adapt->pvq_split_cdf[fctx],
+   sum + 1, adapt->pvq_split_increment);
+  if (shift) od_ec_enc_bits(ec, rest, shift);
+}
+
+void od_encode_band_pvq_splits(od_ec_enc *ec, od_pvq_codeword_ctx *adapt,
+ const int *y, int n, int k, int level) {
+  int mid;
+  int i;
+  int count_right;
+  if (n <= 1 || k == 0) return;
+  if (k == 1 && n <= 16) {
+    int cdf_id;
+    int pos;
+    cdf_id = od_pvq_k1_ctx(n, level == 0);
+    for (pos = 0; !y[pos]; pos++);
+    OD_ASSERT(pos < n);
+    od_encode_cdf_adapt(ec, pos, adapt->pvq_k1_cdf[cdf_id], n,
+     adapt->pvq_k1_increment);
+  }
+  else {
+    mid = n >> 1;
+    count_right = k;
+    for (i = 0; i < mid; i++) count_right -= abs(y[i]);
+    od_encode_pvq_split(ec, adapt, count_right, k, od_pvq_size_ctx(n));
+    od_encode_band_pvq_splits(ec, adapt, y, mid, k - count_right, level + 1);
+    od_encode_band_pvq_splits(ec, adapt, y + mid, n - mid, count_right,
+     level + 1);
+  }
+}
+
+/** Encodes the tail of a Laplace-distributed variable, i.e. it doesn't
+ * do anything special for the zero case.
+ *
+ * @param [in,out] enc     range encoder
+ * @param [in]     x       variable to encode (has to be positive)
+ * @param [in]     decay   decay factor of the distribution in Q8 format,
+ * i.e. pdf ~= decay^x
+ * @param [in]     max     maximum possible value of x (used to truncate
+ * the pdf)
+ */
+void laplace_encode_special(od_ec_enc *enc, int x, unsigned decay, int max) {
+  int shift;
+  int xs;
+  int ms;
+  int sym;
+  const uint16_t *cdf;
+  shift = 0;
+  if (max == 0) return;
+  /* We don't want a large decay value because that would require too many
+     symbols. However, it's OK if the max is below 15. */
+  while (((max >> shift) >= 15 || max == -1) && decay > 235) {
+    decay = (decay*decay + 128) >> 8;
+    shift++;
+  }
+  OD_ASSERT(x <= max || max == -1);
+  decay = OD_MINI(decay, 254);
+  decay = OD_MAXI(decay, 2);
+  xs = x >> shift;
+  ms = max >> shift;
+  cdf = EXP_CDF_TABLE[(decay + 1) >> 1];
+  OD_LOG((OD_LOG_PVQ, OD_LOG_DEBUG, "decay = %d", decay));
+  do {
+    sym = OD_MINI(xs, 15);
+    {
+      int i;
+      OD_LOG((OD_LOG_PVQ, OD_LOG_DEBUG, "%d %d %d %d %d\n", x, xs, shift,
+       sym, max));
+      for (i = 0; i < 16; i++) {
+        OD_LOG_PARTIAL((OD_LOG_PVQ, OD_LOG_DEBUG, "%d ", cdf[i]));
+      }
+      OD_LOG_PARTIAL((OD_LOG_PVQ, OD_LOG_DEBUG, "\n"));
+    }
+    if (ms > 0 && ms < 15) {
+      /* Simple way of truncating the pdf when we have a bound */
+      od_ec_encode_cdf_unscaled(enc, sym, cdf, ms + 1);
+    }
+    else {
+      od_ec_encode_cdf_q15(enc, sym, cdf, 16);
+    }
+    xs -= 15;
+    ms -= 15;
+  }
+  while (sym >= 15 && ms != 0);
+  if (shift) od_ec_enc_bits(enc, x & ((1 << shift) - 1), shift);
+}
+
+/** Encodes a Laplace-distributed variable for use in PVQ
+ *
+ * @param [in,out] enc  range encoder
+ * @param [in]     x    variable to encode (including sign)
+ * @param [in]     ExQ8 expectation of the absolute value of x in Q8
+ * @param [in]     K    maximum value of |x|
+ */
+void laplace_encode(od_ec_enc *enc, int x, int ex_q8, int k) {
+  int j;
+  int shift;
+  int xs;
+  uint16_t cdf[16];
+  int sym;
+  int decay;
+  int offset;
+  /* shift down x if expectation is too high */
+  shift = OD_ILOG(ex_q8) - 11;
+  if (shift < 0) shift = 0;
+  /* Apply the shift with rounding to Ex, K and xs */
+  ex_q8 = (ex_q8 + (1 << shift >> 1)) >> shift;
+  k = (k + (1 << shift >> 1)) >> shift;
+  xs = (x + (1 << shift >> 1)) >> shift;
+  decay = OD_MINI(254, 256*ex_q8/(ex_q8 + 256));
+  offset = LAPLACE_OFFSET[(decay + 1) >> 1];
+  for (j = 0; j < 16; j++) {
+    cdf[j] = EXP_CDF_TABLE[(decay + 1) >> 1][j] - offset;
+  }
+  sym = xs;
+  if (sym > 15) sym = 15;
+  /* Simple way of truncating the pdf when we have a bound */
+  if (k != 0) od_ec_encode_cdf_unscaled(enc, sym, cdf, OD_MINI(k + 1, 16));
+  if (shift) {
+    int special;
+    /* Because of the rounding, there's only half the number of possibilities
+       for xs=0 */
+    special = xs == 0;
+    if (shift - special > 0) {
+      od_ec_enc_bits(enc, x - (xs << shift) + (!special << (shift - 1)),
+       shift - special);
+    }
+  }
+  /* Handle the exponentially-decaying tail of the distribution */
+  OD_ASSERT(xs - 15 <= k - 15);
+  if (xs >= 15) laplace_encode_special(enc, xs - 15, decay, k - 15);
+}
+
+static void laplace_encode_vector_delta(od_ec_enc *enc, const od_coeff *y, int n, int k,
+                                        int32_t *curr, const int32_t *means) {
+  int i;
+  int prev;
+  int sum_ex;
+  int sum_c;
+  int first;
+  int k_left;
+  int coef;
+  prev = 0;
+  sum_ex = 0;
+  sum_c = 0;
+  first = 1;
+  k_left = k;
+  coef = 256*means[OD_ADAPT_COUNT_Q8]/
+   (1 + means[OD_ADAPT_COUNT_EX_Q8]);
+  coef = OD_MAXI(coef, 1);
+  for (i = 0; i < n; i++) {
+    if (y[i] != 0) {
+      int j;
+      int count;
+      int mag;
+      mag = abs(y[i]);
+      count = i - prev;
+      if (first) {
+        int decay;
+        int ex = coef*(n - prev)/k_left;
+        if (ex > 65280) decay = 255;
+        else {
+          decay = OD_MINI(255,
+           (int)((256*ex/(ex + 256) + (ex>>5)*ex/((n + 1)*(n - 1)*(n - 1)))));
+        }
+        /*Update mean position.*/
+        OD_ASSERT(count <= n - 1);
+        laplace_encode_special(enc, count, decay, n - 1);
+        first = 0;
+      }
+      else laplace_encode(enc, count, coef*(n - prev)/k_left, n - prev - 1);
+      sum_ex += 256*(n - prev);
+      sum_c += count*k_left;
+      od_ec_enc_bits(enc, y[i] < 0, 1);
+      for (j = 0; j < mag - 1; j++) {
+        laplace_encode(enc, 0, coef*(n - i)/(k_left - 1 - j), n - i - 1);
+        sum_ex += 256*(n - i);
+      }
+      k_left -= mag;
+      prev = i;
+      if (k_left == 0) break;
+    }
+  }
+  if (k > 0) {
+    curr[OD_ADAPT_COUNT_Q8] = 256*sum_c;
+    curr[OD_ADAPT_COUNT_EX_Q8] = sum_ex;
+  }
+  else {
+    curr[OD_ADAPT_COUNT_Q8] = OD_ADAPT_NO_VALUE;
+    curr[OD_ADAPT_COUNT_EX_Q8] = OD_ADAPT_NO_VALUE;
+  }
+  curr[OD_ADAPT_K_Q8] = 0;
+  curr[OD_ADAPT_SUM_EX_Q8] = 0;
+}
+
+/** Encodes a vector of integers assumed to come from rounding a sequence of
+ * Laplace-distributed real values in decreasing order of variance.
+ *
+ * @param [in,out] enc range encoder
+ * @param [in]     y     vector to encode
+ * @param [in]     N     dimension of the vector
+ * @param [in]     K     sum of the absolute value of components of y
+ * @param [out]    curr  Adaptation context output, may alias means.
+ * @param [in]     means Adaptation context input.
+ */
+void laplace_encode_vector(od_ec_enc *enc, const od_coeff *y, int n, int k,
+                           int32_t *curr, const int32_t *means) {
+  int i;
+  int sum_ex;
+  int kn;
+  int exp_q8;
+  int mean_k_q8;
+  int mean_sum_ex_q8;
+  int ran_delta;
+  ran_delta = 0;
+  if (k <= 1) {
+    laplace_encode_vector_delta(enc, y, n, k, curr, means);
+    return;
+  }
+  sum_ex = 0;
+  kn = k;
+  /* Estimates the factor relating pulses_left and positions_left to E(|x|) */
+  mean_k_q8 = means[OD_ADAPT_K_Q8];
+  mean_sum_ex_q8 = means[OD_ADAPT_SUM_EX_Q8];
+  if (mean_k_q8 < 1 << 23) exp_q8 = 256*mean_k_q8/(1 + mean_sum_ex_q8);
+  else exp_q8 = mean_k_q8/(1 + (mean_sum_ex_q8 >> 8));
+  for (i = 0; i < n; i++) {
+    int ex;
+    int x;
+    if (kn == 0) break;
+    if (kn <= 1 && i != n - 1) {
+      laplace_encode_vector_delta(enc, y + i, n - i, kn, curr, means);
+      ran_delta = 1;
+      break;
+    }
+    x = abs(y[i]);
+    /* Expected value of x (round-to-nearest) is
+       expQ8*pulses_left/positions_left */
+    ex = (2*exp_q8*kn + (n - i))/(2*(n - i));
+    if (ex > kn*256) ex = kn*256;
+    sum_ex += (2*256*kn + (n - i))/(2*(n - i));
+    /* No need to encode the magnitude for the last bin. */
+    if (i != n - 1) laplace_encode(enc, x, ex, kn);
+    if (x != 0) od_ec_enc_bits(enc, y[i] < 0, 1);
+    kn -= x;
+  }
+  /* Adapting the estimates for expQ8 */
+  if (!ran_delta) {
+    curr[OD_ADAPT_COUNT_Q8] = OD_ADAPT_NO_VALUE;
+    curr[OD_ADAPT_COUNT_EX_Q8] = OD_ADAPT_NO_VALUE;
+  }
+  curr[OD_ADAPT_K_Q8] = k - kn;
+  curr[OD_ADAPT_SUM_EX_Q8] = sum_ex;
+}
diff --git a/av1/encoder/pvq_encoder.c b/av1/encoder/pvq_encoder.c
new file mode 100644
index 0000000..747fce9
--- /dev/null
+++ b/av1/encoder/pvq_encoder.c
@@ -0,0 +1,1016 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#ifdef HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include "aom_dsp/entcode.h"
+#include "aom_dsp/entenc.h"
+#include "av1/common/blockd.h"
+#include "av1/common/odintrin.h"
+#include "av1/common/partition.h"
+#include "av1/common/state.h"
+#include "av1/encoder/encodemb.h"
+#include "pvq_encoder.h"
+#include <math.h>
+#include <stdio.h>
+#include <stdlib.h>
+
+#define OD_PVQ_RATE_APPROX (0)
+/*Shift to ensure that the upper bound (i.e. for the max blocksize) of the
+   dot-product of the 1st band of chroma with the luma ref doesn't overflow.*/
+#define OD_CFL_FLIP_SHIFT (OD_LIMIT_BSIZE_MAX + 0)
+
+static void od_encode_pvq_codeword(od_ec_enc *ec, od_pvq_codeword_ctx *adapt,
+ const od_coeff *in, int n, int k) {
+  int i;
+  od_encode_band_pvq_splits(ec, adapt, in, n, k, 0);
+  for (i = 0; i < n; i++) if (in[i]) od_ec_enc_bits(ec, in[i] < 0, 1);
+}
+
+/* Computes 1/sqrt(i) using a table for small values. */
+static double od_rsqrt_table(int i) {
+  static double table[16] = {
+    1.000000, 0.707107, 0.577350, 0.500000,
+    0.447214, 0.408248, 0.377964, 0.353553,
+    0.333333, 0.316228, 0.301511, 0.288675,
+    0.277350, 0.267261, 0.258199, 0.250000};
+  if (i <= 16) return table[i-1];
+  else return 1./sqrt(i);
+}
+
+/*Computes 1/sqrt(start+2*i+1) using a lookup table containing the results
+   where 0 <= i < table_size.*/
+static double od_custom_rsqrt_dynamic_table(const double* table,
+ const int table_size, const double start, const int i) {
+  if (i < table_size) return table[i];
+  else return od_rsqrt_table(start + 2*i + 1);
+}
+
+/*Fills tables used in od_custom_rsqrt_dynamic_table for a given start.*/
+static void od_fill_dynamic_rqrt_table(double *table, const int table_size,
+ const double start) {
+  int i;
+  for (i = 0; i < table_size; i++)
+    table[i] = od_rsqrt_table(start + 2*i + 1);
+}
+
+/** Find the codepoint on the given PSphere closest to the desired
+ * vector. Double-precision PVQ search just to make sure our tests
+ * aren't limited by numerical accuracy.
+ *
+ * @param [in]      xcoeff  input vector to quantize (x in the math doc)
+ * @param [in]      n       number of dimensions
+ * @param [in]      k       number of pulses
+ * @param [out]     ypulse  optimal codevector found (y in the math doc)
+ * @param [out]     g2      multiplier for the distortion (typically squared
+ *                          gain units)
+ * @param [in] pvq_norm_lambda enc->pvq_norm_lambda for quantized RDO
+ * @param [in]      prev_k  number of pulses already in ypulse that we should
+ *                          reuse for the search (or 0 for a new search)
+ * @return                  cosine distance between x and y (between 0 and 1)
+ */
+static double pvq_search_rdo_double(const od_val16 *xcoeff, int n, int k,
+ od_coeff *ypulse, double g2, double pvq_norm_lambda, int prev_k) {
+  int i, j;
+  double xy;
+  double yy;
+  /* TODO - This blows our 8kB stack space budget and should be fixed when
+   converting PVQ to fixed point. */
+  double x[MAXN];
+  double xx;
+  double lambda;
+  double norm_1;
+  int rdo_pulses;
+  double delta_rate;
+  xx = xy = yy = 0;
+  for (j = 0; j < n; j++) {
+    x[j] = fabs((float)xcoeff[j]);
+    xx += x[j]*x[j];
+  }
+  norm_1 = 1./sqrt(1e-30 + xx);
+  lambda = pvq_norm_lambda/(1e-30 + g2);
+  i = 0;
+  if (prev_k > 0 && prev_k <= k) {
+    /* We reuse pulses from a previous search so we don't have to search them
+       again. */
+    for (j = 0; j < n; j++) {
+      ypulse[j] = abs(ypulse[j]);
+      xy += x[j]*ypulse[j];
+      yy += ypulse[j]*ypulse[j];
+      i += ypulse[j];
+    }
+  }
+  else if (k > 2) {
+    double l1_norm;
+    double l1_inv;
+    l1_norm = 0;
+    for (j = 0; j < n; j++) l1_norm += x[j];
+    l1_inv = 1./OD_MAXF(l1_norm, 1e-100);
+    for (j = 0; j < n; j++) {
+      double tmp;
+      tmp = k*x[j]*l1_inv;
+      ypulse[j] = OD_MAXI(0, (int)floor(tmp));
+      xy += x[j]*ypulse[j];
+      yy += ypulse[j]*ypulse[j];
+      i += ypulse[j];
+    }
+  }
+  else OD_CLEAR(ypulse, n);
+
+  /* Only use RDO on the last few pulses. This not only saves CPU, but using
+     RDO on all pulses actually makes the results worse for reasons I don't
+     fully understand. */
+  rdo_pulses = 1 + k/4;
+  /* Rough assumption for now, the last position costs about 3 bits more than
+     the first. */
+  delta_rate = 3./n;
+  /* Search one pulse at a time */
+  for (; i < k - rdo_pulses; i++) {
+    int pos;
+    double best_xy;
+    double best_yy;
+    pos = 0;
+    best_xy = -10;
+    best_yy = 1;
+    for (j = 0; j < n; j++) {
+      double tmp_xy;
+      double tmp_yy;
+      tmp_xy = xy + x[j];
+      tmp_yy = yy + 2*ypulse[j] + 1;
+      tmp_xy *= tmp_xy;
+      if (j == 0 || tmp_xy*best_yy > best_xy*tmp_yy) {
+        best_xy = tmp_xy;
+        best_yy = tmp_yy;
+        pos = j;
+      }
+    }
+    xy = xy + x[pos];
+    yy = yy + 2*ypulse[pos] + 1;
+    ypulse[pos]++;
+  }
+  /* Search last pulses with RDO. Distortion is D = (x-y)^2 = x^2 - 2*x*y + y^2
+     and since x^2 and y^2 are constant, we just maximize x*y, plus a
+     lambda*rate term. Note that since x and y aren't normalized here,
+     we need to divide by sqrt(x^2)*sqrt(y^2). */
+  for (; i < k; i++) {
+    double rsqrt_table[4];
+    int rsqrt_table_size = 4;
+    int pos;
+    double best_cost;
+    pos = 0;
+    best_cost = -1e5;
+    /*Fill the small rsqrt lookup table with inputs relative to yy.
+      Specifically, the table of n values is filled with
+       rsqrt(yy + 1), rsqrt(yy + 2 + 1) .. rsqrt(yy + 2*(n-1) + 1).*/
+    od_fill_dynamic_rqrt_table(rsqrt_table, rsqrt_table_size, yy);
+    for (j = 0; j < n; j++) {
+      double tmp_xy;
+      double tmp_yy;
+      tmp_xy = xy + x[j];
+      /*Calculate rsqrt(yy + 2*ypulse[j] + 1) using an optimized method.*/
+      tmp_yy = od_custom_rsqrt_dynamic_table(rsqrt_table, rsqrt_table_size,
+       yy, ypulse[j]);
+      tmp_xy = 2*tmp_xy*norm_1*tmp_yy - lambda*j*delta_rate;
+      if (j == 0 || tmp_xy > best_cost) {
+        best_cost = tmp_xy;
+        pos = j;
+      }
+    }
+    xy = xy + x[pos];
+    yy = yy + 2*ypulse[pos] + 1;
+    ypulse[pos]++;
+  }
+  for (i = 0; i < n; i++) {
+    if (xcoeff[i] < 0) ypulse[i] = -ypulse[i];
+  }
+  return xy/(1e-100 + sqrt(xx*yy));
+}
+
+/** Encodes the gain so that the return value increases with the
+ * distance |x-ref|, so that we can encode a zero when x=ref. The
+ * value x=0 is not covered because it is only allowed in the noref
+ * case.
+ *
+ * @param [in]      x      quantized gain to encode
+ * @param [in]      ref    quantized gain of the reference
+ * @return                 interleave-encoded quantized gain value
+ */
+static int neg_interleave(int x, int ref) {
+  if (x < ref) return -2*(x - ref) - 1;
+  else if (x < 2*ref) return 2*(x - ref);
+  else return x-1;
+}
+
+int od_vector_is_null(const od_coeff *x, int len) {
+  int i;
+  for (i = 0; i < len; i++) if (x[i]) return 0;
+  return 1;
+}
+
+static double od_pvq_rate(int qg, int icgr, int theta, int ts,
+ const od_adapt_ctx *adapt, const od_coeff *y0, int k, int n,
+ int is_keyframe, int pli, int speed) {
+  double rate;
+  if (k == 0) rate = 0;
+  else if (speed > 0) {
+    int i;
+    int sum;
+    double f;
+    /* Compute "center of mass" of the pulse vector. */
+    sum = 0;
+    for (i = 0; i < n - (theta != -1); i++) sum += i*abs(y0[i]);
+    f = sum/(double)(k*n);
+    /* Estimates the number of bits it will cost to encode K pulses in
+       N dimensions based on hand-tuned fit for bitrate vs K, N and
+       "center of mass". */
+    rate = (1 + .4*f)*n*OD_LOG2(1 + OD_MAXF(0, log(n*2*(1*f + .025))*k/n)) + 3;
+  }
+  else {
+    od_ec_enc ec;
+    od_pvq_codeword_ctx cd;
+    int tell;
+    od_ec_enc_init(&ec, 1000);
+    OD_COPY(&cd, &adapt->pvq.pvq_codeword_ctx, 1);
+    tell = od_ec_enc_tell_frac(&ec);
+    od_encode_pvq_codeword(&ec, &cd, y0, n - (theta != -1), k);
+    rate = (od_ec_enc_tell_frac(&ec)-tell)/8.;
+    od_ec_enc_clear(&ec);
+  }
+  if (qg > 0 && theta >= 0) {
+    /* Approximate cost of entropy-coding theta */
+    rate += .9*OD_LOG2(ts);
+    /* Adding a cost to using the H/V pred because it's going to be off
+       most of the time. Cost is optimized on subset1, while making
+       sure we don't hurt the checkerboard image too much.
+       FIXME: Do real RDO instead of this arbitrary cost. */
+    if (is_keyframe && pli == 0) rate += 6;
+    if (qg == icgr) rate -= .5;
+  }
+  return rate;
+}
+
+#define MAX_PVQ_ITEMS (20)
+/* This stores the information about a PVQ search candidate, so we can sort
+   based on K. */
+typedef struct {
+  int gain;
+  int k;
+  od_val32 qtheta;
+  int theta;
+  int ts;
+  od_val32 qcg;
+} pvq_search_item;
+
+int items_compare(pvq_search_item *a, pvq_search_item *b) {
+  return a->k - b->k;
+}
+
+/** Perform PVQ quantization with prediction, trying several
+ * possible gains and angles. See draft-valin-videocodec-pvq and
+ * http://jmvalin.ca/slides/pvq.pdf for more details.
+ *
+ * @param [out]    out       coefficients after quantization
+ * @param [in]     x0        coefficients before quantization
+ * @param [in]     r0        reference, aka predicted coefficients
+ * @param [in]     n         number of dimensions
+ * @param [in]     q0        quantization step size
+ * @param [out]    y         pulse vector (i.e. selected PVQ codevector)
+ * @param [out]    itheta    angle between input and reference (-1 if noref)
+ * @param [out]    max_theta maximum value of itheta that could have been
+ * @param [out]    vk        total number of pulses
+ * @param [in]     beta      per-band activity masking beta param
+ * @param [out]    skip_diff distortion cost of skipping this block
+ *                           (accumulated)
+ * @param [in]     robust    make stream robust to error in the reference
+ * @param [in]     is_keyframe whether we're encoding a keyframe
+ * @param [in]     pli       plane index
+ * @param [in]     adapt     probability adaptation context
+ * @param [in]     qm        QM with magnitude compensation
+ * @param [in]     qm_inv    Inverse of QM with magnitude compensation
+ * @param [in] pvq_norm_lambda enc->pvq_norm_lambda for quantized RDO
+ * @param [in]     speed     Make search faster by making approximations
+ * @return         gain      index of the quatized gain
+*/
+static int pvq_theta(od_coeff *out, const od_coeff *x0, const od_coeff *r0,
+ int n, int q0, od_coeff *y, int *itheta, int *max_theta, int *vk,
+ od_val16 beta, double *skip_diff, int robust, int is_keyframe, int pli,
+ const od_adapt_ctx *adapt, const int16_t *qm,
+ const int16_t *qm_inv, double pvq_norm_lambda, int speed) {
+  od_val32 g;
+  od_val32 gr;
+  od_coeff y_tmp[MAXN];
+  int i;
+  /* Number of pulses. */
+  int k;
+  /* Companded gain of x and reference, normalized to q. */
+  od_val32 cg;
+  od_val32 cgr;
+  int icgr;
+  int qg;
+  /* Best RDO cost (D + lamdba*R) so far. */
+  double best_cost;
+  double dist0;
+  /* Distortion (D) that corresponds to the best RDO cost. */
+  double best_dist;
+  double dist;
+  /* Sign of Householder reflection. */
+  int s;
+  /* Dimension on which Householder reflects. */
+  int m;
+  od_val32 theta;
+  double corr;
+  int best_k;
+  od_val32 best_qtheta;
+  od_val32 gain_offset;
+  int noref;
+  double skip_dist;
+  int cfl_enabled;
+  int skip;
+  double gain_weight;
+  od_val16 x16[MAXN];
+  od_val16 r16[MAXN];
+  int xshift;
+  int rshift;
+  /* Give more weight to gain error when calculating the total distortion. */
+  gain_weight = 1.0;
+  OD_ASSERT(n > 1);
+  corr = 0;
+#if !defined(OD_FLOAT_PVQ)
+  /* Shift needed to make x fit in 16 bits even after rotation.
+     This shift value is not normative (it can be changed without breaking
+     the bitstream) */
+  xshift = OD_MAXI(0, od_vector_log_mag(x0, n) - 15);
+  /* Shift needed to make the reference fit in 15 bits, so that the Householder
+     vector can fit in 16 bits.
+     This shift value *is* normative, and has to match the decoder. */
+  rshift = OD_MAXI(0, od_vector_log_mag(r0, n) - 14);
+#else
+  xshift = 0;
+  rshift = 0;
+#endif
+  for (i = 0; i < n; i++) {
+#if defined(OD_FLOAT_PVQ)
+    /*This is slightly different from the original float PVQ code,
+       where the qm was applied in the accumulation in od_pvq_compute_gain and
+       the vectors were od_coeffs, not od_val16 (i.e. double).*/
+    x16[i] = x0[i]*(double)qm[i]*OD_QM_SCALE_1;
+    r16[i] = r0[i]*(double)qm[i]*OD_QM_SCALE_1;
+#else
+    x16[i] = OD_SHR_ROUND(x0[i]*qm[i], OD_QM_SHIFT + xshift);
+    r16[i] = OD_SHR_ROUND(r0[i]*qm[i], OD_QM_SHIFT + rshift);
+#endif
+    corr += OD_MULT16_16(x16[i], r16[i]);
+  }
+  cfl_enabled = is_keyframe && pli != 0 && !OD_DISABLE_CFL;
+  cg  = od_pvq_compute_gain(x16, n, q0, &g, beta, xshift);
+  cgr = od_pvq_compute_gain(r16, n, q0, &gr, beta, rshift);
+  if (cfl_enabled) cgr = OD_CGAIN_SCALE;
+  /* gain_offset is meant to make sure one of the quantized gains has
+     exactly the same gain as the reference. */
+#if defined(OD_FLOAT_PVQ)
+  icgr = (int)floor(.5 + cgr);
+#else
+  icgr = OD_SHR_ROUND(cgr, OD_CGAIN_SHIFT);
+#endif
+  gain_offset = cgr - OD_SHL(icgr, OD_CGAIN_SHIFT);
+  /* Start search with null case: gain=0, no pulse. */
+  qg = 0;
+  dist = gain_weight*cg*cg*OD_CGAIN_SCALE_2;
+  best_dist = dist;
+  best_cost = dist + pvq_norm_lambda*od_pvq_rate(0, 0, -1, 0, adapt, NULL, 0,
+   n, is_keyframe, pli, speed);
+  noref = 1;
+  best_k = 0;
+  *itheta = -1;
+  *max_theta = 0;
+  OD_CLEAR(y, n);
+  best_qtheta = 0;
+  m = 0;
+  s = 1;
+  corr = corr/(1e-100 + g*(double)gr/OD_SHL(1, xshift + rshift));
+  corr = OD_MAXF(OD_MINF(corr, 1.), -1.);
+  if (is_keyframe) skip_dist = gain_weight*cg*cg*OD_CGAIN_SCALE_2;
+  else {
+    skip_dist = gain_weight*(cg - cgr)*(cg - cgr)
+     + cgr*(double)cg*(2 - 2*corr);
+    skip_dist *= OD_CGAIN_SCALE_2;
+  }
+  if (!is_keyframe) {
+    /* noref, gain=0 isn't allowed, but skip is allowed. */
+    od_val32 scgr;
+    scgr = OD_MAXF(0,gain_offset);
+    if (icgr == 0) {
+      best_dist = gain_weight*(cg - scgr)*(cg - scgr)
+       + scgr*(double)cg*(2 - 2*corr);
+      best_dist *= OD_CGAIN_SCALE_2;
+    }
+    best_cost = best_dist + pvq_norm_lambda*od_pvq_rate(0, icgr, 0, 0, adapt,
+     NULL, 0, n, is_keyframe, pli, speed);
+    best_qtheta = 0;
+    *itheta = 0;
+    *max_theta = 0;
+    noref = 0;
+  }
+  dist0 = best_dist;
+  if (n <= OD_MAX_PVQ_SIZE && !od_vector_is_null(r0, n) && corr > 0) {
+    od_val16 xr[MAXN];
+    int gain_bound;
+    int prev_k;
+    pvq_search_item items[MAX_PVQ_ITEMS];
+    int idx;
+    int nitems;
+    double cos_dist;
+    idx = 0;
+    gain_bound = OD_SHR(cg - gain_offset, OD_CGAIN_SHIFT);
+    /* Perform theta search only if prediction is useful. */
+    theta = OD_ROUND32(OD_THETA_SCALE*acos(corr));
+    m = od_compute_householder(r16, n, gr, &s, rshift);
+    od_apply_householder(xr, x16, r16, n);
+    prev_k = 0;
+    for (i = m; i < n - 1; i++) xr[i] = xr[i + 1];
+    /* Compute all candidate PVQ searches within a reasonable range of gain
+       and theta. */
+    for (i = OD_MAXI(1, gain_bound - 1); i <= gain_bound + 1; i++) {
+      int j;
+      od_val32 qcg;
+      int ts;
+      int theta_lower;
+      int theta_upper;
+      /* Quantized companded gain */
+      qcg = OD_SHL(i, OD_CGAIN_SHIFT) + gain_offset;
+      /* Set angular resolution (in ra) to match the encoded gain */
+      ts = od_pvq_compute_max_theta(qcg, beta);
+      theta_lower = OD_MAXI(0, (int)floor(.5 +
+       theta*OD_THETA_SCALE_1*2/M_PI*ts) - 2);
+      theta_upper = OD_MINI(ts - 1, (int)ceil(theta*OD_THETA_SCALE_1*2/M_PI*ts));
+      /* Include the angles within a reasonable range. */
+      for (j = theta_lower; j <= theta_upper; j++) {
+        od_val32 qtheta;
+        qtheta = od_pvq_compute_theta(j, ts);
+        k = od_pvq_compute_k(qcg, j, qtheta, 0, n, beta, robust || is_keyframe);
+        items[idx].gain = i;
+        items[idx].theta = j;
+        items[idx].k = k;
+        items[idx].qcg = qcg;
+        items[idx].qtheta = qtheta;
+        items[idx].ts = ts;
+        idx++;
+        OD_ASSERT(idx < MAX_PVQ_ITEMS);
+      }
+    }
+    nitems = idx;
+    cos_dist = 0;
+    /* Sort PVQ search candidates in ascending order of pulses K so that
+       we can reuse all the previously searched pulses across searches. */
+    qsort(items, nitems, sizeof(items[0]),
+     (int (*)(const void *, const void *))items_compare);
+    /* Search for the best gain/theta in order. */
+    for (idx = 0; idx < nitems; idx++) {
+      int j;
+      od_val32 qcg;
+      int ts;
+      double cost;
+      double dist_theta;
+      double sin_prod;
+      od_val32 qtheta;
+      /* Quantized companded gain */
+      qcg = items[idx].qcg;
+      i = items[idx].gain;
+      j = items[idx].theta;
+      /* Set angular resolution (in ra) to match the encoded gain */
+      ts = items[idx].ts;
+      /* Search for the best angle within a reasonable range. */
+      qtheta = items[idx].qtheta;
+      k = items[idx].k;
+      /* Compute the minimal possible distortion by not taking the PVQ
+         cos_dist into account. */
+      dist_theta = 2 - 2.*od_pvq_cos(theta - qtheta)*OD_TRIG_SCALE_1;
+      dist = gain_weight*(qcg - cg)*(qcg - cg) + qcg*(double)cg*dist_theta;
+      dist *= OD_CGAIN_SCALE_2;
+      /* If we have no hope of beating skip (including a 1-bit worst-case
+         penalty), stop now. */
+      if (dist > dist0 + 1.0*pvq_norm_lambda && k != 0) continue;
+      sin_prod = od_pvq_sin(theta)*OD_TRIG_SCALE_1*od_pvq_sin(qtheta)*
+       OD_TRIG_SCALE_1;
+      /* PVQ search, using a gain of qcg*cg*sin(theta)*sin(qtheta) since
+         that's the factor by which cos_dist is multiplied to get the
+         distortion metric. */
+      if (k == 0) {
+        cos_dist = 0;
+        OD_CLEAR(y_tmp, n-1);
+      }
+      else if (k != prev_k) {
+        cos_dist = pvq_search_rdo_double(xr, n - 1, k, y_tmp,
+         qcg*(double)cg*sin_prod*OD_CGAIN_SCALE_2, pvq_norm_lambda, prev_k);
+      }
+      prev_k = k;
+      /* See Jmspeex' Journal of Dubious Theoretical Results. */
+      dist_theta = 2 - 2.*od_pvq_cos(theta - qtheta)*OD_TRIG_SCALE_1
+       + sin_prod*(2 - 2*cos_dist);
+      dist = gain_weight*(qcg - cg)*(qcg - cg) + qcg*(double)cg*dist_theta;
+      dist *= OD_CGAIN_SCALE_2;
+      /* Do approximate RDO. */
+      cost = dist + pvq_norm_lambda*od_pvq_rate(i, icgr, j, ts, adapt, y_tmp,
+       k, n, is_keyframe, pli, speed);
+      if (cost < best_cost) {
+        best_cost = cost;
+        best_dist = dist;
+        qg = i;
+        best_k = k;
+        best_qtheta = qtheta;
+        *itheta = j;
+        *max_theta = ts;
+        noref = 0;
+        OD_COPY(y, y_tmp, n - 1);
+      }
+    }
+  }
+  /* Don't bother with no-reference version if there's a reasonable
+     correlation. The only exception is luma on a keyframe because
+     H/V prediction is unreliable. */
+  if (n <= OD_MAX_PVQ_SIZE &&
+   ((is_keyframe && pli == 0) || corr < .5
+   || cg < (od_val32)(OD_SHL(2, OD_CGAIN_SHIFT)))) {
+    int gain_bound;
+    int prev_k;
+    gain_bound = OD_SHR(cg, OD_CGAIN_SHIFT);
+    prev_k = 0;
+    /* Search for the best gain (haven't determined reasonable range yet). */
+    for (i = OD_MAXI(1, gain_bound); i <= gain_bound + 1; i++) {
+      double cos_dist;
+      double cost;
+      od_val32 qcg;
+      qcg = OD_SHL(i, OD_CGAIN_SHIFT);
+      k = od_pvq_compute_k(qcg, -1, -1, 1, n, beta, robust || is_keyframe);
+      /* Compute the minimal possible distortion by not taking the PVQ
+         cos_dist into account. */
+      dist = gain_weight*(qcg - cg)*(qcg - cg);
+      dist *= OD_CGAIN_SCALE_2;
+      if (dist > dist0 && k != 0) continue;
+      cos_dist = pvq_search_rdo_double(x16, n, k, y_tmp,
+       qcg*(double)cg*OD_CGAIN_SCALE_2, pvq_norm_lambda, prev_k);
+      prev_k = k;
+      /* See Jmspeex' Journal of Dubious Theoretical Results. */
+      dist = gain_weight*(qcg - cg)*(qcg - cg)
+       + qcg*(double)cg*(2 - 2*cos_dist);
+      dist *= OD_CGAIN_SCALE_2;
+      /* Do approximate RDO. */
+      cost = dist + pvq_norm_lambda*od_pvq_rate(i, 0, -1, 0, adapt, y_tmp, k,
+       n, is_keyframe, pli, speed);
+      if (cost <= best_cost) {
+        best_cost = cost;
+        best_dist = dist;
+        qg = i;
+        noref = 1;
+        best_k = k;
+        *itheta = -1;
+        *max_theta = 0;
+        OD_COPY(y, y_tmp, n);
+      }
+    }
+  }
+  k = best_k;
+  theta = best_qtheta;
+  skip = 0;
+  if (noref) {
+    if (qg == 0) skip = OD_PVQ_SKIP_ZERO;
+  }
+  else {
+    if (!is_keyframe && qg == 0) {
+      skip = (icgr ? OD_PVQ_SKIP_ZERO : OD_PVQ_SKIP_COPY);
+    }
+    if (qg == icgr && *itheta == 0 && !cfl_enabled) skip = OD_PVQ_SKIP_COPY;
+  }
+  /* Synthesize like the decoder would. */
+  if (skip) {
+    if (skip == OD_PVQ_SKIP_COPY) OD_COPY(out, r0, n);
+    else OD_CLEAR(out, n);
+  }
+  else {
+    if (noref) gain_offset = 0;
+    g = od_gain_expand(OD_SHL(qg, OD_CGAIN_SHIFT) + gain_offset, q0, beta);
+    od_pvq_synthesis_partial(out, y, r16, n, noref, g, theta, m, s,
+     qm_inv);
+  }
+  *vk = k;
+  *skip_diff += skip_dist - best_dist;
+  /* Encode gain differently depending on whether we use prediction or not.
+     Special encoding on inter frames where qg=0 is allowed for noref=0
+     but not noref=1.*/
+  if (is_keyframe) return noref ? qg : neg_interleave(qg, icgr);
+  else return noref ? qg - 1 : neg_interleave(qg + 1, icgr + 1);
+}
+
+/** Encodes a single vector of integers (eg, a partition within a
+ *  coefficient block) using PVQ
+ *
+ * @param [in,out] ec         range encoder
+ * @param [in]     qg         quantized gain
+ * @param [in]     theta      quantized post-prediction theta
+ * @param [in]     max_theta  maximum possible quantized theta value
+ * @param [in]     in         coefficient vector to code
+ * @param [in]     n          number of coefficients in partition
+ * @param [in]     k          number of pulses in partition
+ * @param [in,out] model      entropy encoder state
+ * @param [in,out] adapt      adaptation context
+ * @param [in,out] exg        ExQ16 expectation of gain value
+ * @param [in,out] ext        ExQ16 expectation of theta value
+ * @param [in]     nodesync   do not use info that depend on the reference
+ * @param [in]     cdf_ctx    selects which cdf context to use
+ * @param [in]     is_keyframe whether we're encoding a keyframe
+ * @param [in]     code_skip  whether the "skip rest" flag is allowed
+ * @param [in]     skip_rest  when set, we skip all higher bands
+ * @param [in]     encode_flip whether we need to encode the CfL flip flag now
+ * @param [in]     flip       value of the CfL flip flag
+ */
+static void pvq_encode_partition(od_ec_enc *ec,
+                                 int qg,
+                                 int theta,
+                                 int max_theta,
+                                 const od_coeff *in,
+                                 int n,
+                                 int k,
+                                 generic_encoder model[3],
+                                 od_adapt_ctx *adapt,
+                                 int *exg,
+                                 int *ext,
+                                 int nodesync,
+                                 int cdf_ctx,
+                                 int is_keyframe,
+                                 int code_skip,
+                                 int skip_rest,
+                                 int encode_flip,
+                                 int flip) {
+  int noref;
+  int id;
+  noref = (theta == -1);
+  id = (qg > 0) + 2*OD_MINI(theta + 1,3) + 8*code_skip*skip_rest;
+  if (is_keyframe) {
+    OD_ASSERT(id != 8);
+    if (id >= 8) id--;
+  }
+  else {
+    OD_ASSERT(id != 10);
+    if (id >= 10) id--;
+  }
+  /* Jointly code gain, theta and noref for small values. Then we handle
+     larger gain and theta values. For noref, theta = -1. */
+  od_encode_cdf_adapt(ec, id, &adapt->pvq.pvq_gaintheta_cdf[cdf_ctx][0],
+   8 + 7*code_skip, adapt->pvq.pvq_gaintheta_increment);
+  if (encode_flip) {
+    /* We could eventually do some smarter entropy coding here, but it would
+       have to be good enough to overcome the overhead of the entropy coder.
+       An early attempt using a "toogle" flag with simple adaptation wasn't
+       worth the trouble. */
+    od_ec_enc_bits(ec, flip, 1);
+  }
+  if (qg > 0) {
+    int tmp;
+    tmp = *exg;
+    generic_encode(ec, &model[!noref], qg - 1, -1, &tmp, 2);
+    OD_IIR_DIADIC(*exg, qg << 16, 2);
+  }
+  if (theta > 1 && (nodesync || max_theta > 3)) {
+    int tmp;
+    tmp = *ext;
+    generic_encode(ec, &model[2], theta - 2, nodesync ? -1 : max_theta - 3,
+     &tmp, 2);
+    OD_IIR_DIADIC(*ext, theta << 16, 2);
+  }
+  od_encode_pvq_codeword(ec, &adapt->pvq.pvq_codeword_ctx, in,
+   n - (theta != -1), k);
+}
+
+/** Quantizes a scalar with rate-distortion optimization (RDO)
+ * @param [in] x      unquantized value
+ * @param [in] q      quantization step size
+ * @param [in] delta0 rate increase for encoding a 1 instead of a 0
+ * @param [in] pvq_norm_lambda enc->pvq_norm_lambda for quantized RDO
+ * @retval quantized value
+ */
+int od_rdo_quant(od_coeff x, int q, double delta0, double pvq_norm_lambda) {
+  int n;
+  /* Optimal quantization threshold is 1/2 + lambda*delta_rate/2. See
+     Jmspeex' Journal of Dubious Theoretical Results for details. */
+  n = OD_DIV_R0(abs(x), q);
+  if ((double)abs(x)/q < (double)n/2 + pvq_norm_lambda*delta0/(2*n)) {
+    return 0;
+  }
+  else {
+    return OD_DIV_R0(x, q);
+  }
+}
+
+#if OD_SIGNAL_Q_SCALING
+void od_encode_quantizer_scaling(daala_enc_ctx *enc, int q_scaling,
+ int sbx, int sby, int skip) {
+  int nhsb;
+  OD_ASSERT(skip == !!skip);
+  nhsb = enc->state.nhsb;
+  OD_ASSERT(sbx < nhsb);
+  OD_ASSERT(sby < enc->state.nvsb);
+  OD_ASSERT(!skip || q_scaling == 0);
+  enc->state.sb_q_scaling[sby*nhsb + sbx] = q_scaling;
+  if (!skip) {
+    int above;
+    int left;
+    /* use value from neighbour if possible, otherwise use 0 */
+    above = sby > 0 ? enc->state.sb_q_scaling[(sby - 1)*enc->state.nhsb + sbx]
+     : 0;
+    left = sbx > 0 ? enc->state.sb_q_scaling[sby*enc->state.nhsb + (sbx - 1)]
+     : 0;
+    od_encode_cdf_adapt(&enc->ec, q_scaling,
+     enc->state.adapt.q_cdf[above + left*4], 4,
+     enc->state.adapt.q_increment);
+  }
+}
+#endif
+
+/** Encode a coefficient block (excepting DC) using PVQ
+ *
+ * @param [in,out] enc     daala encoder context
+ * @param [in]     ref     'reference' (prediction) vector
+ * @param [in]     in      coefficient block to quantize and encode
+ * @param [out]    out     quantized coefficient block
+ * @param [in]     q0      scale/quantizer
+ * @param [in]     pli     plane index
+ * @param [in]     bs      log of the block size minus two
+ * @param [in]     beta    per-band activity masking beta param
+ * @param [in]     robust  make stream robust to error in the reference
+ * @param [in]     is_keyframe whether we're encoding a keyframe
+ * @param [in]     q_scaling scaling factor to apply to quantizer
+ * @param [in]     bx      x-coordinate of this block
+ * @param [in]     by      y-coordinate of this block
+ * @param [in]     qm      QM with magnitude compensation
+ * @param [in]     qm_inv  Inverse of QM with magnitude compensation
+ * @param [in]     speed   Make search faster by making approximations
+ * @param [in]     pvq_info If null, conisdered as RDO search mode
+ * @return         Returns 1 if both DC and AC coefficients are skipped,
+ *                 zero otherwise
+ */
+int od_pvq_encode(daala_enc_ctx *enc,
+                   od_coeff *ref,
+                   const od_coeff *in,
+                   od_coeff *out,
+                   int q_dc,
+                   int q_ac,
+                   int pli,
+                   int bs,
+                   const od_val16 *beta,
+                   int robust,
+                   int is_keyframe,
+                   int q_scaling,
+                   int bx,
+                   int by,
+                   const int16_t *qm,
+                   const int16_t *qm_inv,
+                   int speed,
+                   PVQ_INFO *pvq_info){
+  int theta[PVQ_MAX_PARTITIONS];
+  int max_theta[PVQ_MAX_PARTITIONS];
+  int qg[PVQ_MAX_PARTITIONS];
+  int k[PVQ_MAX_PARTITIONS];
+  od_coeff y[OD_BSIZE_MAX*OD_BSIZE_MAX];
+  int *exg;
+  int *ext;
+  int nb_bands;
+  int i;
+  const int *off;
+  int size[PVQ_MAX_PARTITIONS];
+  generic_encoder *model;
+  double skip_diff;
+  int tell;
+  uint16_t *skip_cdf;
+  od_rollback_buffer buf;
+  int dc_quant;
+  int flip;
+  int cfl_encoded;
+  int skip_rest;
+  int skip_dir;
+  int skip_theta_value;
+  /* const unsigned char *pvq_qm; */
+  double dc_rate;
+#if !OD_SIGNAL_Q_SCALING
+  OD_UNUSED(q_scaling);
+  OD_UNUSED(bx);
+  OD_UNUSED(by);
+#endif
+  /* pvq_qm = &enc->state.pvq_qm_q4[pli][0]; */
+  exg = &enc->state.adapt.pvq.pvq_exg[pli][bs][0];
+  ext = enc->state.adapt.pvq.pvq_ext + bs*PVQ_MAX_PARTITIONS;
+  skip_cdf = enc->state.adapt.skip_cdf[2*bs + (pli != 0)];
+  model = enc->state.adapt.pvq.pvq_param_model;
+  nb_bands = OD_BAND_OFFSETS[bs][0];
+  off = &OD_BAND_OFFSETS[bs][1];
+  /*dc_quant = OD_MAXI(1, q0*pvq_qm[od_qm_get_index(bs, 0)] >> 4);*/
+  dc_quant = OD_MAXI(1, q_dc);
+  tell = 0;
+  for (i = 0; i < nb_bands; i++) size[i] = off[i+1] - off[i];
+  skip_diff = 0;
+  flip = 0;
+  /*If we are coding a chroma block of a keyframe, we are doing CfL.*/
+  if (pli != 0 && is_keyframe) {
+    od_val32 xy;
+    xy = 0;
+    /*Compute the dot-product of the first band of chroma with the luma ref.*/
+    for (i = off[0]; i < off[1]; i++) {
+#if defined(OD_FLOAT_PVQ)
+      xy += ref[i]*(double)qm[i]*OD_QM_SCALE_1*
+       (double)in[i]*(double)qm[i]*OD_QM_SCALE_1;
+#else
+      od_val32 rq;
+      od_val32 inq;
+      rq = ref[i]*qm[i];
+      inq = in[i]*qm[i];
+      xy += OD_SHR(rq*(int64_t)inq, OD_SHL(OD_QM_SHIFT + OD_CFL_FLIP_SHIFT,
+       1));
+#endif
+    }
+    /*If cos(theta) < 0, then |theta| > pi/2 and we should negate the ref.*/
+    if (xy < 0) {
+      flip = 1;
+      for(i = off[0]; i < off[nb_bands]; i++) ref[i] = -ref[i];
+    }
+  }
+  for (i = 0; i < nb_bands; i++) {
+    int q;
+    /*q = OD_MAXI(1, q0*pvq_qm[od_qm_get_index(bs, i + 1)] >> 4);*/
+    q = OD_MAXI(1, q_ac);
+    qg[i] = pvq_theta(out + off[i], in + off[i], ref + off[i], size[i],
+     q, y + off[i], &theta[i], &max_theta[i],
+     &k[i], beta[i], &skip_diff, robust, is_keyframe, pli, &enc->state.adapt,
+     qm + off[i], qm_inv + off[i], enc->pvq_norm_lambda, speed);
+  }
+  od_encode_checkpoint(enc, &buf);
+  if (is_keyframe) out[0] = 0;
+  else {
+    int n;
+    n = OD_DIV_R0(abs(in[0] - ref[0]), dc_quant);
+    if (n == 0) {
+      out[0] = 0;
+    } else {
+      int tell2;
+      od_rollback_buffer dc_buf;
+
+      dc_rate = -OD_LOG2((double)(skip_cdf[3] - skip_cdf[2])/
+       (double)(skip_cdf[2] - skip_cdf[1]));
+      dc_rate += 1;
+
+      tell2 = od_ec_enc_tell_frac(&enc->ec);
+      od_encode_checkpoint(enc, &dc_buf);
+      generic_encode(&enc->ec, &enc->state.adapt.model_dc[pli],
+       n - 1, -1, &enc->state.adapt.ex_dc[pli][bs][0], 2);
+      tell2 = od_ec_enc_tell_frac(&enc->ec) - tell2;
+      dc_rate += tell2/8.0;
+      od_encode_rollback(enc, &dc_buf);
+
+      out[0] = od_rdo_quant(in[0] - ref[0], dc_quant, dc_rate,
+       enc->pvq_norm_lambda);
+    }
+  }
+  tell = od_ec_enc_tell_frac(&enc->ec);
+  /* Code as if we're not skipping. */
+  od_encode_cdf_adapt(&enc->ec, 2 + (out[0] != 0), skip_cdf,
+   4, enc->state.adapt.skip_increment);
+  if (pvq_info)
+    pvq_info->ac_dc_coded = 2 + (out[0] != 0);
+#if OD_SIGNAL_Q_SCALING
+  if (bs == OD_NBSIZES - 1 && pli == 0) {
+    od_encode_quantizer_scaling(enc, q_scaling, bx >> (OD_NBSIZES - 1),
+     by >> (OD_NBSIZES - 1), 0);
+  }
+#endif
+  cfl_encoded = 0;
+  skip_rest = 1;
+  skip_theta_value = is_keyframe ? -1 : 0;
+  for (i = 1; i < nb_bands; i++) {
+    if (theta[i] != skip_theta_value || qg[i]) skip_rest = 0;
+  }
+  skip_dir = 0;
+  if (nb_bands > 1) {
+    for (i = 0; i < 3; i++) {
+      int j;
+      int tmp;
+      tmp = 1;
+      for (j = i + 1; j < nb_bands; j += 3) {
+        if (theta[j] != skip_theta_value || qg[j]) tmp = 0;
+      }
+      skip_dir |= tmp << i;
+    }
+  }
+  if (theta[0] == skip_theta_value && qg[0] == 0 && skip_rest) nb_bands = 0;
+
+  /* NOTE: There was no other better place to put this function. */
+  if (pvq_info)
+    store_pvq_enc_info(pvq_info, qg, theta, max_theta, k,
+      y, nb_bands, off, size,
+      skip_rest, skip_dir, bs);
+
+  for (i = 0; i < nb_bands; i++) {
+    int encode_flip;
+    /* Encode CFL flip bit just after the first time it's used. */
+    encode_flip = pli != 0 && is_keyframe && theta[i] != -1 && !cfl_encoded;
+    if (i == 0 || (!skip_rest && !(skip_dir & (1 << ((i - 1)%3))))) {
+      pvq_encode_partition(&enc->ec, qg[i], theta[i], max_theta[i], y + off[i],
+       size[i], k[i], model, &enc->state.adapt, exg + i, ext + i,
+       robust || is_keyframe, (pli != 0)*OD_NBSIZES*PVQ_MAX_PARTITIONS
+       + bs*PVQ_MAX_PARTITIONS + i, is_keyframe, i == 0 && (i < nb_bands - 1),
+       skip_rest, encode_flip, flip);
+    }
+    if (i == 0 && !skip_rest && bs > 0) {
+      od_encode_cdf_adapt(&enc->ec, skip_dir,
+       &enc->state.adapt.pvq.pvq_skip_dir_cdf[(pli != 0) + 2*(bs - 1)][0], 7,
+       enc->state.adapt.pvq.pvq_skip_dir_increment);
+    }
+    if (encode_flip) cfl_encoded = 1;
+  }
+  tell = od_ec_enc_tell_frac(&enc->ec) - tell;
+  /* Account for the rate of skipping the AC, based on the same DC decision
+     we made when trying to not skip AC. */
+  {
+    double skip_rate;
+    if (out[0] != 0) {
+      skip_rate = -OD_LOG2((skip_cdf[1] - skip_cdf[0])/
+     (double)skip_cdf[3]);
+    }
+    else {
+      skip_rate = -OD_LOG2(skip_cdf[0]/
+     (double)skip_cdf[3]);
+    }
+    tell -= (int)floor(.5+8*skip_rate);
+  }
+  if (nb_bands == 0 || skip_diff <= enc->pvq_norm_lambda/8*tell) {
+    if (is_keyframe) out[0] = 0;
+    else {
+      int n;
+      n = OD_DIV_R0(abs(in[0] - ref[0]), dc_quant);
+      if (n == 0) {
+        out[0] = 0;
+      } else {
+        int tell2;
+        od_rollback_buffer dc_buf;
+
+        dc_rate = -OD_LOG2((double)(skip_cdf[1] - skip_cdf[0])/
+         (double)skip_cdf[0]);
+        dc_rate += 1;
+
+        tell2 = od_ec_enc_tell_frac(&enc->ec);
+        od_encode_checkpoint(enc, &dc_buf);
+        generic_encode(&enc->ec, &enc->state.adapt.model_dc[pli],
+         n - 1, -1, &enc->state.adapt.ex_dc[pli][bs][0], 2);
+        tell2 = od_ec_enc_tell_frac(&enc->ec) - tell2;
+        dc_rate += tell2/8.0;
+        od_encode_rollback(enc, &dc_buf);
+
+        out[0] = od_rdo_quant(in[0] - ref[0], dc_quant, dc_rate,
+         enc->pvq_norm_lambda);
+      }
+    }
+    /* We decide to skip, roll back everything as it was before. */
+    od_encode_rollback(enc, &buf);
+    od_encode_cdf_adapt(&enc->ec, out[0] != 0, skip_cdf,
+     4, enc->state.adapt.skip_increment);
+    if (pvq_info)
+      pvq_info->ac_dc_coded = (out[0] != 0);
+#if OD_SIGNAL_Q_SCALING
+    if (bs == OD_NBSIZES - 1 && pli == 0) {
+      int skip;
+      skip = out[0] == 0;
+      if (skip) {
+        q_scaling = 0;
+      }
+      od_encode_quantizer_scaling(enc, q_scaling, bx >> (OD_NBSIZES - 1),
+       by >> (OD_NBSIZES - 1), skip);
+    }
+#endif
+    if (is_keyframe) for (i = 1; i < 1 << (2*bs + 4); i++) out[i] = 0;
+    else for (i = 1; i < 1 << (2*bs + 4); i++) out[i] = ref[i];
+    if (out[0] == 0) return 1;
+  }
+  return 0;
+}
diff --git a/av1/encoder/pvq_encoder.h b/av1/encoder/pvq_encoder.h
new file mode 100644
index 0000000..d8ddf4c
--- /dev/null
+++ b/av1/encoder/pvq_encoder.h
@@ -0,0 +1,52 @@
+/*Daala video codec
+Copyright (c) 2012 Daala project contributors.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+- Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.*/
+
+/* clang-format off */
+
+#if !defined(_pvq_encoder_H)
+# define _pvq_encoder_H (1)
+# include "aom_dsp/entenc.h"
+# include "av1/common/blockd.h"
+# include "av1/common/pvq.h"
+# include "av1/encoder/encint.h"
+
+void od_encode_band_pvq_splits(od_ec_enc *ec, od_pvq_codeword_ctx *adapt,
+ const int *y, int n, int k, int level);
+
+void laplace_encode_special(od_ec_enc *enc, int x, unsigned decay, int max);
+void laplace_encode(od_ec_enc *enc, int x, int ex_q8, int k);
+void laplace_encode_vector(od_ec_enc *enc, const od_coeff *y, int n, int k,
+                                  int32_t *curr, const int32_t *means);
+
+#if OD_SIGNAL_Q_SCALING
+void od_encode_quantizer_scaling(daala_enc_ctx *enc, int q_scaling, int bx,
+ int by, int skip);
+#endif
+
+int od_pvq_encode(daala_enc_ctx *enc, od_coeff *ref, const od_coeff *in,
+ od_coeff *out, int q_dc, int q_ac, int pli, int bs, const od_val16 *beta, int robust,
+ int is_keyframe, int q_scaling, int bx, int by, const int16_t *qm,
+ const int16_t *qm_inv, int speed, PVQ_INFO *pvq_info);
+
+#endif
