{
  "comments": [
    {
      "key": {
        "uuid": "258f2e47_c121315c",
        "filename": "av1/common/daala_tx_kernels.h",
        "patchSetId": 14
      },
      "lineNbr": 12,
      "author": {
        "id": 6240
      },
      "writtenOn": "2017-12-13T22:22:40Z",
      "side": 1,
      "message": "Is there a good reason for clang-format to be off here?",
      "revId": "eaa6d2b98e3c83c3298a695f1c24c6e7bd6b7c45",
      "serverId": "e5514cf8-2d6e-3e29-adb4-24cd6dde4bf0",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "5d5a7308_655b28ad",
        "filename": "av1/common/daala_tx_kernels.h",
        "patchSetId": 14
      },
      "lineNbr": 12,
      "author": {
        "id": 5046
      },
      "writtenOn": "2017-12-14T04:36:32Z",
      "side": 1,
      "message": "Yes, I have clang-format off so that the parameter lists for the larger transform functions are unmodified.  The way they are now it is easy to see the pattern of how parameters are passed into the embedded N/2-point transforms.  See line 1573 for a good example of this.",
      "parentUuid": "258f2e47_c121315c",
      "revId": "eaa6d2b98e3c83c3298a695f1c24c6e7bd6b7c45",
      "serverId": "e5514cf8-2d6e-3e29-adb4-24cd6dde4bf0",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "2f4ccce1_77f1cb41",
        "filename": "av1/common/daala_tx_kernels.h",
        "patchSetId": 14
      },
      "lineNbr": 23,
      "author": {
        "id": 7984
      },
      "writtenOn": "2017-12-14T17:45:42Z",
      "side": 1,
      "message": "Hi Nathan,\n\nI\u0027m looking further at the hardware implementation complexity and I am wondering about this special halving operation. I can see that it is symmetric about 0 and is designed to remove bias but it does mean that you have to resolve the sign before knowing the round term which makes implementation more difficult. I assume there is also a software cost? So I was wondering what happens if this is replaced by fixed rounds which are chosen to cancel bias. \n\nI performed the following experiment:\n\nod_rshift1() code changed to (v+1)\u003e\u003e1\nod_rshift0() function added which does (v+0)\u003e\u003e1\nod_butterfly_add() changed to use od_rshift0 rather than od_rshift1\nod_butterfly_neg() changed to use od_rshift0 rather than od_rshift1\nod_idct_4() changed to use od_rshift0() for the shift of *q3\n\nWith these changes I seemed to get a smaller reconstruction error on fdct_4+idct_4 compared to the original code. (I haven\u0027t looked at other transforms apart from dct4 but assume a similar approach could be used). I was wondering if with the changes above you see a smaller error on your test bench as well?",
      "revId": "eaa6d2b98e3c83c3298a695f1c24c6e7bd6b7c45",
      "serverId": "e5514cf8-2d6e-3e29-adb4-24cd6dde4bf0",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "28ee084d_68d64e24",
        "filename": "av1/common/daala_tx_kernels.h",
        "patchSetId": 14
      },
      "lineNbr": 23,
      "author": {
        "id": 5046
      },
      "writtenOn": "2017-12-14T23:04:43Z",
      "side": 1,
      "message": "Hi Dominic,\n\nThanks for looking into this.  It was always our hope to come back at the end of this process and see if we could remove the (v \u003c 0) and replace it with fixed rounds as you suggest.  This would also make things easier in software.\n\nMaking the changes you suggest and running them through my test harness gives the following round trip values:\n\nBefore:\n\n1-D Forward Bias:\n-0.003180900000000, -0.010129612500000,  0.062512300000000,  0.006675037500000\n1-D Round-Trip Bias:\n 0.016486000000000,  0.015012900000000,  0.026461600000000,  0.025898500000000\n1-D Q\u003d8 Bias:\n 0.031504600000000,  0.032371100000000,  0.030985800000000,  0.030796400000000\n1-D Q\u003d7 Bias:\n 0.031216500000000,  0.031625700000000,  0.031865400000000,  0.031434700000000\n\nAfter:\n\n1-D Forward Bias:\n-0.003180900000000, -0.010129612500000,  0.062512300000000,  0.006675037500000\n1-D Round-Trip Bias:\n 0.006904700000000,  0.025079700000000,  0.016280000000000,  0.019508900000000\n1-D Q\u003d8 Bias:\n 0.015692400000000,  0.049328800000000,  0.013389100000000,  0.015271000000000\n1-D Q\u003d7 Bias:\n 0.015560800000000,  0.046854600000000,  0.016466100000000,  0.015841100000000\n\nIt looks like those changes might be an improvement overall, though some of the error seems to be shifted into y[1].  Note that all of our transforms are run at Q4 and 1/2^4 \u003d 0.0625.",
      "parentUuid": "2f4ccce1_77f1cb41",
      "revId": "eaa6d2b98e3c83c3298a695f1c24c6e7bd6b7c45",
      "serverId": "e5514cf8-2d6e-3e29-adb4-24cd6dde4bf0",
      "unresolved": true
    }
  ]
}